Introduction
to
Machine
Learning
Second
Edition

Adaptive Computation and Machine Learning

Thomas Dietterich, Editor
Christopher Bishop, David Heckerman, Michael Jordan, and Michael
Kearns, Associate Editors

A complete list of books published in The Adaptive Computation and
Machine Learning series appears at the back of this book.

Introduction
to
Machine
Learning

Second
Edition

Ethem Alpaydın

The MIT Press
Cambridge, Massachusetts
London, England

© 2010 Massachusetts Institute of Technology
All rights reserved. No part of this book may be reproduced in any form by any
electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.
For information about special quantity discounts, please email
special_sales@mitpress.mit.edu.
A
Typeset in 10/13 Lucida Bright by the author using LTEX 2ε .
Printed and bound in the United States of America.

Library of Congress Cataloging-in-Publication Information
Alpaydin, Ethem.
Introduction to machine learning / Ethem Alpaydin. — 2nd ed.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-262-01243-0 (hardcover : alk. paper)
1. Machine learning. I. Title
Q325.5.A46 2010
006.3’1—dc22
2009013169
CIP

10 9 8 7 6 5 4 3 2 1

Brief Contents

1

Introduction

2

Supervised Learning

1

3

Bayesian Decision Theory

4

Parametric Methods

5

Multivariate Methods

6

Dimensionality Reduction

7

Clustering

8

Nonparametric Methods

9

Decision Trees

21
47

61
87
109

143
163

185

10 Linear Discrimination

209

11 Multilayer Perceptrons
12 Local Models

233

279

13 Kernel Machines

309

14 Bayesian Estimation

341

15 Hidden Markov Models
16 Graphical Models

363

387

17 Combining Multiple Learners
18 Reinforcement Learning

419

447

19 Design and Analysis of Machine Learning Experiments
A Probability

517

475

Contents

Series Foreword
Figures

xix

Tables

xvii

xxix

Preface

xxxi

Acknowledgments

xxxiii

Notes for the Second Edition
Notations

xxxix

1 Introduction
1.1
1.2

1.3
1.4
1.5
1.6

1

What Is Machine Learning?
1
Examples of Machine Learning Applications
1.2.1 Learning Associations
4
1.2.2 Classiﬁcation
5
1.2.3 Regression
9
1.2.4 Unsupervised Learning
11
1.2.5 Reinforcement Learning
13
Notes
14
Relevant Resources
16
Exercises
18
References
19

2 Supervised Learning
2.1

xxxv

21

Learning a Class from Examples

21

4

viii

Contents

2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
2.10
2.11

Vapnik-Chervonenkis (VC) Dimension
27
Probably Approximately Correct (PAC) Learning
29
Noise
30
Learning Multiple Classes
32
Regression
34
Model Selection and Generalization
37
Dimensions of a Supervised Machine Learning Algorithm
Notes
42
Exercises
43
References
44

3 Bayesian Decision Theory
3.1
3.2
3.3
3.4
3.5
3.6
3.7
3.8
3.9

Introduction
47
Classiﬁcation
49
Losses and Risks
51
Discriminant Functions
Utility Theory
54
Association Rules
55
Notes
58
Exercises
58
References
59

4 Parametric Methods
4.1
4.2

4.3
4.4
4.5
4.6
4.7
4.8
4.9
4.10
4.11

53

61

Introduction
61
Maximum Likelihood Estimation
62
4.2.1 Bernoulli Density
63
4.2.2 Multinomial Density
64
4.2.3 Gaussian (Normal) Density
64
Evaluating an Estimator: Bias and Variance
65
The Bayes’ Estimator
66
Parametric Classiﬁcation
69
Regression
73
Tuning Model Complexity: Bias/Variance Dilemma
Model Selection Procedures
80
Notes
84
Exercises
84
References
85

5 Multivariate Methods
5.1

47

Multivariate Data

87
87

76

41

ix

Contents

5.2
5.3
5.4
5.5
5.6
5.7
5.8
5.9
5.10
5.11

Parameter Estimation
88
Estimation of Missing Values
89
Multivariate Normal Distribution
90
Multivariate Classiﬁcation
94
Tuning Complexity
99
Discrete Features
102
Multivariate Regression
103
Notes
105
Exercises
106
References
107

6 Dimensionality Reduction
6.1
6.2
6.3
6.4
6.5
6.6
6.7
6.8
6.9
6.10
6.11

Introduction
109
Subset Selection
110
Principal Components Analysis
113
Factor Analysis
120
Multidimensional Scaling
125
Linear Discriminant Analysis
128
Isomap
133
Locally Linear Embedding
135
Notes
138
Exercises
139
References
140

7 Clustering
7.1
7.2
7.3
7.4
7.5
7.6
7.7
7.8
7.9
7.10
7.11

143

Introduction
143
Mixture Densities
144
k-Means Clustering
145
Expectation-Maximization Algorithm
149
Mixtures of Latent Variable Models
154
Supervised Learning after Clustering
155
Hierarchical Clustering
157
Choosing the Number of Clusters
158
Notes
160
Exercises
160
References
161

8 Nonparametric Methods
8.1
8.2

109

163

Introduction
163
Nonparametric Density Estimation

165

x

Contents

8.3
8.4
8.5
8.6

8.7
8.8
8.9
8.10

8.2.1 Histogram Estimator
165
8.2.2 Kernel Estimator
167
8.2.3 k-Nearest Neighbor Estimator
168
Generalization to Multivariate Data
170
Nonparametric Classiﬁcation
171
Condensed Nearest Neighbor
172
Nonparametric Regression: Smoothing Models
174
8.6.1 Running Mean Smoother
175
8.6.2 Kernel Smoother
176
8.6.3 Running Line Smoother
177
How to Choose the Smoothing Parameter
178
Notes
180
Exercises
181
References
182

9 Decision Trees
9.1
9.2

9.3
9.4
9.5
9.6
9.7
9.8
9.9

185

Introduction
185
Univariate Trees
187
9.2.1 Classiﬁcation Trees
188
9.2.2 Regression Trees
192
Pruning
194
Rule Extraction from Trees
197
Learning Rules from Data
198
Multivariate Trees
202
Notes
204
Exercises
207
References
207

10 Linear Discrimination
10.1
10.2
10.3

10.4
10.5
10.6
10.7

209

Introduction
209
Generalizing the Linear Model
211
Geometry of the Linear Discriminant
10.3.1 Two Classes
212
10.3.2 Multiple Classes
214
Pairwise Separation
216
Parametric Discrimination Revisited
Gradient Descent
218
Logistic Discrimination
220
10.7.1 Two Classes
220

212

217

xi

Contents

10.8
10.9
10.10
10.11

10.7.2 Multiple Classes
224
Discrimination by Regression
228
Notes
230
Exercises
230
References
231

11 Multilayer Perceptrons
11.1

11.2
11.3
11.4
11.5
11.6
11.7

11.8

11.9
11.10
11.11
11.12

11.13
11.14
11.15

Introduction
233
11.1.1 Understanding the Brain
234
11.1.2 Neural Networks as a Paradigm for Parallel
Processing
235
The Perceptron
237
Training a Perceptron
240
Learning Boolean Functions
243
Multilayer Perceptrons
245
MLP as a Universal Approximator
248
Backpropagation Algorithm
249
11.7.1 Nonlinear Regression
250
11.7.2 Two-Class Discrimination
252
11.7.3 Multiclass Discrimination
254
11.7.4 Multiple Hidden Layers
256
Training Procedures
256
11.8.1 Improving Convergence
256
11.8.2 Overtraining
257
11.8.3 Structuring the Network
258
11.8.4 Hints
261
Tuning the Network Size
263
Bayesian View of Learning
266
Dimensionality Reduction
267
Learning Time
270
11.12.1 Time Delay Neural Networks
270
11.12.2 Recurrent Networks
271
Notes
272
Exercises
274
References
275

12 Local Models
12.1
12.2

233

279

Introduction
279
Competitive Learning

280

xii

Contents

12.2.1 Online k-Means
280
12.2.2 Adaptive Resonance Theory
285
12.2.3 Self-Organizing Maps
286
12.3 Radial Basis Functions
288
12.4 Incorporating Rule-Based Knowledge
294
12.5 Normalized Basis Functions
295
12.6 Competitive Basis Functions
297
12.7 Learning Vector Quantization
300
12.8 Mixture of Experts
300
12.8.1 Cooperative Experts
303
12.8.2 Competitive Experts
304
12.9 Hierarchical Mixture of Experts
304
12.10 Notes
305
12.11 Exercises
306
12.12 References
307
13 Kernel Machines
13.1
13.2
13.3
13.4
13.5
13.6
13.7
13.8
13.9
13.10
13.11
13.12
13.13
13.14
13.15

Introduction
309
Optimal Separating Hyperplane
311
The Nonseparable Case: Soft Margin Hyperplane
ν-SVM
318
Kernel Trick
319
Vectorial Kernels
321
Deﬁning Kernels
324
Multiple Kernel Learning
325
Multiclass Kernel Machines
327
Kernel Machines for Regression
328
One-Class Kernel Machines
333
Kernel Dimensionality Reduction
335
Notes
337
Exercises
338
References
339

14 Bayesian Estimation
14.1
14.2

14.3

309

315

341

Introduction
341
Estimating the Parameter of a Distribution
343
14.2.1 Discrete Variables
343
14.2.2 Continuous Variables
345
Bayesian Estimation of the Parameters of a Function

348

xiii

Contents

14.4
14.5
14.6
14.7

14.3.1 Regression
348
14.3.2 The Use of Basis/Kernel Functions
14.3.3 Bayesian Classiﬁcation
353
Gaussian Processes
356
Notes
359
Exercises
360
References
361

15 Hidden Markov Models
15.1
15.2
15.3
15.4
15.5
15.6
15.7
15.8
15.9
15.10
15.11
15.12
15.13

16.4
16.5

16.6
16.7
16.8

363

Introduction
363
Discrete Markov Processes
364
Hidden Markov Models
367
Three Basic Problems of HMMs
369
Evaluation Problem
369
Finding the State Sequence
373
Learning Model Parameters
375
Continuous Observations
378
The HMM with Input
379
Model Selection in HMM
380
Notes
382
Exercises
383
References
384

16 Graphical Models
16.1
16.2
16.3

352

387

Introduction
387
Canonical Cases for Conditional Independence
389
Example Graphical Models
396
16.3.1 Naive Bayes’ Classiﬁer
396
16.3.2 Hidden Markov Model
398
16.3.3 Linear Regression
401
d-Separation
402
Belief Propagation
402
16.5.1 Chains
403
16.5.2 Trees
405
16.5.3 Polytrees
407
16.5.4 Junction Trees
409
Undirected Graphs: Markov Random Fields
410
Learning the Structure of a Graphical Model
413
Inﬂuence Diagrams
414

xiv

Contents

16.9 Notes
414
16.10 Exercises
417
16.11 References
417
17 Combining Multiple Learners
17.1
17.2
17.3
17.4
17.5
17.6
17.7
17.8
17.9
17.10
17.11
17.12
17.13
17.14

Rationale
419
Generating Diverse Learners
420
Model Combination Schemes
423
Voting
424
Error-Correcting Output Codes
427
Bagging
430
Boosting
431
Mixture of Experts Revisited
434
Stacked Generalization
435
Fine-Tuning an Ensemble
437
Cascading
438
Notes
440
Exercises
442
References
443

18 Reinforcement Learning
18.1
18.2
18.3
18.4

419

447

Introduction
447
Single State Case: K-Armed Bandit
449
Elements of Reinforcement Learning
450
Model-Based Learning
453
18.4.1 Value Iteration
453
18.4.2 Policy Iteration
454
18.5 Temporal Diﬀerence Learning
454
18.5.1 Exploration Strategies
455
18.5.2 Deterministic Rewards and Actions
456
18.5.3 Nondeterministic Rewards and Actions
457
18.5.4 Eligibility Traces
459
18.6 Generalization
461
18.7 Partially Observable States
464
18.7.1 The Setting
464
18.7.2 Example: The Tiger Problem
465
18.8 Notes
470
18.9 Exercises
472
18.10 References
473

xv

Contents

19 Design and Analysis of Machine Learning Experiments
19.1
19.2
19.3
19.4
19.5
19.6

19.7
19.8
19.9
19.10

19.11

19.12
19.13

19.14
19.15
19.16

Introduction
475
Factors, Response, and Strategy of Experimentation
478
Response Surface Design
481
Randomization, Replication, and Blocking
482
Guidelines for Machine Learning Experiments
483
Cross-Validation and Resampling Methods
486
19.6.1 K-Fold Cross-Validation
487
19.6.2 5×2 Cross-Validation
488
19.6.3 Bootstrapping
489
Measuring Classiﬁer Performance
489
Interval Estimation
493
Hypothesis Testing
496
Assessing a Classiﬁcation Algorithm’s Performance
498
19.10.1 Binomial Test
499
19.10.2 Approximate Normal Test
500
19.10.3 t Test
500
Comparing Two Classiﬁcation Algorithms
501
19.11.1 McNemar’s Test
501
19.11.2 K-Fold Cross-Validated Paired t Test
501
19.11.3 5 × 2 cv Paired t Test
502
503
19.11.4 5 × 2 cv Paired F Test
Comparing Multiple Algorithms: Analysis of Variance
504
Comparison over Multiple Datasets
508
19.13.1 Comparing Two Algorithms
509
19.13.2 Multiple Algorithms
511
Notes
512
Exercises
513
References
514

A Probability
A.1

A.2

475

517

Elements of Probability
517
A.1.1 Axioms of Probability
518
A.1.2 Conditional Probability
518
Random Variables
519
A.2.1 Probability Distribution and Density Functions
A.2.2 Joint Distribution and Density Functions
520
A.2.3 Conditional Distributions
520
A.2.4 Bayes’ Rule
521

519

xvi

Contents

A.3

A.4
Index

A.2.5 Expectation
521
A.2.6 Variance
522
A.2.7 Weak Law of Large Numbers
523
Special Random Variables
523
A.3.1 Bernoulli Distribution
523
A.3.2 Binomial Distribution
524
A.3.3 Multinomial Distribution
524
A.3.4 Uniform Distribution
524
A.3.5 Normal (Gaussian) Distribution
525
A.3.6 Chi-Square Distribution
526
A.3.7 t Distribution
527
A.3.8 F Distribution
527
References
527
529

Series Foreword

The goal of building systems that can adapt to their environments and
learn from their experience has attracted researchers from many ﬁelds,
including computer science, engineering, mathematics, physics, neuroscience, and cognitive science. Out of this research has come a wide
variety of learning techniques that are transforming many industrial and
scientiﬁc ﬁelds. Recently, several research communities have converged
on a common set of issues surrounding supervised, semi-supervised, unsupervised, and reinforcement learning problems. The MIT Press Series
on Adaptive Computation and Machine Learning seeks to unify the many
diverse strands of machine learning research and to foster high-quality
research and innovative applications.
The MIT Press is extremely pleased to publish this second edition of
Ethem Alpaydın’s introductory textbook. This book presents a readable
and concise introduction to machine learning that reﬂects these diverse
research strands while providing a uniﬁed treatment of the ﬁeld. The
book covers all of the main problem formulations and introduces the
most important algorithms and techniques encompassing methods from
computer science, neural computation, information theory, and statistics. The second edition expands and updates coverage of several areas,
particularly kernel machines and graphical models, that have advanced
rapidly over the past ﬁve years. This updated work continues to be a
compelling textbook for introductory courses in machine learning at the
undergraduate and beginning graduate level.

Figures

1.1

Example of a training dataset where each circle corresponds
to one data instance with input values in the corresponding
axes and its sign indicates the class.

6

1.2

A training dataset of used cars and the function ﬁtted.

10

2.1

Training set for the class of a “family car.”

22

2.2

Example of a hypothesis class.

23

2.3

C is the actual class and h is our induced hypothesis.

25

2.4

S is the most speciﬁc and G is the most general hypothesis.

26

2.5

We choose the hypothesis with the largest margin, for best
separation.

27

2.6

An axis-aligned rectangle can shatter four points.

28

2.7

The diﬀerence between h and C is the sum of four
rectangular strips, one of which is shaded.

30

2.8

When there is noise, there is not a simple boundary
between the positive and negative instances, and zero
misclassiﬁcation error may not be possible with a simple
hypothesis.

31

2.9

There are three classes: family car, sports car, and luxury
sedan.

33

Linear, second-order, and sixth-order polynomials are ﬁtted
to the same set of points.

36

2.11

A line separating positive and negative instances.

44

3.1

Example of decision regions and decision boundaries.

54

2.10

xx

Figures

4.1
4.2
4.3
4.4
4.5
4.6

4.7
4.8

5.1
5.2
5.3
5.4
5.5
5.6
5.7

6.1

6.2
6.3
6.4

θ is the parameter to be estimated.
(a) Likelihood functions and (b) posteriors with equal priors
for two classes when the input is one-dimensional.
(a) Likelihood functions and (b) posteriors with equal priors
for two classes when the input is one-dimensional.
Regression assumes 0 mean Gaussian noise added to the
model; here, the model is linear.
(a) Function, f (x) = 2 sin(1.5x), and one noisy (N (0, 1))
dataset sampled from the function.
In the same setting as that of ﬁgure 4.5, using one hundred
models instead of ﬁve, bias, variance, and error for
polynomials of order 1 to 5.
In the same setting as that of ﬁgure 4.5, training and
validation sets (each containing 50 instances) are generated.
In the same setting as that of ﬁgure 4.5, polynomials of
order 1 to 4 are ﬁtted.

67

Bivariate normal distribution.
Isoprobability contour plot of the bivariate normal
distribution.
Classes have diﬀerent covariance matrices.
Covariances may be arbitary but shared by both classes.
All classes have equal, diagonal covariance matrices, but
variances are not equal.
All classes have equal, diagonal covariance matrices of
equal variances on both dimensions.
Diﬀerent cases of the covariance matrices ﬁtted to the same
data lead to diﬀerent boundaries.

91

Principal components analysis centers the sample and then
rotates the axes to line up with the directions of highest
variance.
(a) Scree graph. (b) Proportion of variance explained is given
for the Optdigits dataset from the UCI Repository.
Optdigits data plotted in the space of two principal
components.
Principal components analysis generates new variables that
are linear combinations of the original input variables.

71
72
74
78

79
81
83

92
96
97
98
99
101

115
117
118
121

xxi

Figures

6.5
6.6
6.7
6.8
6.9

6.10

7.1

7.2
7.3
7.4
7.5

8.1
8.2
8.3
8.4
8.5
8.6
8.7
8.8
8.9
8.10
8.11
8.12

Factors are independent unit normals that are stretched,
rotated, and translated to make up the inputs.
Map of Europe drawn by MDS.
Two-dimensional, two-class data projected on w.
Optdigits data plotted in the space of the ﬁrst two
dimensions found by LDA.
Geodesic distance is calculated along the manifold as
opposed to the Euclidean distance that does not use this
information.
Local linear embedding ﬁrst learns the constraints in the
original space and next places the points in the new space
respecting those constraints.
Given x, the encoder sends the index of the closest code
word and the decoder generates the code word with the
received index as x .
Evolution of k-means.
k-means algorithm.
Data points and the ﬁtted Gaussians by EM, initialized by
one k-means iteration of ﬁgure 7.2.
A two-dimensional dataset and the dendrogram showing
the result of single-link clustering is shown.
Histograms for various bin lengths.
Naive estimate for various bin lengths.
Kernel estimate for various bin lengths.
k-nearest neighbor estimate for various k values.
Dotted lines are the Voronoi tesselation and the straight
line is the class discriminant.
Condensed nearest neighbor algorithm.
Regressograms for various bin lengths. ‘×’ denote data
points.
Running mean smooth for various bin lengths.
Kernel smooth for various bin lengths.
Running line smooth for various bin lengths.
Kernel estimate for various bin lengths for a two-class
problem.
Regressograms with linear ﬁts in bins for various bin lengths.

122
126
129
132

134

136

147
148
149
153
159
166
167
168
169
173
174
175
176
177
178
179
182

xxii

Figures

9.1
9.2
9.3
9.4
9.5
9.6
9.7
9.8

Example of a dataset and the corresponding decision tree.
Entropy function for a two-class problem.
Classiﬁcation tree construction.
Regression tree smooths for various values of θr .
Regression trees implementing the smooths of ﬁgure 9.4
for various values of θr .
Example of a (hypothetical) decision tree.
Ripper algorithm for learning rules.
Example of a linear multivariate decision tree.

In the two-dimensional case, the linear discriminant is a
line that separates the examples from two classes.
10.2 The geometric interpretation of the linear discriminant.
10.3 In linear classiﬁcation, each hyperplane Hi separates the
examples of Ci from the examples of all other classes.
10.4 In pairwise linear separation, there is a separate hyperplane
for each pair of classes.
10.5 The logistic, or sigmoid, function.
10.6 Logistic discrimination algorithm implementing gradient
descent for the single output case with two classes.
10.7 For a univariate two-class problem (shown with ‘◦’ and ‘×’ ),
the evolution of the line w x + w0 and the sigmoid output
after 10, 100, and 1,000 iterations over the sample.
10.8 Logistic discrimination algorithm implementing gradient
descent for the case with K > 2 classes.
10.9 For a two-dimensional problem with three classes, the
solution found by logistic discrimination.
10.10 For the same example in ﬁgure 10.9, the linear
discriminants (top), and the posterior probabilities after the
softmax (bottom).

186
189
191
195
196
197
200
203

10.1

11.1
11.2
11.3
11.4
11.5
11.6

Simple perceptron.
K parallel perceptrons.
Perceptron training algorithm implementing stochastic
online gradient descent for the case with K > 2 classes.
The perceptron that implements AND and its geometric
interpretation.
XOR problem is not linearly separable.
The structure of a multilayer perceptron.

213
214
215
216
219
222

223
226
226

227
237
239
243
244
245
247

xxiii

Figures

11.7
11.8
11.9
11.10

11.11
11.12

11.13
11.14
11.15

11.16
11.17
11.18
11.19
11.20
11.21
11.22
12.1
12.2
12.3

12.4

The multilayer perceptron that solves the XOR problem.
Sample training data shown as ‘+’, where xt ∼ U(−0.5, 0.5),
and y t = f (xt ) + N (0, 0.1).
The mean square error on training and validation sets as a
function of training epochs.
(a) The hyperplanes of the hidden unit weights on the ﬁrst
layer, (b) hidden unit outputs, and (c) hidden unit outputs
multiplied by the weights on the second layer.
Backpropagation algorithm for training a multilayer
perceptron for regression with K outputs.
As complexity increases, training error is ﬁxed but the
validation error starts to increase and the network starts to
overﬁt.
As training continues, the validation error starts to increase
and the network starts to overﬁt.
A structured MLP.
In weight sharing, diﬀerent units have connections to
diﬀerent inputs but share the same weight value (denoted
by line type).
The identity of the object does not change when it is
translated, rotated, or scaled.
Two examples of constructive algorithms.
Optdigits data plotted in the space of the two hidden units
of an MLP trained for classiﬁcation.
In the autoassociator, there are as many outputs as there
are inputs and the desired outputs are the inputs.
A time delay neural network.
Examples of MLP with partial recurrency.
Backpropagation through time.
Shaded circles are the centers and the empty circle is the
input instance.
Online k-means algorithm.
The winner-take-all competitive neural network, which is a
network of k perceptrons with recurrent connections at the
output.
The distance from x a to the closest center is less than the
vigilance value ρ and the center is updated as in online
k-means.

249
252
253

254
255

259
259
260

261
262
265
268
269
271
272
273

282
283

284

285

xxiv

Figures

12.5

In the SOM, not only the closest unit but also its neighbors,
in terms of indices, are moved toward the input.
12.6 The one-dimensional form of the bell-shaped function used
in the radial basis function network.
12.7 The diﬀerence between local and distributed representations.
12.8 The RBF network where ph are the hidden units using the
bell-shaped activation function.
12.9 (-) Before and (- -) after normalization for three Gaussians
whose centers are denoted by ‘*’.
12.10 The mixture of experts can be seen as an RBF network
where the second-layer weights are outputs of linear models.
12.11 The mixture of experts can be seen as a model for
combining multiple models.
For a two-class problem where the instances of the classes
are shown by plus signs and dots, the thick line is the
boundary and the dashed lines deﬁne the margins on either
side.
13.2 In classifying an instance, there are four possible cases.
13.3 Comparison of diﬀerent loss functions for r t = 1.
13.4 The discriminant and margins found by a polynomial
kernel of degree 2.
13.5 The boundary and margins found by the Gaussian kernel
with diﬀerent spread values, s 2 .
13.6 Quadratic and -sensitive error functions.
13.7 The ﬁtted regression line to data points shown as crosses
and the -tube are shown (C = 10, = 0.25).
13.8 The ﬁtted regression line and the -tube using a quadratic
kernel are shown (C = 10, = 0.25).
13.9 The ﬁtted regression line and the -tube using a Gaussian
kernel with two diﬀerent spreads are shown
(C = 10, = 0.25).
13.10 One-class support vector machine places the smoothest
boundary (here using a linear kernel, the circle with the
smallest radius) that encloses as much of the instances as
possible.
13.11 One-class support vector machine using a Gaussian kernel
with diﬀerent spreads.

287
289
290
292
296
301
302

13.1

314
316
318
322
323
329
331
332

332

334
336

Figures

xxv

13.12 Instead of using a quadratic kernel in the original space (a),
we can use kernel PCA on the quadratic kernel values to
map to a two-dimensional new space where we use a linear
discriminant (b); these two dimensions (out of ﬁve) explain
80 percent of the variance.

337

14.1
14.2
14.3

14.4
14.5
14.6
14.7

15.1
15.2
15.3
15.4
15.5
16.1
16.2
16.3
16.4
16.5

16.6
16.7

The generative graphical model.
Plots of beta distributions for diﬀerent sets of (α, β).
20 data points are drawn from p(x) ∼ N (6, 1.52 ), prior is
p(μ) ∼ N (4, 0.82 ), and posterior is then
p(μ|X) ∼ N (5.7, 0.32 ).
Bayesian linear regression for diﬀerent values of α and β.
Bayesian regression using kernels with one standard
deviation error bars.
Gaussian process regression with one standard deviation
error bars.
Gaussian process regression using a Gaussian kernel with
s 2 = 0.5 and varying number of training data.
Example of a Markov model with three states.
An HMM unfolded in time as a lattice (or trellis) showing all
the possible trajectories.
Forward-backward procedure.
Computation of arc probabilities, ξt (i, j).
Example of a left-to-right HMM.
Bayesian network modeling that rain is the cause of wet
grass.
Head-to-tail connection.
Tail-to-tail connection.
Head-to-head connection.
Larger graphs are formed by combining simpler subgraphs
over which information is propagated using the implied
conditional independencies.
(a) Graphical model for classiﬁcation. (b) Naive Bayes’
classiﬁer assumes independent inputs.
Hidden Markov model can be drawn as a graphical model
where q t are the hidden states and shaded O t are observed.

342
346

347
351
354
357
359
365
368
371
375
381

388
390
391
392

394
397
398

xxvi

Figures

16.8

16.9
16.10
16.11
16.12
16.13

16.14
16.15

16.16
16.17

17.1
17.2
17.3
17.4

17.5

18.1
18.2
18.3
18.4
18.5
18.6
18.7

Diﬀerent types of HMM model diﬀerent assumptions about
the way the observed data (shown shaded) is generated
from Markov sequences of latent variables.
399
Bayesian network for linear regression.
401
Examples of d-separation.
403
Inference along a chain.
404
In a tree, a node may have several children but a single parent. 406
In a polytree, a node may have several children and several
parents, but the graph is singly connected; that is, there is a
single chain between Ui and Yj passing through X.
407
(a) A multiply connected graph, and (b) its corresponding
junction tree with nodes clustered.
410
(a) A directed graph that would have a loop after
moralization, and (b) its corresponding factor graph that is
a tree.
412
Inﬂuence diagram corresponding to classiﬁcation.
415
A dynamic version where we have a chain of graphs to
show dependency in weather in consecutive days.
416
Base-learners are dj and their outputs are combined using
f (·).
AdaBoost algorithm.
Mixture of experts is a voting method where the votes, as
given by the gating system, are a function of the input.
In stacked generalization, the combiner is another learner
and is not restricted to being a linear combination as in
voting.
Cascading is a multistage method where there is a sequence
of classiﬁers, and the next one is used only when the
preceding ones are not conﬁdent.
The agent interacts with an environment.
Value iteration algorithm for model-based learning.
Policy iteration algorithm for model-based learning.
Example to show that Q values increase but never decrease.
Q learning, which is an oﬀ-policy temporal diﬀerence
algorithm.
Sarsa algorithm, which is an on-policy version of Q learning.
Example of an eligibility trace for a value.

424
432
434

436

439
448
453
454
457
458
459
460

xxvii

Figures

18.8
18.9

Sarsa(λ) algorithm.
In the case of a partially observable environment, the agent
has a state estimator (SE) that keeps an internal belief state
b and the policy π generates actions based on the belief
states.
18.10 Expected rewards and the eﬀect of sensing in the Tiger
problem.
18.11 Expected rewards change (a) if the hidden state can change,
and (b) when we consider episodes of length two.
18.12 The grid world.
19.1
19.2
19.3
19.4

19.5
19.6
A.1

The process generates an output given an input and is
aﬀected by controllable and uncontrollable factors.
Diﬀerent strategies of experimentation with two factors
and ﬁve levels each.
(a) Typical ROC curve. (b) A classiﬁer is preferred if its ROC
curve is closer to the upper-left corner (larger AUC).
(a) Deﬁnition of precision and recall using Venn diagrams.
(b) Precision is 1; all the retrieved records are relevant but
there may be relevant ones not retrieved. (c) Recall is 1; all
the relevant records are retrieved but there may also be
irrelevant records that are retrieved.
95 percent of the unit normal distribution lies between
−1.96 and 1.96.
95 percent of the unit normal distribution lies before 1.64.
Probability density function of Z, the unit normal
distribution.

461

465
468
470
472

479
480
491

492
494
496

525

Tables

2.1

With two inputs, there are four possible cases and sixteen
possible Boolean functions.

37

5.1

Reducing variance through simplifying assumptions.

100

11.1
11.2

Input and output for the AND function.
Input and output for the XOR function.

244
245

17.1
17.2

Classiﬁer combination rules.
Example of combination rules on three learners and three
classes.

425

19.1
19.2
19.3
19.4

Confusion matrix for two classes.
Performance measures used in two-class problems.
Type I error, type II error, and power of a test.
The analysis of variance (ANOVA) table for a single factor
model.

425
489
490
497
507

Preface

Machine learning is programming computers to optimize a performance
criterion using example data or past experience. We need learning in
cases where we cannot directly write a computer program to solve a given
problem, but need example data or experience. One case where learning
is necessary is when human expertise does not exist, or when humans
are unable to explain their expertise. Consider the recognition of spoken
speech—that is, converting the acoustic speech signal to an ASCII text;
we can do this task seemingly without any diﬃculty, but we are unable
to explain how we do it. Diﬀerent people utter the same word diﬀerently
due to diﬀerences in age, gender, or accent. In machine learning, the approach is to collect a large collection of sample utterances from diﬀerent
people and learn to map these to words.
Another case is when the problem to be solved changes in time, or
depends on the particular environment. We would like to have generalpurpose systems that can adapt to their circumstances, rather than explicitly writing a diﬀerent program for each special circumstance. Consider routing packets over a computer network. The path maximizing
the quality of service from a source to destination changes continuously
as the network traﬃc changes. A learning routing program is able to
adapt to the best path by monitoring the network traﬃc. Another example is an intelligent user interface that can adapt to the biometrics of
its user—namely, his or her accent, handwriting, working habits, and so
forth.
Already, there are many successful applications of machine learning
in various domains: There are commercially available systems for recognizing speech and handwriting. Retail companies analyze their past
sales data to learn their customers’ behavior to improve customer rela-

xxxii

Preface

tionship management. Financial institutions analyze past transactions
to predict customers’ credit risks. Robots learn to optimize their behavior to complete a task using minimum resources. In bioinformatics, the
huge amount of data can only be analyzed and knowledge extracted using computers. These are only some of the applications that we—that
is, you and I—will discuss throughout this book. We can only imagine
what future applications can be realized using machine learning: Cars
that can drive themselves under diﬀerent road and weather conditions,
phones that can translate in real time to and from a foreign language,
autonomous robots that can navigate in a new environment, for example,
on the surface of another planet. Machine learning is certainly an exciting
ﬁeld to be working in!
The book discusses many methods that have their bases in diﬀerent
ﬁelds: statistics, pattern recognition, neural networks, artiﬁcial intelligence, signal processing, control, and data mining. In the past, research
in these diﬀerent communities followed diﬀerent paths with diﬀerent
emphases. In this book, the aim is to incorporate them together to give a
uniﬁed treatment of the problems and the proposed solutions to them.
This is an introductory textbook, intended for senior undergraduate
and graduate-level courses on machine learning, as well as engineers
working in the industry who are interested in the application of these
methods. The prerequisites are courses on computer programming, probability, calculus, and linear algebra. The aim is to have all learning algorithms suﬃciently explained so it will be a small step from the equations
given in the book to a computer program. For some cases, pseudocode
of algorithms are also included to make this task easier.
The book can be used for a one-semester course by sampling from the
chapters, or it can be used for a two-semester course, possibly by discussing extra research papers; in such a case, I hope that the references
at the end of each chapter are useful.
The Web page is http://www.cmpe.boun.edu.tr/∼ethem/i2ml/ where I
will post information related to the book that becomes available after the
book goes to press, for example, errata. I welcome your feedback via
email to alpaydin@boun.edu.tr.
I very much enjoyed writing this book; I hope you will enjoy reading it.

Acknowledgments

The way you get good ideas is by working with talented people who are
also fun to be with. The Department of Computer Engineering of Bo˘aziçi
g
University is a wonderful place to work, and my colleagues gave me all the
support I needed while working on this book. I would also like to thank
my past and present students on whom I have ﬁeld-tested the content
that is now in book form.
While working on this book, I was supported by the Turkish Academy
of Sciences, in the framework of the Young Scientist Award Program (EATÜBA-GEB˙
IP/2001-1-1).
My special thanks go to Michael Jordan. I am deeply indebted to him
for his support over the years and last for this book. His comments on
the general organization of the book, and the ﬁrst chapter, have greatly
improved the book, both in content and form. Taner Bilgiç, Vladimir
Cherkassky, Tom Dietterich, Fikret Gürgen, Olcay Taner Yıldız, and anonymous reviewers of the MIT Press also read parts of the book and provided
invaluable feedback. I hope that they will sense my gratitude when they
notice ideas that I have taken from their comments without proper acknowledgment. Of course, I alone am responsible for any errors or shortcomings.
My parents believe in me, and I am grateful for their enduring love
and support. Sema Oktu˘ is always there whenever I need her, and I will
g
always be thankful for her friendship. I would also like to thank Hakan
Ünlü for our many discussions over the years on several topics related to
life, the universe, and everything.
A
This book is set using L TEX macros prepared by Chris Manning for
which I thank him. I would like to thank the editors of the Adaptive Computation and Machine Learning series, and Bob Prior, Valerie Geary, Kath-

xxxiv

Acknowledgments

leen Caruso, Sharon Deacon Warne, Erica Schultz, and Emily Gutheinz
from the MIT Press for their continuous support and help during the
completion of the book.

Notes for the Second Edition

Machine learning has seen important developments since the ﬁrst edition
appeared in 2004. First, application areas have grown rapidly. Internetrelated technologies, such as search engines, recommendation systems,
spam ﬁters, and intrusion detection systems are now routinely using machine learning. In the ﬁeld of bioinformatics and computational biology,
methods that learn from data are being used more and more widely. In
natural language processing applications—for example, machine translation—we are seeing a faster and faster move from programmed expert
systems to methods that learn automatically from very large corpus of
example text. In robotics, medical diagnosis, speech and image recognition, biometrics, ﬁnance, sometimes under the name pattern recognition,
sometimes disguised as data mining, or under one of its many cloaks,
we see more and more applications of the machine learning methods we
discuss in this textbook.
Second, there have been supporting advances in theory. Especially, the
idea of kernel functions and the kernel machines that use them allow
a better representation of the problem and the associated convex optimization framework is one step further than multilayer perceptrons with
sigmoid hidden units trained using gradient-descent. Bayesian methods through appropriately chosen prior distributions add expert knowledge to what the data tells us. Graphical models allow a representation as a network of interrelated nodes and eﬃcient inference algorithms
allow querying the network. It has thus become necessary that these
three topics—namely, kernel methods, Bayesian estimation, and graphical models—which were sections in the ﬁrst edition, be treated in more
length, as three new chapters.
Another revelation hugely signiﬁcant for the ﬁeld has been in the real-

xxxvi

Notes for the Second Edition

ization that machine learning experiments need to be designed better. We
have gone a long way from using a single test set to methods for crossvalidation to paired t tests. That is why, in this second edition, I have
rewritten the chapter on statistical tests as one that includes the design
and analysis of machine learning experiments. The point is that testing
should not be a separate step done after all runs are completed (despite
the fact that this new chapter is at the very end of the book); the whole
process of experimentation should be designed beforehand, relevant factors deﬁned, proper experimentation procedure decided upon, and then,
and only then, the runs should be done and the results analyzed.
It has long been believed, especially by older members of the scientiﬁc
community, that for machines to be as intelligent as us, that is, for artiﬁcial intelligence to be a reality, our current knowledge in general, or
computer science in particular, is not suﬃcient. People largely are of
the opinion that we need a new technology, a new type of material, a
new type of computational mechanism or a new programming methodology, and that, until then, we can only “simulate” some aspects of human
intelligence and only in a limited way but can never fully attain it.
I believe that we will soon prove them wrong. First we saw this in
chess, and now we are seeing it in a whole variety of domains. Given
enough memory and computation power, we can realize tasks with relatively simple algorithms; the trick here is learning, either learning from
example data or learning from trial and error using reinforcement learning. It seems as if using supervised and mostly unsupervised learning algorithms—for example, machine translation—will soon be possible.
The same holds for many other domains, for example, unmanned navigation in robotics using reinforcement learning. I believe that this will
continue for many domains in artiﬁcial intelligence, and the key is learning. We do not need to come up with new algorithms if machines can
learn themselves, assuming that we can provide them with enough data
(not necessarily supervised) and computing power.
I would like to thank all the instructors and students of the ﬁrst edition,
from all over the world, including the reprint in India and the German
translation. I am grateful to those who sent me words of appreciation
and errata or who provided feedback in any other way. Please keep those
emails coming. My email address is alpaydin@boun.edu.tr.
The second edition also provides more support on the Web. The book’s

Notes for the Second Edition

xxxvii

Web site is http://www.cmpe.boun.edu.tr/∼ethem/i2ml.
I would like to thank my past and present thesis students, Mehmet Gönen,
Esma Kılıç, Murat Semerci, M. Aydın Ula¸, and Olcay Taner Yıldız, and also
s
those who have taken CmpE 544, CmpE 545, CmpE 591, and CmpE 58E
during these past few years. The best way to test your knowledge of a
topic is by teaching it.
It has been a pleasure working with the MIT Press again on this second
edition, and I thank Bob Prior, Ada Brunstein, Erin K. Shoudy, Kathleen
Caruso, and Marcy Ross for all their help and support.

Notations

x

Scalar value

x

Vector

X

Matrix

x

T

Transpose

−1

X

Inverse

X

Random variable

P (X)

Probability mass function when X is discrete

p(X)

Probability density function when X is continuous

P (X|Y )

Conditional probability of X given Y

E[X]

Expected value of the random variable X

Var(X)

Variance of X

Cov(X, Y )

Covariance of X and Y

Corr(X, Y )

Correlation of X and Y

μ

Mean

σ

2

Variance

Σ

Covariance matrix

m

Estimator to the mean

s2

Estimator to the variance

S

Estimator to the covariance matrix

xl

Notations

N (μ, σ 2 )

Univariate normal distribution with mean μ and variance σ 2

Z

Unit normal distribution: N (0, 1)

Nd (μ, Σ)

d-variate normal distribution with mean vector μ and
covariance matrix Σ

x

Input

d

Number of inputs (input dimensionality)

y

Output

r

Required output

K

Number of outputs (classes)

N

Number of training instances

z

Hidden value, intrinsic dimension, latent factor

k

Number of hidden dimensions, latent factors

Ci

Class i

X

Training sample

{xt }N
t=1

Set of x with index t ranging from 1 to N

{x , r }t

Set of ordered pairs of input and desired output with
index t

g(x|θ)

Function of x deﬁned up to a set of parameters θ

t

t

arg maxθ g(x|θ) The argument θ for which g has its maximum value
arg minθ g(x|θ) The argument θ for which g has its minimum value
E(θ|X)

Error function with parameters θ on the sample X

l(θ|X)

Likelihood of parameters θ on the sample X

L(θ|X)

Log likelihood of parameters θ on the sample X

1(c)

1 if c is true, 0 otherwise

#{c}

Number of elements for which c is true

δij

Kronecker delta: 1 if i = j, 0 otherwise

1
1.1

Introduction

What Is Machine Learning?
To s ol v e a problem on a computer, we need an algorithm. An algorithm is a sequence of instructions that should be carried out to transform the input to output. For example, one can devise an algorithm for
sorting. The input is a set of numbers and the output is their ordered
list. For the same task, there may be various algorithms and we may be
interested in ﬁnding the most eﬃcient one, requiring the least number of
instructions or memory or both.
For some tasks, however, we do not have an algorithm—for example,
to tell spam emails from legitimate emails. We know what the input is:
an email document that in the simplest case is a ﬁle of characters. We
know what the output should be: a yes/no output indicating whether the
message is spam or not. We do not know how to transform the input
to the output. What can be considered spam changes in time and from
individual to individual.
What we lack in knowledge, we make up for in data. We can easily
compile thousands of example messages some of which we know to be
spam and what we want is to “learn” what consititutes spam from them.
In other words, we would like the computer (machine) to extract automatically the algorithm for this task. There is no need to learn to sort
numbers, we already have algorithms for that; but there are many applications for which we do not have an algorithm but do have example
data.
With advances in computer technology, we currently have the ability to
store and process large amounts of data, as well as to access it from physically distant locations over a computer network. Most data acquisition

2

1

Introduction

devices are digital now and record reliable data. Think, for example, of a
supermarket chain that has hundreds of stores all over a country selling
thousands of goods to millions of customers. The point of sale terminals
record the details of each transaction: date, customer identiﬁcation code,
goods bought and their amount, total money spent, and so forth. This
typically amounts to gigabytes of data every day. What the supermarket
chain wants is to be able to predict who are the likely customers for a
product. Again, the algorithm for this is not evident; it changes in time
and by geographic location. The stored data becomes useful only when
it is analyzed and turned into information that we can make use of, for
example, to make predictions.
We do not know exactly which people are likely to buy this ice cream
ﬂavor, or the next book of this author, or see this new movie, or visit this
city, or click this link. If we knew, we would not need any analysis of the
data; we would just go ahead and write down the code. But because we
do not, we can only collect data and hope to extract the answers to these
and similar questions from data.
We do believe that there is a process that explains the data we observe.
Though we do not know the details of the process underlying the generation of data—for example, consumer behavior—we know that it is not
completely random. People do not go to supermarkets and buy things
at random. When they buy beer, they buy chips; they buy ice cream in
summer and spices for Glühwein in winter. There are certain patterns in
the data.
We may not be able to identify the process completely, but we believe
we can construct a good and useful approximation. That approximation
may not explain everything, but may still be able to account for some part
of the data. We believe that though identifying the complete process may
not be possible, we can still detect certain patterns or regularities. This
is the niche of machine learning. Such patterns may help us understand
the process, or we can use those patterns to make predictions: Assuming
that the future, at least the near future, will not be much diﬀerent from
the past when the sample data was collected, the future predictions can
also be expected to be right.
Application of machine learning methods to large databases is called
data mining. The analogy is that a large volume of earth and raw material is extracted from a mine, which when processed leads to a small
amount of very precious material; similarly, in data mining, a large volume of data is processed to construct a simple model with valuable use,

1.1 What Is Machine Learning?

3

for example, having high predictive accuracy. Its application areas are
abundant: In addition to retail, in ﬁnance banks analyze their past data
to build models to use in credit applications, fraud detection, and the
stock market. In manufacturing, learning models are used for optimization, control, and troubleshooting. In medicine, learning programs are
used for medical diagnosis. In telecommunications, call patterns are analyzed for network optimization and maximizing the quality of service.
In science, large amounts of data in physics, astronomy, and biology can
only be analyzed fast enough by computers. The World Wide Web is huge;
it is constantly growing, and searching for relevant information cannot be
done manually.
But machine learning is not just a database problem; it is also a part
of artiﬁcial intelligence. To be intelligent, a system that is in a changing
environment should have the ability to learn. If the system can learn and
adapt to such changes, the system designer need not foresee and provide
solutions for all possible situations.
Machine learning also helps us ﬁnd solutions to many problems in vision, speech recognition, and robotics. Let us take the example of recognizing faces: This is a task we do eﬀortlessly; every day we recognize
family members and friends by looking at their faces or from their photographs, despite diﬀerences in pose, lighting, hair style, and so forth.
But we do it unconsciously and are unable to explain how we do it. Because we are not able to explain our expertise, we cannot write the computer program. At the same time, we know that a face image is not just a
random collection of pixels; a face has structure. It is symmetric. There
are the eyes, the nose, the mouth, located in certain places on the face.
Each person’s face is a pattern composed of a particular combination
of these. By analyzing sample face images of a person, a learning program captures the pattern speciﬁc to that person and then recognizes by
checking for this pattern in a given image. This is one example of pattern
recognition.
Machine learning is programming computers to optimize a performance
criterion using example data or past experience. We have a model deﬁned
up to some parameters, and learning is the execution of a computer program to optimize the parameters of the model using the training data or
past experience. The model may be predictive to make predictions in the
future, or descriptive to gain knowledge from data, or both.
Machine learning uses the theory of statistics in building mathematical
models, because the core task is making inference from a sample. The

4

1

Introduction

role of computer science is twofold: First, in training, we need eﬃcient
algorithms to solve the optimization problem, as well as to store and process the massive amount of data we generally have. Second, once a model
is learned, its representation and algorithmic solution for inference needs
to be eﬃcient as well. In certain applications, the eﬃciency of the learning or inference algorithm, namely, its space and time complexity, may
be as important as its predictive accuracy.
Let us now discuss some example applications in more detail to gain
more insight into the types and uses of machine learning.

1.2
1.2.1

association rule

Examples of Machine Learning Applications
Learning Associations
In the case of retail—for example, a supermarket chain—one application
of machine learning is basket analysis, which is ﬁnding associations between products bought by customers: If people who buy X typically also
buy Y , and if there is a customer who buys X and does not buy Y , he
or she is a potential Y customer. Once we ﬁnd such customers, we can
target them for cross-selling.
In ﬁnding an association rule, we are interested in learning a conditional
probability of the form P (Y |X) where Y is the product we would like to
condition on X, which is the product or the set of products which we
know that the customer has already purchased.
Let us say, going over our data, we calculate that P (chips|beer) = 0.7.
Then, we can deﬁne the rule:
70 percent of customers who buy beer also buy chips.
We may want to make a distinction among customers and toward this,
estimate P (Y |X, D) where D is the set of customer attributes, for example, gender, age, marital status, and so on, assuming that we have access
to this information. If this is a bookseller instead of a supermarket, products can be books or authors. In the case of a Web portal, items correspond to links to Web pages, and we can estimate the links a user is likely
to click and use this information to download such pages in advance for
faster access.

1.2 Examples of Machine Learning Applications

1.2.2

classification

5

Classiﬁcation
A credit is an amount of money loaned by a ﬁnancial institution, for
example, a bank, to be paid back with interest, generally in installments.
It is important for the bank to be able to predict in advance the risk
associated with a loan, which is the probability that the customer will
default and not pay the whole amount back. This is both to make sure
that the bank will make a proﬁt and also to not inconvenience a customer
with a loan over his or her ﬁnancial capacity.
In credit scoring (Hand 1998), the bank calculates the risk given the
amount of credit and the information about the customer. The information about the customer includes data we have access to and is relevant in
calculating his or her ﬁnancial capacity—namely, income, savings, collaterals, profession, age, past ﬁnancial history, and so forth. The bank has
a record of past loans containing such customer data and whether the
loan was paid back or not. From this data of particular applications, the
aim is to infer a general rule coding the association between a customer’s
attributes and his risk. That is, the machine learning system ﬁts a model
to the past data to be able to calculate the risk for a new application and
then decides to accept or refuse it accordingly.
This is an example of a classiﬁcation problem where there are two
classes: low-risk and high-risk customers. The information about a customer makes up the input to the classiﬁer whose task is to assign the
input to one of the two classes.
After training with the past data, a classiﬁcation rule learned may be
of the form
IF income> θ1 AND savings> θ2 THEN low-risk ELSE high-risk

discriminant

prediction

for suitable values of θ1 and θ2 (see ﬁgure 1.1). This is an example of
a discriminant; it is a function that separates the examples of diﬀerent
classes.
Having a rule like this, the main application is prediction: Once we have
a rule that ﬁts the past data, if the future is similar to the past, then we
can make correct predictions for novel instances. Given a new application
with a certain income and savings, we can easily decide whether it is lowrisk or high-risk.
In some cases, instead of making a 0/1 (low-risk/high-risk) type decision, we may want to calculate a probability, namely, P (Y |X), where
X are the customer attributes and Y is 0 or 1 respectively for low-risk

6

Savings

1

θ2

Introduction

Low-Risk

High-Risk

θ1

Income

Figure 1.1 Example of a training dataset where each circle corresponds to one
data instance with input values in the corresponding axes and its sign indicates
the class. For simplicity, only two customer attributes, income and savings,
are taken as input and the two classes are low-risk (‘+’) and high-risk (‘−’). An
example discriminant that separates the two types of examples is also shown.

pattern
recognition

and high-risk. From this perspective, we can see classiﬁcation as learning an association from X to Y . Then for a given X = x, if we have
P (Y = 1|X = x) = 0.8, we say that the customer has an 80 percent probability of being high-risk, or equivalently a 20 percent probability of being
low-risk. We then decide whether to accept or refuse the loan depending
on the possible gain and loss.
There are many applications of machine learning in pattern recognition.
One is optical character recognition, which is recognizing character codes
from their images. This is an example where there are multiple classes,
as many as there are characters we would like to recognize. Especially interesting is the case when the characters are handwritten—for example,
to read zip codes on envelopes or amounts on checks. People have diﬀerent handwriting styles; characters may be written small or large, slanted,
with a pen or pencil, and there are many possible images corresponding

1.2 Examples of Machine Learning Applications

7

to the same character. Though writing is a human invention, we do not
have any system that is as accurate as a human reader. We do not have a
formal description of ‘A’ that covers all ‘A’s and none of the non-‘A’s. Not
having it, we take samples from writers and learn a deﬁnition of A-ness
from these examples. But though we do not know what it is that makes
an image an ‘A’, we are certain that all those distinct ‘A’s have something
in common, which is what we want to extract from the examples. We
know that a character image is not just a collection of random dots; it
is a collection of strokes and has a regularity that we can capture by a
learning program.
If we are reading a text, one factor we can make use of is the redundancy in human languages. A word is a sequence of characters and successive characters are not independent but are constrained by the words
of the language. This has the advantage that even if we cannot recognize
a character, we can still read t?e word. Such contextual dependencies
may also occur in higher levels, between words and sentences, through
the syntax and semantics of the language. There are machine learning
algorithms to learn sequences and model such dependencies.
In the case of face recognition, the input is an image, the classes are
people to be recognized, and the learning program should learn to associate the face images to identities. This problem is more diﬃcult than
optical character recognition because there are more classes, input image is larger, and a face is three-dimensional and diﬀerences in pose and
lighting cause signiﬁcant changes in the image. There may also be occlusion of certain inputs; for example, glasses may hide the eyes and
eyebrows, and a beard may hide the chin.
In medical diagnosis, the inputs are the relevant information we have
about the patient and the classes are the illnesses. The inputs contain the
patient’s age, gender, past medical history, and current symptoms. Some
tests may not have been applied to the patient, and thus these inputs
would be missing. Tests take time, may be costly, and may inconvience
the patient so we do not want to apply them unless we believe that they
will give us valuable information. In the case of a medical diagnosis, a
wrong decision may lead to a wrong or no treatment, and in cases of
doubt it is preferable that the classiﬁer reject and defer decision to a
human expert.
In speech recognition, the input is acoustic and the classes are words
that can be uttered. This time the association to be learned is from an
acoustic signal to a word of some language. Diﬀerent people, because

8

1

knowledge
extraction

compression

Introduction

of diﬀerences in age, gender, or accent, pronounce the same word diﬀerently, which makes this task rather diﬃcult. Another diﬀerence of speech
is that the input is temporal; words are uttered in time as a sequence of
speech phonemes and some words are longer than others.
Acoustic information only helps up to a certain point, and as in optical
character recognition, the integration of a “language model” is critical in
speech recognition, and the best way to come up with a language model
is again by learning it from some large corpus of example data. The applications of machine learning to natural language processing is constantly
increasing. Spam ﬁltering is one where spam generators on one side and
ﬁlters on the other side keep ﬁnding more and more ingenious ways to
outdo each other. Perhaps the most impressive would be machine translation. After decades of research on hand-coded translation rules, it has
become apparent recently that the most promising way is to provide a
very large number of example pairs of translated texts and have a program ﬁgure out automatically the rules to map one string of characters
to another.
Biometrics is recognition or authentication of people using their physiological and/or behavioral characteristics that requires an integration of
inputs from diﬀerent modalities. Examples of physiological characteristics are images of the face, ﬁngerprint, iris, and palm; examples of behavioral characteristics are dynamics of signature, voice, gait, and key stroke.
As opposed to the usual identiﬁcation procedures—photo, printed signature, or password—when there are many diﬀerent (uncorrelated) inputs,
forgeries (spooﬁng) would be more diﬃcult and the system would be
more accurate, hopefully without too much inconvenience to the users.
Machine learning is used both in the separate recognizers for these diﬀerent modalities and in the combination of their decisions to get an overall
accept/reject decision, taking into account how reliable these diﬀerent
sources are.
Learning a rule from data also allows knowledge extraction. The rule is
a simple model that explains the data, and looking at this model we have
an explanation about the process underlying the data. For example, once
we learn the discriminant separating low-risk and high-risk customers,
we have the knowledge of the properties of low-risk customers. We can
then use this information to target potential low-risk customers more
eﬃciently, for example, through advertising.
Learning also performs compression in that by ﬁtting a rule to the data,
we get an explanation that is simpler than the data, requiring less mem-

1.2 Examples of Machine Learning Applications

outlier detection

1.2.3

regression

9

ory to store and less computation to process. Once you have the rules of
addition, you do not need to remember the sum of every possible pair of
numbers.
Another use of machine learning is outlier detection, which is ﬁnding
the instances that do not obey the rule and are exceptions. In this case,
after learning the rule, we are not interested in the rule but the exceptions
not covered by the rule, which may imply anomalies requiring attention—
for example, fraud.

Regression
Let us say we want to have a system that can predict the price of a used
car. Inputs are the car attributes—brand, year, engine capacity, mileage,
and other information—that we believe aﬀect a car’s worth. The output
is the price of the car. Such problems where the output is a number are
regression problems.
Let X denote the car attributes and Y be the price of the car. Again
surveying the past transactions, we can collect a training data and the
machine learning program ﬁts a function to this data to learn Y as a
function of X. An example is given in ﬁgure 1.2 where the ﬁtted function
is of the form
y = w x + w0

supervised learning

for suitable values of w and w0 .
Both regression and classiﬁcation are supervised learning problems
where there is an input, X, an output, Y , and the task is to learn the mapping from the input to the output. The approach in machine learning is
that we assume a model deﬁned up to a set of parameters:
y = g(x|θ)
where g(·) is the model and θ are its parameters. Y is a number in regression and is a class code (e.g., 0/1) in the case of classiﬁcation. g(·)
is the regression function or in classiﬁcation, it is the discriminant function separating the instances of diﬀerent classes. The machine learning
program optimizes the parameters, θ, such that the approximation error
is minimized, that is, our estimates are as close as possible to the correct values given in the training set. For example in ﬁgure 1.2, the model
is linear and w and w0 are the parameters optimized for best ﬁt to the

10

Introduction

y: price

1

x: mileage

Figure 1.2 A training dataset of used cars and the function ﬁtted. For simplicity, mileage is taken as the only input attribute and a linear model is used.

training data. In cases where the linear model is too restrictive, one can
use for example a quadratic
y = w2 x2 + w1 x + w0
or a higher-order polynomial, or any other nonlinear function of the input, this time optimizing its parameters for best ﬁt.
Another example of regression is navigation of a mobile robot, for example, an autonomous car, where the output is the angle by which the
steering wheel should be turned at each time, to advance without hitting
obstacles and deviating from the route. Inputs in such a case are provided by sensors on the car—for example, a video camera, GPS, and so
forth. Training data can be collected by monitoring and recording the
actions of a human driver.
One can envisage other applications of regression where one is trying

1.2 Examples of Machine Learning Applications

11

to optimize a function1 . Let us say we want to build a machine that roasts
coﬀee. The machine has many inputs that aﬀect the quality: various
settings of temperatures, times, coﬀee bean type, and so forth. We make
a number of experiments and for diﬀerent settings of these inputs, we
measure the quality of the coﬀee, for example, as consumer satisfaction.
To ﬁnd the optimal setting, we ﬁt a regression model linking these inputs
to coﬀee quality and choose new points to sample near the optimum of
the current model to look for a better conﬁguration. We sample these
points, check quality, and add these to the data and ﬁt a new model. This
is generally called response surface design.

1.2.4

density estimation
clustering

Unsupervised Learning
In supervised learning, the aim is to learn a mapping from the input to
an output whose correct values are provided by a supervisor. In unsupervised learning, there is no such supervisor and we only have input data.
The aim is to ﬁnd the regularities in the input. There is a structure to the
input space such that certain patterns occur more often than others, and
we want to see what generally happens and what does not. In statistics,
this is called density estimation.
One method for density estimation is clustering where the aim is to
ﬁnd clusters or groupings of input. In the case of a company with a data
of past customers, the customer data contains the demographic information as well as the past transactions with the company, and the company
may want to see the distribution of the proﬁle of its customers, to see
what type of customers frequently occur. In such a case, a clustering
model allocates customers similar in their attributes to the same group,
providing the company with natural groupings of its customers; this is
called customer segmentation. Once such groups are found, the company
may decide strategies, for example, services and products, speciﬁc to different groups; this is known as customer relationship management. Such
a grouping also allows identifying those who are outliers, namely, those
who are diﬀerent from other customers, which may imply a niche in the
market that can be further exploited by the company.
An interesting application of clustering is in image compression. In
this case, the input instances are image pixels represented as RGB values. A clustering program groups pixels with similar colors in the same
1. I would like to thank Michael Jordan for this example.

12

1

Introduction

group, and such groups correspond to the colors occurring frequently in
the image. If in an image, there are only shades of a small number of
colors, and if we code those belonging to the same group with one color,
for example, their average, then the image is quantized. Let us say the
pixels are 24 bits to represent 16 million colors, but if there are shades
of only 64 main colors, for each pixel we need 6 bits instead of 24. For
example, if the scene has various shades of blue in diﬀerent parts of the
image, and if we use the same average blue for all of them, we lose the
details in the image but gain space in storage and transmission. Ideally,
one would like to identify higher-level regularities by analyzing repeated
image patterns, for example, texture, objects, and so forth. This allows a
higher-level, simpler, and more useful description of the scene, and for
example, achieves better compression than compressing at the pixel level.
If we have scanned document pages, we do not have random on/oﬀ pixels but bitmap images of characters. There is structure in the data, and
we make use of this redundancy by ﬁnding a shorter description of the
data: 16 × 16 bitmap of ‘A’ takes 32 bytes; its ASCII code is only 1 byte.
In document clustering, the aim is to group similar documents. For
example, news reports can be subdivided as those related to politics,
sports, fashion, arts, and so on. Commonly, a document is represented
as a bag of words, that is, we predeﬁne a lexicon of N words and each
document is an N-dimensional binary vector whose element i is 1 if word
i appears in the document; suﬃxes “–s” and “–ing” are removed to avoid
duplicates and words such as “of,” “and,” and so forth, which are not
informative, are not used. Documents are then grouped depending on
the number of shared words. It is of course here critical how the lexicon
is chosen.
Machine learning methods are also used in bioinformatics. DNA in our
genome is the “blueprint of life” and is a sequence of bases, namely, A, G,
C, and T. RNA is transcribed from DNA, and proteins are translated from
the RNA. Proteins are what the living body is and does. Just as a DNA is
a sequence of bases, a protein is a sequence of amino acids (as deﬁned
by bases). One application area of computer science in molecular biology
is alignment, which is matching one sequence to another. This is a difﬁcult string matching problem because strings may be quite long, there
are many template strings to match against, and there may be deletions,
insertions, and substitutions. Clustering is used in learning motifs, which
are sequences of amino acids that occur repeatedly in proteins. Motifs
are of interest because they may correspond to structural or functional

1.2 Examples of Machine Learning Applications

13

elements within the sequences they characterize. The analogy is that if
the amino acids are letters and proteins are sentences, motifs are like
words, namely, a string of letters with a particular meaning occurring
frequently in diﬀerent sentences.

1.2.5

reinforcement
learning

Reinforcement Learning
In some applications, the output of the system is a sequence of actions.
In such a case, a single action is not important; what is important is the
policy that is the sequence of correct actions to reach the goal. There is
no such thing as the best action in any intermediate state; an action is
good if it is part of a good policy. In such a case, the machine learning
program should be able to assess the goodness of policies and learn from
past good action sequences to be able to generate a policy. Such learning
methods are called reinforcement learning algorithms.
A good example is game playing where a single move by itself is not
that important; it is the sequence of right moves that is good. A move is
good if it is part of a good game playing policy. Game playing is an important research area in both artiﬁcial intelligence and machine learning.
This is because games are easy to describe and at the same time, they are
quite diﬃcult to play well. A game like chess has a small number of rules
but it is very complex because of the large number of possible moves at
each state and the large number of moves that a game contains. Once
we have good algorithms that can learn to play games well, we can also
apply them to applications with more evident economic utility.
A robot navigating in an environment in search of a goal location is
another application area of reinforcement learning. At any time, the robot
can move in one of a number of directions. After a number of trial runs,
it should learn the correct sequence of actions to reach to the goal state
from an initial state, doing this as quickly as possible and without hitting
any of the obstacles.
One factor that makes reinforcement learning harder is when the system has unreliable and partial sensory information. For example, a robot
equipped with a video camera has incomplete information and thus at
any time is in a partially observable state and should decide taking into
account this uncertainty; for example, it may not know its exact location
in a room but only that there is a wall to its left. A task may also require a concurrent operation of multiple agents that should interact and

14

1

Introduction

cooperate to accomplish a common goal. An example is a team of robots
playing soccer.

1.3

Notes
Evolution is the major force that deﬁnes our bodily shape as well as our
built-in instincts and reﬂexes. We also learn to change our behavior during our lifetime. This helps us cope with changes in the environment
that cannot be predicted by evolution. Organisms that have a short life
in a well-deﬁned environment may have all their behavior built-in, but
instead of hardwiring into us all sorts of behavior for any circumstance
that we could encounter in our life, evolution gave us a large brain and a
mechanism to learn, such that we could update ourselves with experience
and adapt to diﬀerent environments. When we learn the best strategy in
a certain situation, that knowledge is stored in our brain, and when the
situation arises again, when we re-cognize (“cognize” means to know) the
situation, we can recall the suitable strategy and act accordingly. Learning has its limits though; there may be things that we can never learn with
the limited capacity of our brains, just like we can never “learn” to grow
a third arm, or an eye on the back of our head, even if either would be
useful. See Leahey and Harris 1997 for learning and cognition from the
point of view of psychology. Note that unlike in psychology, cognitive science, or neuroscience, our aim in machine learning is not to understand
the processes underlying learning in humans and animals, but to build
useful systems, as in any domain of engineering.
Almost all of science is ﬁtting models to data. Scientists design experiments and make observations and collect data. They then try to extract
knowledge by ﬁnding out simple models that explain the data they observed. This is called induction and is the process of extracting general
rules from a set of particular cases.
We are now at a point that such analysis of data can no longer be done
by people, both because the amount of data is huge and because people
who can do such analysis are rare and manual analysis is costly. There
is thus a growing interest in computer models that can analyze data and
extract information automatically from them, that is, learn.
The methods we are going to discuss in the coming chapters have their
origins in diﬀerent scientiﬁc domains. Sometimes the same algorithm

1.3 Notes

15

was independently invented in more than one ﬁeld, following a diﬀerent
historical path.
In statistics, going from particular observations to general descriptions
is called inference and learning is called estimation. Classiﬁcation is
called discriminant analysis in statistics (McLachlan 1992; Hastie, Tibshirani, and Friedman 2001). Before computers were cheap and abundant, statisticians could only work with small samples. Statisticians, being mathematicians, worked mostly with simple parametric models that
could be analyzed mathematically. In engineering, classiﬁcation is called
pattern recognition and the approach is nonparametric and much more
empirical (Duda, Hart, and Stork 2001; Webb 1999). Machine learning is
related to artiﬁcial intelligence (Russell and Norvig 2002) because an intelligent system should be able to adapt to changes in its environment.
Application areas like vision, speech, and robotics are also tasks that
are best learned from sample data. In electrical engineering, research in
signal processing resulted in adaptive computer vision and speech programs. Among these, the development of hidden Markov models (HMM)
for speech recognition is especially important.
In the late 1980s with advances in VLSI technology and the possibility of building parallel hardware containing thousands of processors,
the ﬁeld of artiﬁcial neural networks was reinvented as a possible theory to distribute computation over a large number of processing units
(Bishop 1995). Over time, it has been realized in the neural network community that most neural network learning algorithms have their basis in
statistics—for example, the multilayer perceptron is another class of nonparametric estimator—and claims of brainlike computation have started
to fade.
In recent years, kernel-based algorithms, such as support vector machines, have become popular, which, through the use of kernel functions,
can be adapted to various applications, especially in bioinformatics and
language processing. It is common knowledge nowadays that a good representation of data is critical for learning and kernel functions turn out
to be a very good way to introduce such expert knowledge.
Recently, with the reduced cost of storage and connectivity, it has become possible to have very large datasets available over the Internet, and
this, coupled with cheaper computation, have made it possible to run
learning algorithms on a lot of data. In the past few decades, it was generally believed that for artiﬁcial intelligence to be possible, we needed
a new paradigm, a new type of thinking, a new model of computation

16

1

Introduction

or a whole new set of algorithms. Taking into account the recent successes in machine learning in various domains, it may be claimed that
what we needed was not new algorithms but a lot of example data and
suﬃcient computing power to run the algorithms on that much data. For
example, the roots of support vector machines go to potential functions,
linear classiﬁers, and neighbor-based methods, proposed in the 1950s or
the 1960s; it is just that we did not have fast computers or large storage
then for these algorithms to show their full potential. It may be conjectured that tasks such as machine translation, and even planning, can
be solved with such relatively simple learning algorithms but trained on
large amounts of example data, or through long runs of trial and error.
Intelligence seems not to originate from some outlandish formula, but
rather from the patient, almost brute-force use of a simple, straightforward algorithm.
Data mining is the name coined in the business world for the application of machine learning algorithms to large amounts of data (Witten and
Frank 2005; Han and Kamber 2006). In computer science, it used to be
called knowledge discovery in databases (KDD).
Research in these diﬀerent communities (statistics, pattern recognition, neural networks, signal processing, control, artiﬁcial intelligence,
and data mining) followed diﬀerent paths in the past with diﬀerent emphases. In this book, the aim is to incorporate these emphases together
to give a uniﬁed treatment of the problems and the proposed solutions
to them.

1.4

Relevant Resources
The latest research on machine learning is distributed over journals and
conferences from diﬀerent ﬁelds. Dedicated journals are Machine Learning and Journal of Machine Learning Research. Journals with a neural
network emphasis are Neural Computation, Neural Networks, and the
IEEE Transactions on Neural Networks. Statistics journals like Annals of
Statistics and Journal of the American Statistical Association also publish
machine learning papers. IEEE Transactions on Pattern Analysis and Machine Intelligence is another source.
Journals on artiﬁcial intelligence, pattern recognition, fuzzy logic, and
signal processing also contain machine learning papers. Journals with an
emphasis on data mining are Data Mining and Knowledge Discovery, IEEE

1.4 Relevant Resources

17

Transactions on Knowledge and Data Engineering, and ACM Special Interest Group on Knowledge Discovery and Data Mining Explorations Journal.
The major conferences on machine learning are Neural Information
Processing Systems (NIPS), Uncertainty in Artiﬁcial Intelligence (UAI), International Conference on Machine Learning (ICML), European Conference
on Machine Learning (ECML), and Computational Learning Theory (COLT).
International Joint Conference on Artiﬁcial Intelligence (IJCAI), as well as
conferences on neural networks, pattern recognition, fuzzy logic, and genetic algorithms, have sessions on machine learning and conferences on
application areas like computer vision, speech technology, robotics, and
data mining.
There are a number of dataset repositories on the Internet that are used
frequently by machine learning researchers for benchmarking purposes:
UCI Repository for machine learning is the most popular repository:
http://www.ics.uci.edu/∼mlearn/MLRepository.html
UCI KDD Archive:
http://kdd.ics.uci.edu/summary.data.application.html
Statlib: http://lib.stat.cmu.edu
Delve: http://www.cs.utoronto.ca/∼delve/
In addition to these, there are also repositories for particular applications, for example, computional biology, face recognition, speech recognition, and so forth.
New and larger datasets are constantly being added to these repositories, especially to the UCI repository. Still, some researchers believe
that such repositories do not reﬂect the full characteristics of real data
and are of limited scope, and therefore accuracies on datasets from such
repositories are not indicative of anything. It may even be claimed that
when some datasets from a ﬁxed repository are used repeatedly while tailoring a new algorithm, we are generating a new set of “UCI algorithms”
specialized for those datasets.
As we will see in later chapters, diﬀerent algorithms are better on different tasks anyway, and therefore it is best to keep one application in
mind, to have one or a number of large datasets drawn for that and compare algorithms on those, for that speciﬁc task.
Most recent papers by machine learning researchers are accessible over
the Internet, and a good place to start searching is the NEC Research In-

18

1

Introduction

dex at http://citeseer.ist.psu.edu. Most authors also make codes of their
algorithms available over the Web. There are also free software packages
implementing various machine learning algorithms, and among these,
Weka is especially noteworthy: http://www.cs.waikato.ac.nz/ml/weka/.

1.5

Exercises
1. Imagine you have two possibilities: You can fax a document, that is, send the
image, or you can use an optical character reader (OCR) and send the text
ﬁle. Discuss the advantage and disadvantages of the two approaches in a
comparative manner. When would one be preferable over the other?
2. Let us say we are building an OCR and for each character, we store the bitmap
of that character as a template that we match with the read character pixel by
pixel. Explain when such a system would fail. Why are barcode readers still
used?
3. Assume we are given the task to build a system that can distinguish junk email. What is in a junk e-mail that lets us know that it is junk? How can the
computer detect junk through a syntactic analysis? What would you like the
computer to do if it detects a junk e-mail—delete it automatically, move it to
a diﬀerent ﬁle, or just highlight it on the screen?
4. Let us say you are given the task of building an automated taxi. Deﬁne the
constraints. What are the inputs? What is the output? How can you communicate with the passenger? Do you need to communicate with the other
automated taxis, that is, do you need a “language”?
5. In basket analysis, we want to ﬁnd the dependence between two items X
and Y . Given a database of customer transactions, how can you ﬁnd these
dependencies? How would you generalize this to more than two items?
6. How can you predict the next command to be typed by the user? Or the
next page to be downloaded over the Web? When would such a prediction be
useful? When would it be annoying?
7. In your everyday newspaper, ﬁnd ﬁve sample news reports for each category
of politics, sports, and the arts. Go over these reports and ﬁnd words that are
used frequently for each category, which may help us discriminate between
diﬀerent categories. For example, a news report on politics is likely to include
words such as “government,” “recession,” “congress,” and so forth, whereas
a news report on the arts may include “album,” “canvas,” or “theater.” There
are also words such as “goal” that are ambiguous.
8. If a face image is a 100 × 100 image, written in row-major, this is a 10,000dimensional vector. If we shift the image one pixel to the right, this will be a

1.6 References

19

very diﬀerent vector in the 10,000-dimensional space. How can we build face
recognizers robust to such distortions?
9. Take a word, for example, “machine.” Write it ten times. Also ask a friend
to write it ten times. Analyzing these twenty images, try to ﬁnd features,
types of strokes, curvatures, loops, how you make the dots, and so on, that
discriminate your handwriting from your friend’s.
10. In estimating the price of a used car, rather than estimating the absolute price
it makes more sense to estimate the percent depreciation over the original
price. Why?

1.6

References
Bishop, C. M. 1995. Neural Networks for Pattern Recognition. Oxford: Oxford
University Press.
Duda, R. O., P. E. Hart, and D. G. Stork. 2001. Pattern Classiﬁcation, 2nd ed.
New York: Wiley.
Han, J., and M. Kamber. 2006. Data Mining: Concepts and Techniques, 2nd ed.
San Francisco: Morgan Kaufmann.
Hand, D. J. 1998. “Consumer Credit and Statistics.” In Statistics in Finance, ed.
D. J. Hand and S. D. Jacka, 69–81. London: Arnold.
Hastie, T., R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical
Learning: Data Mining, Inference, and Prediction. New York: Springer.
Leahey, T. H., and R. J. Harris. 1997. Learning and Cognition, 4th ed. New York:
Prentice Hall.
McLachlan, G. J. 1992. Discriminant Analysis and Statistical Pattern Recognition.
New York: Wiley.
Russell, S., and P. Norvig. 2002. Artiﬁcial Intelligence: A Modern Approach, 2nd
ed. New York: Prentice Hall.
Webb, A. 1999. Statistical Pattern Recognition. London: Arnold.
Witten, I. H., and E. Frank. 2005. Data Mining: Practical Machine Learning Tools
and Techniques, 2nd ed. San Francisco: Morgan Kaufmann.

2

Supervised Learning

We discuss supervised learning starting from the simplest case, which
is learning a class from its positive and negative examples. We generalize and discuss the case of multiple classes, then regression, where
the outputs are continuous.

2.1

positive examples
negative examples

input
representation

Learning a Class from Examples
Let us say we want to learn the class, C, of a “family car.” We have a
set of examples of cars, and we have a group of people that we survey to
whom we show these cars. The people look at the cars and label them;
the cars that they believe are family cars are positive examples, and the
other cars are negative examples. Class learning is ﬁnding a description
that is shared by all positive examples and none of the negative examples.
Doing this, we can make a prediction: Given a car that we have not seen
before, by checking with the description learned, we will be able to say
whether it is a family car or not. Or we can do knowledge extraction:
This study may be sponsored by a car company, and the aim may be to
understand what people expect from a family car.
After some discussions with experts in the ﬁeld, let us say that we reach
the conclusion that among all features a car may have, the features that
separate a family car from other cars are the price and engine power.
These two attributes are the inputs to the class recognizer. Note that
when we decide on this particular input representation, we are ignoring
various other attributes as irrelevant. Though one may think of other
attributes such as seating capacity and color that might be important for
distinguishing among car types, we will consider only price and engine
power to keep this example simple.

22

x2: Engine power

2 Supervised Learning

x2t

x1t

x1: Price

Figure 2.1 Training set for the class of a “family car.” Each data point corresponds to one example car, and the coordinates of the point indicate the price
and engine power of that car. ‘+’ denotes a positive example of the class (a family
car), and ‘−’ denotes a negative example (not a family car); it is another type of
car.

Let us denote price as the ﬁrst input attribute x1 (e.g., in U.S. dollars)
and engine power as the second attribute x2 (e.g., engine volume in cubic
centimeters). Thus we represent each car using two numeric values
(2.1)

x=

x1
x2

and its label denotes its type
(2.2)

r=

1
0

if x is a positive example
if x is a negative example

Each car is represented by such an ordered pair (x, r ) and the training
set contains N such examples
(2.3)

X = {xt , r t }N
t=1
where t indexes diﬀerent examples in the set; it does not represent time
or any such order.

23

x2: Engine power

2.1 Learning a Class from Examples

C

e2

e1

p1

p2

x1: Price

Figure 2.2 Example of a hypothesis class. The class of family car is a rectangle
in the price-engine power space.

Our training data can now be plotted in the two-dimensional (x1 , x2 )
space where each instance t is a data point at coordinates (xt , xt ) and its
1
2
type, namely, positive versus negative, is given by r t (see ﬁgure 2.1).
After further discussions with the expert and the analysis of the data,
we may have reason to believe that for a car to be a family car, its price
and engine power should be in a certain range
(2.4)

hypothesis class

hypothesis

(p1 ≤ price ≤ p2 ) AND (e1 ≤ engine power ≤ e2 )
for suitable values of p1 , p2 , e1 , and e2 . Equation 2.4 thus assumes C to
be a rectangle in the price-engine power space (see ﬁgure 2.2).
Equation 2.4 ﬁxes H , the hypothesis class from which we believe C is
drawn, namely, the set of rectangles. The learning algorithm then ﬁnds
the particular hypothesis, h ∈ H , to approximate C as closely as possible.
Though the expert deﬁnes this hypothesis class, the values of the parameters are not known; that is, though we choose H , we do not know
which particular h ∈ H is equal, or closest, to C. But once we restrict our

24

2 Supervised Learning

attention to this hypothesis class, learning the class reduces to the easier
problem of ﬁnding the four parameters that deﬁne h.
The aim is to ﬁnd h ∈ H that is as similar as possible to C. Let us say
the hypothesis h makes a prediction for an instance x such that
(2.5)

empirical error

h(x) =

1
0

if h classiﬁes x as a positive example
if h classiﬁes x as a negative example

In real life we do not know C(x), so we cannot evaluate how well h(x)
matches C(x). What we have is the training set X, which is a small subset
of the set of all possible x. The empirical error is the proportion of training instances where predictions of h do not match the required values
given in X. The error of hypothesis h given the training set X is
N

(2.6)

1(h(x t ) = r t )

E(h|X) =
t=1

generalization

most specific
hypothesis

most general
hypothesis

version space

where 1(a = b) is 1 if a = b and is 0 if a = b (see ﬁgure 2.3).
In our example, the hypothesis class H is the set of all possible rech
h h h
tangles. Each quadruple (p1 , p2 , e1 , e2 ) deﬁnes one hypothesis, h, from
H , and we need to choose the best one, or in other words, we need to
ﬁnd the values of these four parameters given the training set, to include all the positive examples and none of the negative examples. Note
that if x1 and x2 are real-valued, there are inﬁnitely many such h for
which this is satisﬁed, namely, for which the error, E, is 0, but given a
future example somewhere close to the boundary between positive and
negative examples, diﬀerent candidate hypotheses may make diﬀerent
predictions. This is the problem of generalization—that is, how well our
hypothesis will correctly classify future examples that are not part of the
training set.
One possibility is to ﬁnd the most speciﬁc hypothesis, S, that is the
tightest rectangle that includes all the positive examples and none of the
negative examples (see ﬁgure 2.4). This gives us one hypothesis, h = S, as
our induced class. Note that the actual class C may be larger than S but is
never smaller. The most general hypothesis, G, is the largest rectangle we
can draw that includes all the positive examples and none of the negative
examples (ﬁgure 2.4). Any h ∈ H between S and G is a valid hypothesis
with no error, said to be consistent with the training set, and such h make
up the version space. Given another training set, S, G, version space, the
parameters and thus the learned hypothesis, h, can be diﬀerent.

2.1 Learning a Class from Examples

25

 
¡¢
£
¤
¥¦
¤
§
¨
©

§

Figure 2.3 C is the actual class and h is our induced hypothesis. The point
where C is 1 but h is 0 is a false negative, and the point where C is 0 but h is 1
is a false positive. Other points—namely, true positives and true negatives—are
correctly classiﬁed.

margin

Actually, depending on X and H , there may be several Si and Gj which
respectively make up the S-set and the G-set. Every member of the S-set
is consistent with all the instances, and there are no consistent hypotheses that are more speciﬁc. Similarly, every member of the G-set is consistent with all the instances, and there are no consistent hypotheses that
are more general. These two make up the boundary sets and any hypothesis between them is consistent and is part of the version space. There is
an algorithm called candidate elimination that incrementally updates the
S- and G-sets as it sees training instances one by one; see Mitchell 1997.
We assume X is large enough that there is a unique S and G.
Given X, we can ﬁnd S, or G, or any h from the version space and use
it as our hypothesis, h. It seems intuitive to choose h halfway between S
and G; this is to increase the margin, which is the distance between the

26

x2: Engine power

2 Supervised Learning

G
C
S

x1: Price

Figure 2.4 S is the most speciﬁc and G is the most general hypothesis.

doubt

boundary and the instances closest to it (see ﬁgure 2.5). For our error
function to have a minimum at h with the maximum margin, we should
use an error (loss) function which not only checks whether an instance
is on the correct side of the boundary but also how far away it is. That
is, instead of h(x) that returns 0/1, we need to have a hypothesis that
returns a value which carries a measure of the distance to the boundary
and we need to have a loss function which uses it, diﬀerent from 1(·)
that checks for equality.
In some applications, a wrong decision may be very costly and in such
a case, we can say that any instance that falls in between S and G is a
case of doubt, which we cannot label with certainty due to lack of data.
In such a case, the system rejects the instance and defers the decision to
a human expert.
Here, we assume that H includes C; that is, there exists h ∈ H , such
that E(h|X) is 0. Given a hypothesis class H , it may be the case that we
cannot learn C; that is, there exists no h ∈ H for which the error is 0.
Thus, in any application, we need to make sure that H is ﬂexible enough,
or has enough “capacity,” to learn C.

2.2 Vapnik-Chervonenkis (VC) Dimension

27

 
¡¢
£
¤
¥¦
¤
§
¨
©

§


Figure 2.5 We choose the hypothesis with the largest margin, for best separation. The shaded instances are those that deﬁne (or support) the margin; other
instances can be removed without aﬀecting h.

2.2

VC dimension

Vapnik-Chervonenkis (VC) Dimension
Let us say we have a dataset containing N points. These N points can
be labeled in 2N ways as positive and negative. Therefore, 2N diﬀerent
learning problems can be deﬁned by N data points. If for any of these
problems, we can ﬁnd a hypothesis h ∈ H that separates the positive examples from the negative, then we say H shatters N points. That is, any
learning problem deﬁnable by N examples can be learned with no error
by a hypothesis drawn from H . The maximum number of points that
can be shattered by H is called the Vapnik-Chervonenkis (VC) dimension
of H , is denoted as V C(H ), and measures the capacity of H .
In ﬁgure 2.6, we see that an axis-aligned rectangle can shatter four
points in two dimensions. Then V C(H ), when H is the hypothesis class
of axis-aligned rectangles in two dimensions, is four. In calculating the
VC dimension, it is enough that we ﬁnd four points that can be shattered;
it is not necessary that we be able to shatter any four points in two di-

28

x2

2 Supervised Learning

x1
Figure 2.6 An axis-aligned rectangle can shatter four points. Only rectangles
covering two points are shown.

mensions. For example, four points placed on a line cannot be shattered
by rectangles. However, we cannot place ﬁve points in two dimensions
anywhere such that a rectangle can separate the positive and negative
examples for all possible labelings.
VC dimension may seem pessimistic. It tells us that using a rectangle
as our hypothesis class, we can learn only datasets containing four points
and not more. A learning algorithm that can learn datasets of four points
is not very useful. However, this is because the VC dimension is independent of the probability distribution from which instances are drawn.
In real life, the world is smoothly changing, instances close by most of
the time have the same labels, and we need not worry about all possible
labelings. There are a lot of datasets containing many more data points
than four that are learnable by our hypothesis class (ﬁgure 2.1). So even
hypothesis classes with small VC dimensions are applicable and are preferred over those with large VC dimensions, for example, a lookup table
that has inﬁnite VC dimension.

2.3 Probably Approximately Correct (PAC) Learning

2.3

PAC learning

29

Probably Approximately Correct (PAC) Learning
Using the tightest rectangle, S, as our hypothesis, we would like to ﬁnd
how many examples we need. We would like our hypothesis to be approximately correct, namely, that the error probability be bounded by some
value. We also would like to be conﬁdent in our hypothesis in that we
want to know that our hypothesis will be correct most of the time (if not
always); so we want to be probably correct as well (by a probability we
can specify).
In Probably Approximately Correct (PAC) learning, given a class, C, and
examples drawn from some unknown but ﬁxed probability distribution,
p(x), we want to ﬁnd the number of examples, N, such that with probability at least 1 − δ, the hypothesis h has error at most , for arbitrary
δ ≤ 1/2 and > 0
P {CΔh ≤ } ≥ 1 − δ
where CΔh is the region of diﬀerence between C and h.
In our case, because S is the tightest possible rectangle, the error region
between C and h = S is the sum of four rectangular strips (see ﬁgure 2.7).
We would like to make sure that the probability of a positive example
falling in here (and causing an error) is at most . For any of these strips,
if we can guarantee that the probability is upper bounded by /4, the
error is at most 4( /4) = . Note that we count the overlaps in the corners
twice, and the total actual error in this case is less than 4( /4). The
probability that a randomly drawn example misses this strip is 1 − /4.
The probability that all N independent draws miss the strip is (1 − /4)N ,
and the probability that all N independent draws miss any of the four
strips is at most 4(1 − /4)N , which we would like to be at most δ. We
have the inequality
(1 − x) ≤ exp[−x]
So if we choose N and δ such that we have
4 exp[− N/4] ≤ δ
we can also write 4(1 − /4)N ≤ δ. Dividing both sides by 4, taking
(natural) log and rearranging terms, we have

(2.7)

N ≥ (4/ ) log(4/δ)

30

x2

2 Supervised Learning

C
h

x1

Figure 2.7 The diﬀerence between h and C is the sum of four rectangular strips,
one of which is shaded.

Therefore, provided that we take at least (4/ ) log(4/δ) independent
examples from C and use the tightest rectangle as our hypothesis h, with
conﬁdence probability at least 1 − δ, a given point will be misclassiﬁed
with error probability at most . We can have arbitrary large conﬁdence
by decreasing δ and arbitrary small error by decreasing , and we see in
equation 2.7 that the number of examples is a slowly growing function of
1/ and 1/δ, linear and logarithmic, respectively.

2.4
noise

Noise
Noise is any unwanted anomaly in the data and due to noise, the class
may be more diﬃcult to learn and zero error may be infeasible with a
simple hypothesis class (see ﬁgure 2.8). There are several interpretations
of noise:
There may be imprecision in recording the input attributes, which may
shift the data points in the input space.
There may be errors in labeling the data points, which may relabel

31

x2

2.4 Noise

h2

h1

x1

Figure 2.8 When there is noise, there is not a simple boundary between the positive and negative instances, and zero misclassiﬁcation error may not be possible
with a simple hypothesis. A rectangle is a simple hypothesis with four parameters deﬁning the corners. An arbitrary closed form can be drawn by piecewise
functions with a larger number of control points.

positive instances as negative and vice versa. This is sometimes called
teacher noise.
There may be additional attributes, which we have not taken into account, that aﬀect the label of an instance. Such attributes may be
hidden or latent in that they may be unobservable. The eﬀect of these
neglected attributes is thus modeled as a random component and is
included in “noise.”
As can be seen in ﬁgure 2.8, when there is noise, there is not a simple
boundary between the positive and negative instances and to separate
them, one needs a complicated hypothesis that corresponds to a hypothesis class with larger capacity. A rectangle can be deﬁned by four numbers, but to deﬁne a more complicated shape one needs a more complex
model with a much larger number of parameters. With a complex model,

32

2 Supervised Learning

one can make a perfect ﬁt to the data and attain zero error; see the wiggly
shape in ﬁgure 2.8. Another possibility is to keep the model simple and
allow some error; see the rectangle in ﬁgure 2.8.
Using the simple rectangle (unless its training error is much bigger)
makes more sense because of the following:
1. It is a simple model to use. It is easy to check whether a point is
inside or outside a rectangle and we can easily check, for a future data
instance, whether it is a positive or a negative instance.
2. It is a simple model to train and has fewer parameters. It is easier
to ﬁnd the corner values of a rectangle than the control points of an
arbitrary shape. With a small training set when the training instances
diﬀer a little bit, we expect the simpler model to change less than a
complex model: A simple model is thus said to have less variance.
On the other hand, a too simple model assumes more, is more rigid,
and may fail if indeed the underlying class is not that simple: A simpler model has more bias. Finding the optimal model corresponds to
minimizing both the bias and the variance.
3. It is a simple model to explain. A rectangle simply corresponds to
deﬁning intervals on the two attributes. By learning a simple model,
we can extract information from the raw data given in the training set.

Occam’s razor

2.5

4. If indeed there is mislabeling or noise in input and the actual class
is really a simple model like the rectangle, then the simple rectangle,
because it has less variance and is less aﬀected by single instances,
will be a better discriminator than the wiggly shape, although the simple one may make slightly more errors on the training set. Given
comparable empirical error, we say that a simple (but not too simple)
model would generalize better than a complex model. This principle
is known as Occam’s razor, which states that simpler explanations are
more plausible and any unnecessary complexity should be shaved oﬀ.

Learning Multiple Classes
In our example of learning a family car, we have positive examples belonging to the class family car and the negative examples belonging to all
other cars. This is a two-class problem. In the general case, we have K

33

2.5 Learning Multiple Classes

Engine power

Sports car

?
?

Luxury sedan

Family car

Price

Figure 2.9 There are three classes: family car, sports car, and luxury sedan.
There are three hypotheses induced, each one covering the instances of one
class and leaving outside the instances of the other two classes. ‘?’ are reject
regions where no, or more than one, class is chosen.

classes denoted as Ci , i = 1, . . . , K, and an input instance belongs to one
and exactly one of them. The training set is now of the form
X = {x t , r t }N
t=1
where r has K dimensions and
(2.8)

rit =

1
0

if x t ∈ Ci
if x t ∈ Cj , j = i

An example is given in ﬁgure 2.9 with instances from three classes:
family car, sports car, and luxury sedan.
In machine learning for classiﬁcation, we would like to learn the boundary separating the instances of one class from the instances of all other
classes. Thus we view a K-class classiﬁcation problem as K two-class
problems. The training examples belonging to Ci are the positive instances of hypothesis hi and the examples of all other classes are the

34

2 Supervised Learning

negative instances of hi . Thus in a K-class problem, we have K hypotheses to learn such that
(2.9)

hi (x t ) =

1
0

if x t ∈ Ci
if x t ∈ Cj , j = i

The total empirical error takes a sum over the predictions for all classes
over all instances:
N

(2.10)

K

1(hi (x t ) = rit )

E({hi }K |X) =
i=1
t=1 i=1

reject

2.6

For a given x, ideally only one of hi (x), i = 1, . . . , K is 1 and we can
choose a class. But when no, or two or more, hi (x) is 1, we cannot choose
a class, and this is the case of doubt and the classiﬁer rejects such cases.
In our example of learning a family car, we used only one hypothesis
and only modeled the positive examples. Any negative example outside
is not a family car. Alternatively, sometimes we may prefer to build two
hypotheses, one for the positive and the other for the negative instances.
This assumes a structure also for the negative instances that can be covered by another hypothesis. Separating family cars from sports cars is
such a problem; each class has a structure of its own. The advantage is
that if the input is a luxury sedan, we can have both hypotheses decide
negative and reject the input.
If in a dataset, we expect to have all classes with similar distribution—
shapes in the input space—then the same hypothesis class can be used
for all classes. For example, in a handwritten digit recognition dataset,
we would expect all digits to have similar distributions. But in a medical
diagnosis dataset, for example, where we have two classes for sick and
healthy people, we may have completely diﬀerent distributions for the
two classes; there may be multiple ways for a person to be sick, reﬂected
diﬀerently in the inputs: All healthy people are alike; each sick person is
sick in his or her own way.

Regression
In classiﬁcation, given an input, the output that is generated is Boolean;
it is a yes/no answer. When the output is a numeric value, what we would
like to learn is not a class, C(x) ∈ {0, 1}, but is a numeric function. In

35

2.6 Regression

machine learning, the function is not known but we have a training set of
examples drawn from it
X = {x t , r t }N
t=1
interpolation

where r t ∈ . If there is no noise, the task is interpolation. We would like
to ﬁnd the function f (x) that passes through these points such that we
have
r t = f (x t )

extrapolation

regression

(2.11)

In polynomial interpolation, given N points, we ﬁnd the (N −1)st degree
polynomial that we can use to predict the output for any x. This is called
extrapolation if x is outside of the range of x t in the training set. In
time-series prediction, for example, we have data up to the present and
we want to predict the value for the future. In regression, there is noise
added to the output of the unknown function
r t = f (x t ) +
where f (x) ∈
is the unknown function and is random noise. The explanation for noise is that there are extra hidden variables that we cannot
observe

(2.12)

r t = f ∗ (x t , z t )
where z t denote those hidden variables. We would like to approximate
the output by our model g(x). The empirical error on the training set X
is

(2.13)

E(g|X) =

1
N

N

[r t − g(x t )]2
t=1

Because r and g(x) are numeric quantities, for example, ∈ , there is
an ordering deﬁned on their values and we can deﬁne a distance between
values, as the square of the diﬀerence, which gives us more information than equal/not equal, as used in classiﬁcation. The square of the
diﬀerence is one error (loss) function that can be used; another is the absolute value of the diﬀerence. We will see other examples in the coming
chapters.
Our aim is to ﬁnd g(·) that minimizes the empirical error. Again our
approach is the same; we assume a hypothesis class for g(·) with a small
set of parameters. If we assume that g(x) is linear, we have
d

(2.14)

g(x) = w1 x1 + · · · + wd xd + w0 =

wj xj + w0
j=1

36

y: price

2 Supervised Learning

x: milage

Figure 2.10 Linear, second-order, and sixth-order polynomials are ﬁtted to the
same set of points. The highest order gives a perfect ﬁt, but given this much
data it is very unlikely that the real curve is so shaped. The second order seems
better than the linear ﬁt in capturing the trend in the training data.

Let us now go back to our example in section 1.2.3 where we estimated
the price of a used car. There we used a single input linear model
(2.15)

g(x) = w1 x + w0
where w1 and w0 are the parameters to learn from data. The w1 and w0
values should minimize

(2.16)

E(w1 , w0 |X) =

1
N

N

[r t − (w1 xt + w0 )]2
t=1

Its minimum point can be calculated by taking the partial derivatives
of E with respect to w1 and w0 , setting them equal to 0, and solving for
the two unknowns:
w1
(2.17)

=

w0

=

t

xt r t − xr N

t (x

t )2

r − w1 x

− Nx2

37

2.7 Model Selection and Generalization

Table 2.1 With two inputs, there are four possible cases and sixteen possible
Boolean functions.

x1
0
0
1
1

x2 h1
0 0
1 0
0 0
1 0

h2
0
0
0
1

h3
0
0
1
0

h4
0
0
1
1

h5
0
1
0
0

h6
0
1
0
1

h7
0
1
1
0

h8
0
1
1
1

h9 h10 h11 h12 h13 h14 h15 h16
1 1
1
1
1
1
1
1
0 0
0
0
1
1
1
1
0 0
1
1
0
0
1
1
0 1
0
1
0
1
0
1

where x = t xt /N and r = t r t /N. The line found is shown in ﬁgure 1.2.
If the linear model is too simple, it is too constrained and incurs a
large approximation error, and in such a case, the output may be taken
as a higher-order function of the input—for example, quadratic
(2.18)

g(x) = w2 x2 + w1 x + w0
where similarly we have an analytical solution for the parameters. When
the order of the polynomial is increased, the error on the training data decreases. But a high-order polynomial follows individual examples closely,
instead of capturing the general trend; see the sixth-order polynomial in
ﬁgure 2.10. This implies that Occam’s razor also applies in the case of regression and we should be careful when ﬁne-tuning the model complexity
to match it with the complexity of the function underlying the data.

2.7

Model Selection and Generalization
Let us start with the case of learning a Boolean function from examples.
In a Boolean function, all inputs and the output are binary. There are
2d possible ways to write d binary values and therefore, with d inputs,
the training set has at most 2d examples. As shown in table 2.1, each
d
of these can be labeled as 0 or 1, and therefore, there are 22 possible
Boolean functions of d inputs.
Each distinct training example removes half the hypotheses, namely,
those whose guesses are wrong. For example, let us say we have x1 = 0,
x2 = 1 and the output is 0; this removes h5 , h6 , h7 , h8 , h13 , h14 , h15 , h16 .
This is one way to interpret learning: we start with all possible hypothesis and as we see more training examples, we remove those hypotheses

38

2 Supervised Learning

ill-posed problem

inductive bias

model selection

that are not consistent with the training data. In the case of a Boolean
function, to end up with a single hypothesis we need to see all 2d training
examples. If the training set we are given contains only a small subset of
all possible instances, as it generally does—that is, if we know what the
output should be for only a small percentage of the cases—the solution
d
is not unique. After seeing N example cases, there remain 22 −N possible
functions. This is an example of an ill-posed problem where the data by
itself is not suﬃcient to ﬁnd a unique solution.
The same problem also exists in other learning applications, in classiﬁcation, and in regression. As we see more training examples, we know
more about the underlying function, and we carve out more hypotheses
that are inconsistent from the hypothesis class, but we still are left with
many consistent hypotheses.
So because learning is ill-posed, and data by itself is not suﬃcient to
ﬁnd the solution, we should make some extra assumptions to have a
unique solution with the data we have. The set of assumptions we make
to have learning possible is called the inductive bias of the learning algorithm. One way we introduce inductive bias is when we assume a hypothesis class H . In learning the class of family car, there are inﬁnitely
many ways of separating the positive examples from the negative examples. Assuming the shape of a rectangle is one inductive bias, and then
the rectangle with the largest margin for example, is another inductive
bias. In linear regression, assuming a linear function is an inductive bias,
and among all lines, choosing the one that minimizes squared error is
another inductive bias.
But we know that each hypothesis class has a certain capacity and can
learn only certain functions. The class of functions that can be learned
can be extended by using a hypothesis class with larger capacity, containing more complex hypotheses. For example, the hypothesis class that is a
union of two rectangles has higher capacity, but its hypotheses are more
complex. Similarly in regression, as we increase the order of the polynomial, the capacity and complexity increase. The question now is to decide
where to stop.
Thus learning is not possible without inductive bias, and now the question is how to choose the right bias. This is called model selection, which
is choosing between possible H . In answering this question, we should
remember that the aim of machine learning is rarely to replicate the training data but the prediction for new cases. That is we would like to be able
to generate the right output for an input instance outside the training set,

2.7 Model Selection and Generalization

generalization

underfitting

overfitting

triple trade-off

39

one for which the correct output is not given in the training set. How well
a model trained on the training set predicts the right output for new
instances is called generalization.
For best generalization, we should match the complexity of the hypothesis class H with the complexity of the function underlying the data. If
H is less complex than the function, we have underﬁtting, for example,
when trying to ﬁt a line to data sampled from a third-order polynomial. In
such a case, as we increase the complexity, the training error decreases.
But if we have H that is too complex, the data is not enough to constrain
it and we may end up with a bad hypothesis, h ∈ H , for example, when
ﬁtting two rectangles to data sampled from one rectangle. Or if there
is noise, an overcomplex hypothesis may learn not only the underlying
function but also the noise in the data and may make a bad ﬁt, for example, when ﬁtting a sixth-order polynomial to noisy data sampled from a
third-order polynomial. This is called overﬁtting. In such a case, having
more training data helps but only up to a certain point. Given a training
set and H , we can ﬁnd h ∈ H that has the minimum training error but if
H is not chosen well, no matter which h ∈ H we pick, we will not have
good generalization.
We can summarize our discussion citing the triple trade-oﬀ (Dietterich
2003). In all learning algorithms that are trained from example data,
there is a trade-oﬀ between three factors:
the complexity of the hypothesis we ﬁt to data, namely, the capacity
of the hypothesis class,
the amount of training data, and
the generalization error on new examples.
As the amount of training data increases, the generalization error decreases. As the complexity of the model class H increases, the generalization error decreases ﬁrst and then starts to increase. The generalization error of an overcomplex H can be kept in check by increasing the
amount of training data but only up to a point. If the data is sampled
from a line and if we are ﬁtting a higher-order polynomial, the ﬁt will be
constrained to lie close to the line if there is training data in the vicinity; where it has not been trained, a high-order polynomial may behave
erratically.
We can measure the generalization ability of a hypothesis, namely, the
quality of its inductive bias, if we have access to data outside the training

40

2 Supervised Learning

validation set

cross-validation

test set

set. We simulate this by dividing the training set we have into two parts.
We use one part for training (i.e., to ﬁt a hypothesis), and the remaining
part is called the validation set and is used to test the generalization
ability. That is, given a set of possible hypothesis classes Hi , for each
we ﬁt the best hi ∈ Hi on the training set. Then, assuming large enough
training and validation sets, the hypothesis that is the most accurate on
the validation set is the best one (the one that has the best inductive bias).
This process is called cross-validation. So, for example, to ﬁnd the right
order in polynomial regression, given a number of candidate polynomials
of diﬀerent orders where polynomials of diﬀerent orders correspond to
Hi , for each order, we ﬁnd the coeﬃcients on the training set, calculate
their errors on the validation set, and take the one that has the least
validation error as the best polynomial.
Note that if we need to report the error to give an idea about the expected error of our best model, we should not use the validation error.
We have used the validation set to choose the best model, and it has effectively become a part of the training set. We need a third set, a test set,
sometimes also called the publication set, containing examples not used
in training or validation. An analogy from our lives is when we are taking
a course: the example problems that the instructor solves in class while
teaching a subject form the training set; exam questions are the validation set; and the problems we solve in our later, professional life are the
test set.
We cannot keep on using the same training/validation split either, because after having been used once, the validation set eﬀectively becomes
part of training data. This will be like an instructor who uses the same
exam questions every year; a smart student will ﬁgure out not to bother
with the lectures and will only memorize the answers to those questions.
We should always remember that the training data we use is a random
sample, that is, for the same application, if we collect data once more, we
will get a slightly diﬀerent dataset, the ﬁtted h will be slightly diﬀerent
and will have a slightly diﬀerent validation error. Or if we have a ﬁxed set
which we divide for training, validation, and test, we will have diﬀerent
errors depending on how we do the division. These slight diﬀerences in
error will allow us to estimate how large diﬀerences should be to be considered signiﬁcant and not due to chance. That is, in choosing between
two hypothesis classes Hi and Hj , we will use them both multiple times
on a number of training and validation sets and check if the diﬀerence
between average errors of hi and hj is larger than the average diﬀerence

2.8 Dimensions of a Supervised Machine Learning Algorithm

41

between multiple hi . In chapter 19, we discuss how to design machine
learning experiments using limited data to best answer our questions—
for example, which is the best hypothesis class?—and how to analyze the
results of these experiments so that we can achieve statistically signiﬁcant conclusions minimally aﬀected by random chance.

2.8

Dimensions of a Supervised Machine Learning Algorithm
Let us now recapitulate and generalize. We have a sample

(2.19)
iid

X = {xt , r t }N
t=1
The sample is independent and identically distributed (iid); the ordering
is not important and all instances are drawn from the same joint distribution p(x, r ). t indexes one of the N instances, xt is the arbitrary
dimensional input, and r t is the associated desired output. r t is 0/1 for
two-class learning, is a K-dimensional binary vector (where exactly one of
the dimensions is 1 and all others 0) for (K > 2)-class classiﬁcation, and
is a real value in regression.
The aim is to build a good and useful approximation to r t using the
model g(xt |θ). In doing this, there are three decisions we must make:
1. Model we use in learning, denoted as
g(x|θ)
where g(·) is the model, x is the input, and θ are the parameters.
g(·) deﬁnes the hypothesis class H , and a particular value of θ instantiates one hypothesis h ∈ H . For example, in class learning, we
have taken a rectangle as our model whose four coordinates make up
θ; in linear regression, the model is the linear function of the input
whose slope and intercept are the parameters learned from the data.
The model (inductive bias), or H , is ﬁxed by the machine learning system designer based on his or her knowledge of the application and the
hypothesis h ic chosen (parameters are tuned) by a learning algorithm
using the training set, sampled from p(x, r ).
2. Loss function, L(·), to compute the diﬀerence between the desired output, r t , and our approximation to it, g(xt |θ), given the current value

42

2 Supervised Learning

of the parameters, θ. The approximation error, or loss, is the sum of
losses over the individual instances
(2.20)

L(r t , g(xt |θ))

E(θ|X) =
t

In class learning where outputs are 0/1, L(·) checks for equality or not;
in regression, because the output is a numeric value, we have ordering
information for distance and one possibility is to use the square of the
diﬀerence.
3. Optimization procedure to ﬁnd θ ∗ that minimizes the total error
(2.21)

θ ∗ = arg min E(θ|X)
θ

where arg min returns the argument that minimizes. In regression, we
can solve analytically for the optimum. With more complex models
and error functions, we may need to use more complex optimization
methods, for example, gradient-based methods, simulated annealing,
or genetic algorithms.
For this to work well, the following conditions should be satisﬁed: First,
the hypothesis class of g(·) should be large enough, that is, have enough
capacity, to include the unknown function that generated the data that is
represented in r t in a noisy form. Second, there should be enough training data to allow us to pinpoint the correct (or a good enough) hypothesis
from the hypothesis class. Third, we should have a good optimization
method that ﬁnds the correct hypothesis given the training data.
Diﬀerent machine learning algorithms diﬀer either in the models they
assume (their hypothesis class/inductive bias), the loss measures they
employ, or the optimization procedure they use. We will see many examples in the coming chapters.

2.9

Notes
Mitchell proposed version spaces and the candidate elimination algorithm to incrementally build S and G as instances are given one by one;
see Mitchell 1997 for a recent review. The rectangle-learning is from exercise 2.4 of Mitchell 1997. Hirsh (1990) discusses how version spaces can
handle the case when instances are perturbed by small amount of noise.

2.10 Exercises

43

In one of the earliest works on machine learning, Winston (1975) proposed the idea of a “near miss." A near miss is a negative example that
is very much like a positive example. In our terminology, we see that
a near miss would be an instance that falls in the gray area between S
and G, an instance which would aﬀect the margin, and would hence be
more useful for learning, than an ordinary positive or negative example.
The instances that are close to the boundary are the ones that deﬁne it
(or support it); those which are surrounded by many instances with the
same label can be added/removed without aﬀecting the boundary.
Related to this idea is active learning where the learning algorithm can
generate instances itself and ask for them to be labeled, instead of passively being given them (Angluin 1988) (see exercise 4).
VC dimension was proposed by Vapnik and Chervonenkis in the early
1970s. A recent source is Vapnik 1995 where he writes, “Nothing is more
practical than a good theory” (p. x), which is as true in machine learning as
in any other branch of science. You should not rush to the computer; you
can save yourself from hours of useless programming by some thinking,
a notebook, and a pencil—you may also need an eraser.
The PAC model was proposed by Valiant (1984). The PAC analysis of
learning a rectangle is from Blumer et al. 1989. A good textbook on computational learning theory covering PAC learning and VC dimension is
Kearns and Vazirani 1994.

2.10

Exercises
1. Let us say our hypothesis class is a circle instead of a rectangle. What are the
parameters? How can the parameters of a circle hypothesis be calculated in
such a case? What if it is an ellipse? Why does it make more sense to use
an ellipse instead of a circle? How can you generalize your code to K > 2
classes?
2. Imagine our hypothesis is not one rectangle but a union of two (or m > 1)
rectangles. What is the advantage of such a hypothesis class? Show that any
class can be represented by such a hypothesis class with large enough m.
3. The complexity of most learning algorithms is a function of the training set.
Can you propose a ﬁltering algorithm that ﬁnds redundant instances?
4. If we have a supervisor who can provide us with the label for any x, where
should we choose x to learn with fewer queries?
5. In equation 2.13, we summed up the squares of the diﬀerences between the
actual value and the estimated value. This error function is the one most

44

x2

2 Supervised Learning

x1
Figure 2.11 A line separating positive and negative instances.

frequently used, but it is one of several possible error functions. Because
it sums up the squares of the diﬀerences, it is not robust to outliers. What
would be a better error function to implement robust regression?
6. Derive equation 2.17.
7. Assume our hypothesis class is the set of lines, and we use a line to separate
the positive and negative examples, instead of bounding the positive examples as in a rectangle, leaving the negatives outside (see ﬁgure 2.11). Show
that the VC dimension of a line is 3.
8. Show that the VC dimension of the triangle hypothesis class is 7 in two dimensions. (Hint: For best separation, it is best to place the seven points
equidistant on a circle.)
9. Assume as in exercise 7 that our hypothesis class is the set of lines. Write
down an error function that not only minimizes the number of misclassiﬁcations but also maximizes the margin.
10. One source of noise is error in the labels. Can you propose a method to ﬁnd
data points that are highly likely to be mislabeled?

2.11

References
Angluin, D. 1988. “Queries and Concept Learning.” Machine Learning 2: 319–
342.
Blumer, A., A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. 1989. “Learnability
and the Vapnik-Chervonenkis Dimension.” Journal of the ACM 36: 929–965.

2.11 References

45

Dietterich, T. G. 2003. “Machine Learning.” In Nature Encyclopedia of Cognitive
Science. London: Macmillan.
Hirsh, H. 1990. Incremental Version Space Merging: A General Framework for
Concept Learning. Boston: Kluwer.
Kearns, M. J., and U. V. Vazirani. 1994. An Introduction to Computational Learning Theory. Cambridge, MA: MIT Press.
Mitchell, T. 1997. Machine Learning. New York: McGraw-Hill.
Valiant, L. 1984. “A Theory of the Learnable.” Communications of the ACM 27:
1134–1142.
Vapnik, V. N. 1995. The Nature of Statistical Learning Theory. New York:
Springer.
Winston, P. H. 1975. “Learning Structural Descriptions from Examples.” In
The Psychology of Computer Vision, ed. P. H. Winston, 157–209. New York:
McGraw-Hill.

3

Bayesian Decision Theory

We discuss probability theory as the framework for making decisions
under uncertainty. In classiﬁcation, Bayes’ rule is used to calculate the probabilities of the classes. We generalize to discuss how
we can make rational decisions among multiple actions to minimize
expected risk. We also discuss learning association rules from data.

3.1

Introduction
Pr ogr am m i n g c om p ute r s to make inference from data is a cross
between statistics and computer science, where statisticians provide the
mathematical framework of making inference from data and computer
scientists work on the eﬃcient implementation of the inference methods.
Data comes from a process that is not completely known. This lack
of knowledge is indicated by modeling the process as a random process.
Maybe the process is actually deterministic, but because we do not have
access to complete knowledge about it, we model it as random and use
probability theory to analyze it. At this point, it may be a good idea to
jump to the appendix and review basic probability theory before continuing with this chapter.
Tossing a coin is a random process because we cannot predict at any
toss whether the outcome will be heads or tails—that is why we toss
coins, or buy lottery tickets, or get insurance. We can only talk about the
probability that the outcome of the next toss will be heads or tails. It may
be argued that if we have access to extra knowledge such as the exact
composition of the coin, its initial position, the force and its direction
that is applied to the coin when tossing it, where and how it is caught,
and so forth, the exact outcome of the toss can be predicted.

48

3 Bayesian Decision Theory

unobservable
variables
observable variable

The extra pieces of knowledge that we do not have access to are named
the unobservable variables. In the coin tossing example, the only observable variable is the outcome of the toss. Denoting the unobservables by
z and the observable as x, in reality we have
x = f (z)
where f (·) is the deterministic function that deﬁnes the outcome from
the unobservable pieces of knowledge. Because we cannot model the
process this way, we deﬁne the outcome X as a random variable drawn
from a probability distribution P (X = x) that speciﬁes the process.
The outcome of tossing a coin is heads or tails, and we deﬁne a random
variable that takes one of two values. Let us say X = 1 denotes that the
outcome of a toss is heads and X = 0 denotes tails. Such X are Bernoullidistributed where the parameter of the distribution po is the probability
that the outcome is heads:
P (X = 1) = po and P (X = 0) = 1 − P (X = 1) = 1 − po

sample

Assume that we are asked to predict the outcome of the next toss. If
we know po , our prediction will be heads if po > 0.5 and tails otherwise.
This is because if we choose the more probable case, the probability of
error, which is 1 minus the probability of our choice, will be minimum.
If this is a fair coin with po = 0.5, we have no better means of prediction
than choosing heads all the time or tossing a fair coin ourselves!
If we do not know P (X) and want to estimate this from a given sample,
then we are in the realm of statistics. We have a sample, X, containing
examples drawn from the probability distribution of the observables xt ,
ˆ
denoted as p(x). The aim is to build an approximator to it, p(x), using
the sample X.
In the coin tossing example, the sample contains the outcomes of the
past N tosses. Then using X, we can estimate po , which is the parameter
that uniquely speciﬁes the distribution. Our estimate of po is
ˆ
po =

#{tosses with outcome heads}
#{tosses}

Numerically using the random variables, xt is 1 if the outcome of toss t
is heads and 0 otherwise. Given the sample {heads, heads, heads, tails,
heads, tails, tails, heads, heads}, we have X = {1, 1, 1, 0, 1, 0, 0, 1, 1} and
the estimate is
ˆ
po =

N
t
t=1 x

N

=

6
9

49

3.2 Classiﬁcation

3.2

Classiﬁcation
We discussed credit scoring in section 1.2.2, where we saw that in a bank,
according to their past transactions, some customers are low-risk in that
they paid back their loans and the bank proﬁted from them and other
customers are high-risk in that they defaulted. Analyzing this data, we
would like to learn the class “high-risk customer” so that in the future,
when there is a new application for a loan, we can check whether that
person obeys the class description or not and thus accept or reject the
application. Using our knowledge of the application, let us say that we
decide that there are two pieces of information that are observable. We
observe them because we have reason to believe that they give us an
idea about the credibility of a customer. Let us say, for example, we
observe customer’s yearly income and savings, which we represent by
two random variables X1 and X2 .
It may again be claimed that if we had access to other pieces of knowledge such as the state of economy in full detail and full knowledge about
the customer, his or her intention, moral codes, and so forth, whether
someone is a low-risk or high-risk customer could have been deterministically calculated. But these are nonobservables and with what we can
observe, the credibility of a customer is denoted by a Bernoulli random
variable C conditioned on the observables X = [X1 , X2 ]T where C = 1
indicates a high-risk customer and C = 0 indicates a low-risk customer.
Thus if we know P (C|X1 , X2 ), when a new application arrives with X1 = x1
and X2 = x2 , we can
choose

C=1
C=0

if P (C = 1|x1 , x2 ) > 0.5
otherwise

or equivalently
(3.1)

Bayes’ rule

(3.2)

choose

C=1
C=0

if P (C = 1|x1 , x2 ) > P (C = 0|x1 , x2 )
otherwise

The probability of error is 1 − max(P (C = 1|x1 , x2 ), P (C = 0|x1 , x2 )).
This example is similar to the coin tossing example except that here, the
Bernoulli random variable C is conditioned on two other observable variables. Let us denote by x the vector of observed variables, x = [x1 , x2 ]T .
The problem then is to be able to calculate P (C|x). Using Bayes’ rule, it
can be written as
P (C)p(x|C)
P (C|x) =
p(x)

50

3 Bayesian Decision Theory

prior probability

P (C = 1) is called the prior probability that C takes the value 1, which
in our example corresponds to the probability that a customer is highrisk, regardless of the x value. It is called the prior probability because
it is the knowledge we have as to the value of C before looking at the
observables x, satisfying
P (C = 0) + P (C = 1) = 1

class likelihood

evidence

(3.3)

p(x|C) is called the class likelihood and is the conditional probability
that an event belonging to C has the associated observation value x. In
our case, p(x1 , x2 |C = 1) is the probability that a high-risk customer has
his or her X1 = x1 and X2 = x2 . It is what the data tells us regarding the
class.
p(x), the evidence, is the marginal probability that an observation x is
seen, regardless of whether it is a positive or negative example.
p(x) =

p(x, C) = p(x|C = 1)P (C = 1) + p(x|C = 0)P (C = 0)
C

posterior
probability

Combining the prior and what the data tells us using Bayes’ rule, we
calculate the posterior probability of the concept, P (C|x), after having
seen the observation, x.
prior × likelihood
evidence
Because of normalization by the evidence, the posteriors sum up to 1:

posterior =

P (C = 0|x) + P (C = 1|x) = 1
Once we have the posteriors, we decide by using equation 3.1. For now,
we assume that we know the prior and likelihoods; in later chapters, we
discuss how to estimate P (C) and p(x|C) from a given training sample.
In the general case, we have K mutually exclusive and exhaustive classes;
Ci , i = 1, . . . , K; for example, in optical digit recognition, the input is a
bitmap image and there are ten classes. We have the prior probabilities
satisfying
K

(3.4)

P (Ci ) ≥ 0 and

P (Ci ) = 1
i=1

(3.5)

p(x|Ci ) is the probability of seeing x as the input when it is known to
belong to class Ci . The posterior probability of class Ci can be calculated
as
p(x|Ci )P (Ci )
p(x|Ci )P (Ci )
= K
P (Ci |x) =
p(x)
k=1 p(x|Ck )P (Ck )

51

3.3 Losses and Risks

Bayes’ classifier

(3.6)

3.3

loss function
expected risk

and for minimum error, the Bayes’ classiﬁer chooses the class with the
highest posterior probability; that is, we
choose Ci if P (Ci |x) = max P (Ck |x)
k

Losses and Risks
It may be the case that decisions are not equally good or costly. A ﬁnancial institution when making a decision for a loan applicant should take
into account the potential gain and loss as well. An accepted low-risk
applicant increases proﬁt, while a rejected high-risk applicant decreases
loss. The loss for a high-risk applicant erroneously accepted may be different from the potential gain for an erroneously rejected low-risk applicant. The situation is much more critical and far from symmetry in other
domains like medical diagnosis or earthquake prediction.
Let us deﬁne action αi as the decision to assign the input to class Ci
and λik as the loss incurred for taking action αi when the input actually
belongs to Ck . Then the expected risk for taking action αi is
K

(3.7)

R(αi |x) =

λik P (Ck |x)
k=1

and we choose the action with minimum risk:
(3.8)

0/1 loss

(3.9)

choose αi if R(αi |x) = min R(αk |x)
k

Let us deﬁne K actions αi , i = 1, . . . , K, where αi is the action of assigning x to Ci . In the special case of the 0/1 loss case where
λik =

if i = k
if i = k

0
1

all correct decisions have no loss and all errors are equally costly. The
risk of taking action αi is
K

R(αi |x)

=

λik P (Ck |x)
k=1

=

P (Ck |x)
k=i

=

1 − P (Ci |x)

52

3 Bayesian Decision Theory

reject

(3.10)

because k P (Ck |x) = 1. Thus to minimize risk, we choose the most
probable case. In later chapters, for simplicity, we will always assume
this case and choose the class with the highest posterior, but note that
this is indeed a special case and rarely do applications have a symmetric,
0/1 loss. In the general case, it is a simple postprocessing to go from
posteriors to risks and to take the action to minimize the risk.
In some applications, wrong decisions—namely, misclassiﬁcations—
may have very high cost, and it is generally required that a more complex—
for example, manual—decision is made if the automatic system has low
certainty of its decision. For example, if we are using an optical digit recognizer to read postal codes on envelopes, wrongly recognizing the code
causes the envelope to be sent to a wrong destination.
In such a case, we deﬁne an additional action of reject or doubt, αK+1 ,
with αi , i = 1, . . . , K, being the usual actions of deciding on classes Ci , i =
1, . . . , K (Duda, Hart, and Stork 2001).
A possible loss function is
⎧
⎪ 0 if i = k
⎨
λ if i = K + 1
λik =
⎪
⎩
1 otherwise
where 0 < λ < 1 is the loss incurred for choosing the (K + 1)st action of
reject. Then the risk of reject is
K

(3.11)

R(αK+1 |x) =

λP (Ck |x) = λ
k=1

and the risk of choosing class Ci is
(3.12)

R(αi |x) =

P (Ck |x) = 1 − P (Ci |x)
k=i

The optimal decision rule is to
choose Ci

if R(αi |x) < R(αk |x) for all k = i and
R(αi |x) < R(αK+1 |x)

(3.13)

reject

if R(αK+1 |x) < R(αi |x), i = 1, . . . , K

Given the loss function of equation 3.10, this simpliﬁes to
choose Ci

if P (Ci |x) > P (Ck |x) for all k = i and
P (Ci |x) > 1 − λ

(3.14)

reject

otherwise

3.4 Discriminant Functions

53

This whole approach is meaningful if 0 < λ < 1: If λ = 0, we always
reject; a reject is as good as a correct classiﬁcation. If λ ≥ 1, we never
reject; a reject is as costly as, or costlier than, an error.

3.4
discriminant
functions

(3.15)

Discriminant Functions
Classiﬁcation can also be seen as implementing a set of discriminant functions, gi (x), i = 1, . . . , K, such that we
choose Ci if gi (x) = max gk (x)
k

We can represent the Bayes’ classiﬁer in this way by setting
gi (x) = −R(αi |x)
and the maximum discriminant function corresponds to minimum conditional risk. When we use the 0/1 loss function, we have
gi (x) = P (Ci |x)
or ignoring the common normalizing term, p(x), we can write
gi (x) = p(x|Ci )P (Ci )
decision regions

This divides the feature space into K decision regions R1 , . . . , RK , where
Ri = {x|gi (x) = maxk gk (x)}. The regions are separated by decision
boundaries, surfaces in feature space where ties occur among the largest
discriminant functions (see ﬁgure 3.1).
When there are two classes, we can deﬁne a single discriminant
g(x) = g1 (x) − g2 (x)
and we
choose

dichotomizer
polychotomizer

C1
C2

if g(x) > 0
otherwise

An example is a two-class learning problem where the positive examples can be taken as C1 and the negative examples as C2 . When K = 2,
the classiﬁcation system is a dichotomizer and for K ≥ 3, it is a polychotomizer.

54

x2

3 Bayesian Decision Theory

C1

reject
C2

C3

x1

Figure 3.1 Example of decision regions and decision boundaries.

3.5

utility theory

utility function
expected utility

(3.16)

Utility Theory
In equation 3.7, we deﬁned the expected risk and chose the action that
minimizes expected risk. We now generalize this to utility theory, which
is concerned with making rational decisions when we are uncertain about
the state. Let us say that given evidence x, the probability of state Sk is
calculated as P (Sk |x). We deﬁne a utility function, Uik , which measures
how good it is to take action αi when the state is Sk . The expected utility
is
EU(αi |x) =

Uik P (Sk |x)
k

A rational decision maker chooses the action that maximizes the expected utility
(3.17)

Choose αi if EU(αi |x) = max EU(αj |x)
j

In the context of classiﬁcation, decisions correspond to choosing one
of the classes, and maximizing the expected utility is equivalent to minimizing expected risk. Uik are generally measured in monetary terms, and
this gives us a way to deﬁne the loss matrix λik as well. For example, in

55

3.6 Association Rules

deﬁning a reject option (equation 3.10), if we know how much money we
will gain as a result of a correct decision, how much money we will lose
on a wrong decision, and how costly it is to defer the decision to a human
expert, depending on the particular application we have, we can ﬁll in the
correct values Uik in a currency unit, instead of 0, λ, and 1, and make our
decision so as to maximize expected earnings.
Note that maximizing expected utility is just one possibility; one may
deﬁne other types of rational behavior, for example, minimizing the worst
possible loss.
In the case of reject, we are choosing between the automatic decision
made by the computer program and human decision that is costlier but
assumed to have a higher probability of being correct. Similarly one can
imagine a cascade of multiple automatic decision makers, which as we
proceed are costlier but have a higher chance of being correct; we are going to discuss such cascades in chapter 17 where we talk about combining
multiple learners.

3.6
association rule

basket analysis

support

(3.18)
confidence

Association Rules
An association rule is an implication of the form X → Y where X is the
antecedent and Y is the consequent of the rule. One example of association rules is in basket analysis where we want to ﬁnd the dependency
between two items X and Y . The typical application is in retail where X
and Y are items sold, as we discussed in section 1.2.1.
In learning association rules, there are three measures that are frequently calculated:
Support of the association rule X → Y :
Support(X, Y ) ≡ P (X, Y ) =

#{customers who bought X and Y }
#{customers}

Conﬁdence of the association rule X → Y :
Conﬁdence(X → Y ) ≡ P (Y |X)

(3.19)

lift
interest

=
=

P (X, Y )
P (X)
#{customers who bought X and Y }
#{customers who bought X}

Lift, also known as interest of the association rule X → Y :

56

3 Bayesian Decision Theory

(3.20)

Apriori algorithm

Lift(X → Y ) =

P (X, Y )
P (Y |X)
=
P (X)P (Y )
P (Y )

There are other measures as well (Omiecinski 2003), but these three,
especially the ﬁrst two, are the most widely known and used. Conﬁdence
is the conditional probability, P (Y |X), which is what we normally calculate. To be able to say that the rule holds with enough conﬁdence, this
value should be close to 1 and signiﬁcantly larger than P (Y ), the overall
probability of people buying Y . We are also interested in maximizing the
support of the rule, because even if there is a dependency with a strong
conﬁdence value, if the number of such customers is small, the rule is
worthless. Support shows the statistical signiﬁcance of the rule, whereas
conﬁdence shows the strength of the rule. The minimum support and
conﬁdence values are set by the company, and all rules with higher support and conﬁdence are searched for in the database.
If X and Y are independent, then we expect lift to be close to 1; if the
ratio diﬀers—if P (Y |X) and P (Y ) are diﬀerent—we expect there to be a
dependency between the two items: If the lift is more than 1, we can say
that X makes Y more likely, and if the lift is less than 1, having X makes
Y less likely.
These formulas can easily be generalized to more than two items. For
example, {X, Y , Z} is a three-item set, and we may look for a rule, such
as X, Z → Y , that is, P (Y |X, Z). We are interested in ﬁnding all such rules
having high enough support and conﬁdence and because a sales database
is generally very large, we want to ﬁnd them by doing a small number of
passes over the database. There is an eﬃcient algorithm, called Apriori (Agrawal et al. 1996) that does this, which has two steps: (1) ﬁnding
frequent itemsets, that is, those which have enough support, and (2) converting them to rules with enough conﬁdence, by splitting the items into
two, as items in the antecedent and items in the consequent:
1. To ﬁnd frequent itemsets quickly (without complete enumeration of all
subsets of items), the Apriori algorithm uses the fact that for {X, Y , Z}
to be frequent (have enough support), all its subsets {X, Y }, {X, Z},
and {Y , Z} should be frequent as well—adding another item can never
increase support. That is, we only need to check for three-item sets all
of whose two-item subsets are frequent; or, in other words, if a twoitem set is known not to be frequent, all its supersets can be pruned
and need not be checked.

3.6 Association Rules

57

We start by ﬁnding the frequent one-item sets and at each step, inductively, from frequent k-item sets, we generate candidate k + 1-item sets
and then do a pass over the data to check if they have enough support.
The Apriori algorithm stores the frequent itemsets in a hash table for
easy access. Note that the number of candidate itemsets will decrease
very rapidly as k increases. If the largest itemset has n items, we need
a total of n + 1 passes over the data.
2. Once we ﬁnd the frequent k-item sets, we need to convert them to
rules by splitting the k items into two as antecedent and consequent.
Just like we do for generating the itemsets, we start by putting a single
consequent and k − 1 items in the antecedent. Then, for all possible
single consequents, we check if the rule has enough conﬁdence and
remove it if it does not.
Note that for the same itemset, there may be multiple rules with different subsets as antecedent and consequent. Then, inductively, we
check whether we can move another item from the antecedent to the
consequent. Rules with more items in the consequent are more speciﬁc and more useful. Here, as in itemset generation, we use the fact
that to be able to have rules with two items in the consequent with
enough conﬁdence, each of the two rules with single consequent by
itself should have enough conﬁdence; that is, we go from one consequent rules to two consequent rules and need not check for all possible
two-term consequents (exercise 7).

hidden variables

It should be kept in mind that a rule X → Y need not imply causality
but just an association. In a problem, there may also be hidden variables
whose values are never known through evidence. The advantage of using hidden variables is that the dependency structure can be more easily
deﬁned. For example, in basket analysis when we want to ﬁnd the dependencies among items sold, let us say we know that there is a dependency
among “baby food,” “diapers,” and “milk” in that a customer buying one
of these is very much likely to buy the other two. Instead of representing
dependencies among these three, we may designate a hidden node, “baby
at home,” as the hidden cause of the consumption of these three items.
Graphical models that we will discuss in chapter 16 allow us to represent
such hidden variables. When there are hidden nodes, their values are
estimated given the values of observed nodes and ﬁlled in.

58

3 Bayesian Decision Theory

3.7

Notes
Making decisions under uncertainty has a long history, and over time humanity has looked at all sorts of strange places for evidence to remove the
uncertainty: stars, crystal balls, and coﬀee cups. Reasoning from meaningful evidence using probability theory is only a few hundred years old;
see Newman 1988 for the history of probability and statistics and some
very early articles by Laplace, Bernoulli, and others who have founded the
theory.
Russell and Norvig (1995) give an excellent discussion of utility theory
and the value of information, also discussing the assignment of utilities
in monetary terms. Shafer and Pearl 1990 is an early collection of articles
on reasoning under uncertainty.
Association rules are successfully used in many data mining applications, and we see such rules on many Web sites that recommend books,
movies, music, and so on. The algorithm is very simple and its eﬃcient implementation on very large databases is critical (Zhang and Zhang
2002; Li 2006). Later, we will see in chapter 16 about graphical models
how to generalize from association rules to concepts that need not be
binary and where associations can be of diﬀerent types, also allowing
hidden variables.

3.8
likelihood ratio

log odds

Exercises
1. In a two-class problem, the likelihood ratio is
p(x|C1 )
p(x|C2 )
Write the discriminant function in terms of the likelihood ratio.
2. In a two-class problem, the log odds is deﬁned as
P (C1 |x)
log
P (C2 |x)
Write the discriminant function in terms of the log odds.
3. In a two-class, two-action problem, if the loss function is λ11 = λ22 = 0,
λ12 = 10, and λ21 = 1, write the optimal decision rule.
4. Propose a three-level cascade where when one level rejects, the next one is
used as in equation 3.10. How can we ﬁx the λ on diﬀerent levels?
5. Somebody tosses a fair coin and if the result is heads, you get nothing; otherwise you get $5. How much would you pay to play this game? What if the
win is $500 instead of $5?

3.9 References

59

6. Generalize the conﬁdence and support formulas for basket analysis to calculate k-dependencies, namely, P (Y |X1 , . . . , Xk ).
7. Show that as we move an item from the consequent to the antecedent, conﬁdence can never increase: conﬁdence(ABC → D) ≥ conﬁdence(AB → CD).
8. Associated with each item sold in basket analysis, if we also have a number
indicating how much the customer enjoyed the product, for example, in a
scale of 0 to 10, how can you use this extra information to calculate which
item to propose to a customer?
9. Show example transaction data where for the rule X → Y :
(a) Both support and conﬁdence are high.
(b) Support is high and conﬁdence is low.
(c) Support is low and conﬁdence is high.
(d) Both support and conﬁdence are low.

3.9

References
Agrawal, R., H. Mannila, R. Srikant, H. Toivonen, and A. Verkamo. 1996. “Fast
Discovery of Association Rules.” In Advances in Knowledge Discovery and
Data Mining, ed. U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, 307–328. Cambridge, MA: MIT Press.
Duda, R. O., P. E. Hart, and D. G. Stork. 2001. Pattern Classiﬁcation, 2nd ed.
New York: Wiley.
Li, J. 2006. “On Optimal Rule Discovery.” IEEE Transactions on Knowledge and
Data Discovery 18: 460–471.
Newman, J. R., ed. 1988. The World of Mathematics. Redmond, WA: Tempus.
Omiecinski, E. R. 2003. “Alternative Interest Measures for Mining Associations
in Databases.” IEEE Transactions on Knowledge and Data Discovery 15: 57–69.
Russell, S., and P. Norvig. 1995. Artiﬁcial Intelligence: A Modern Approach. New
York: Prentice Hall.
Shafer, G., and J. Pearl, eds. 1990. Readings in Uncertain Reasoning. San Mateo,
CA: Morgan Kaufmann.
Zhang, C., and S. Zhang. 2002. Association Rule Mining: Models and Algorithms.
New York: Springer.

4

Parametric Methods

Having discussed how to make optimal decisions when the uncertainty is modeled using probabilities, we now see how we can estimate these probabilities from a given training set. We start with the
parametric approach for classiﬁcation and regression. We discuss
the semiparametric and nonparametric approaches in later chapters. We introduce bias/variance dilemma and model selection methods for trading oﬀ model complexity and empirical error.

4.1

Introduction
A s t a t i s t i c is any value that is calculated from a given sample. In
statistical inference, we make a decision using the information provided
by a sample. Our ﬁrst approach is parametric where we assume that the
sample is drawn from some distribution that obeys a known model, for
example, Gaussian. The advantage of the parametric approach is that
the model is deﬁned up to a small number of parameters—for example,
mean, variance—the suﬃcient statistics of the distribution. Once those parameters are estimated from the sample, the whole distribution is known.
We estimate the parameters of the distribution from the given sample,
plug in these estimates to the assumed model, and get an estimated distribution, which we then use to make a decision. The method we use
to estimate the parameters of a distribution is maximum likelihood estimation. We also introduce Bayesian estimation, which we will continue
discussing in chapter 14.
We start with density estimation, which is the general case of estimating
p(x). We use this for classiﬁcation where the estimated densities are the
class densities, p(x|Ci ), and priors, P (Ci ), to be able to calculate the pos-

62

4

Parametric Methods

teriors, P (Ci |x), and make our decision. We then discuss regression where
the estimated density is p(y|x). In this chapter, x is one-dimensional and
thus the densities are univariate. We generalize to the multivariate case
in chapter 5.

4.2

Maximum Likelihood Estimation
Let us say we have an independent and identically distributed (iid) sample
X = {xt }N . We assume that xt are instances drawn from some known
t=1
probability density family, p(x|θ), deﬁned up to parameters, θ:
xt ∼ p(x|θ)

likelihood

We want to ﬁnd θ that makes sampling xt from p(x|θ) as likely as
possible. Because xt are independent, the likelihood of parameter θ given
sample X is the product of the likelihoods of the individual points:
N

(4.1)

p(xt |θ)

l(θ|X) ≡ p(X|θ) =
t=1

maximum likelihood
estimation

log likelihood

In maximum likelihood estimation, we are interested in ﬁnding θ that
makes X the most likely to be drawn. We thus search for θ that maximizes the likelihood, which we denote by l(θ|X). We can maximize the
log of the likelihood without changing the value where it takes its maximum. log(·) converts the product into a sum and leads to further computational simpliﬁcation when certain densities are assumed, for example,
containing exponents. The log likelihood is deﬁned as
N

(4.2)

log p(xt |θ)

L(θ|X) ≡ log l(θ|X) =
t=1

Let us now see some distributions that arise in the applications we
are interested in. If we have a two-class problem, the distribution we
use is Bernoulli. When there are K > 2 classes, its generalization is the
multinomial. Gaussian (normal) density is the one most frequently used
for modeling class-conditional input densities with numeric input. For
these three distributions, we discuss the maximum likelihood estimators
(MLE) of their parameters.

63

4.2 Maximum Likelihood Estimation

4.2.1

Bernoulli Density
In a Bernoulli distribution, there are two outcomes: An event occurs or
it does not; for example, an instance is a positive example of the class,
or it is not. The event occurs and the Bernoulli random variable X takes
the value 1 with probability p, and the nonoccurrence of the event has
probability 1 − p and this is denoted by X taking the value 0. This is
written as

(4.3)

P (x) = px (1 − p)1−x , x ∈ {0, 1}
The expected value and variance can be calculated as
E[X]

=

xp(x) = 1 · p + 0 · (1 − p) = p
x

Var(X)

(x − E[X])2 p(x) = p(1 − p)

=
x

p is the only parameter and given an iid sample X = {xt }N , where
t=1
ˆ
xt ∈ {0, 1}, we want to calculate its estimator, p. The log likelihood is
N

L(p|X)

=

t

t

p(x ) (1 − p)(1−x )

log
t=1

⎞

⎛

xt ⎠ log(1 − p)

xt log p + ⎝N −

=
t

t

ˆ
p that maximizes the log likelihood can be found by solving for dL/dp =
0. The hat (circumﬂex) denotes that it is an estimate.
(4.4)

ˆ
p=

xt
N
t

The estimate for p is the ratio of the number of occurrences of the event
to the number of experiments. Remembering that if X is Bernoulli with
p, E[X] = p, and, as expected, the maximum likelihood estimator of the
mean is the sample average.
Note that the estimate is a function of the sample and is another ranˆ
dom variable; we can talk about the distribution of pi given diﬀerent Xi
sampled from the same p(x). For example, the variance of the distriˆ
bution of pi is expected to decrease as N increases; as the samples get
bigger, they (and hence their averages) get more similar.

64

4

4.2.2

Parametric Methods

Multinomial Density
Consider the generalization of Bernoulli where instead of two states, the
outcome of a random event is one of K mutually exclusive and exhaustive
states, for example, classes, each of which has a probability of occurring
K
pi with i=1 pi = 1. Let x1 , x2 , . . . , xK are the indicator variables where xi
is 1 if the outcome is state i and 0 otherwise.
K

(4.5)

x

P (x1 , x2 , . . . , xK ) =

pi i
i=1

Let us say we do N such independent experiments with outcomes X =
{xt }N where
t=1
1
0

xt =
i
with
(4.6)

4.2.3

i

if experiment t chooses state i
otherwise

xt = 1. The MLE of pi is
i

xt
i
N
The estimate for the probability of state i is the ratio of experiments
with outcome of state i to the total number of experiments. There are two
ways one can get this: If xi are 0/1, then they can be thought of as K separate Bernoulli experiments. Or, one can explicitly write the log likelihood
and ﬁnd pi that maximize it (subject to the condition that i pi = 1).
t

ˆ
pi =

Gaussian (Normal) Density
X is Gaussian (normal) distributed with mean E[X] ≡ μ and variance
Var(X) ≡ σ 2 , denoted as N (μ, σ 2 ), if its density function is

(4.7)

p(x) = √

1
(x − μ)2
exp −
2σ 2
2π σ

, −∞ < x < ∞

Given a sample X = {xt }N with xt ∼ N (μ, σ 2 ), the log likelihood is
t=1
N
(xt − μ)2
log(2π ) − N log σ − t
2
2σ 2
The MLE that we ﬁnd by taking the partial derivatives of the log likelihood and setting them equal to 0 are

L(μ, σ |X) = −

(4.8)

m

=

s2

=

xt
N
t
2
t (x − m)
N
t

4.3 Evaluating an Estimator: Bias and Variance

65

We follow the usual convention and use Greek letters for the population parameters and Roman letters for their estimates from the sample.
ˆ
Sometimes, the hat is also used to denote the estimator, for example, μ .

4.3

mean square error

(4.9)
bias

(4.10)
unbiased estimator

Evaluating an Estimator: Bias and Variance
Let X be a sample from a population speciﬁed up to a parameter θ, and
let d = d(X) be an estimator of θ. To evaluate the quality of this estimator, we can measure how much it is diﬀerent from θ, that is, (d(X) − θ)2 .
But since it is a random variable (it depends on the sample), we need to
average this over possible X and consider r (d, θ), the mean square error
of the estimator d deﬁned as
r (d, θ) = E[(d(X) − θ)2 ]
The bias of an estimator is given as
bθ (d) = E[d(X)] − θ
If bθ (d) = 0 for all θ values, then we say that d is an unbiased estimator
of θ. For example, with xt drawn from some density with mean μ, the
sample average, m, is an unbiased estimator of the mean, μ, because
E[m] = E

xt
N
t

=

1
N

E[xt ] =
t

Nμ
=μ
N

This means that though on a particular sample, m may be diﬀerent
from μ, if we take many such samples, Xi , and estimate many mi =
m(Xi ), their average will get close to μ as the number of such samples
increases. m is also a consistent estimator, that is, Var(m) → 0 as N → ∞.
xt
N
t

Var(m) = Var

=

1
N2

Var(xt ) =
t

Nσ 2
σ2
=
N2
N

As N, the number of points in the sample, gets larger, m deviates less
from μ. Let us now check, s 2 , the MLE of σ 2 :
s2

=

E[s 2 ]

=

− m)2
(xt )2 − Nm2
= t
N
N
E[(xt )2 ] − N · E[m2 ]
t
N
t (x

t

Given that Var(X) = E[X 2 ] − E[X]2 , we get E[X 2 ] = Var(X) + E[X]2 ,
and we can write
E[(xt )2 ] = σ 2 + μ 2 and E[m2 ] = σ 2 /N + μ 2

66

4

Parametric Methods

Then, plugging these in, we get
E[s 2 ] =

N(σ 2 + μ 2 ) − N(σ 2 /N + μ 2 )
=
N

N −1
σ2 = σ2
N

which shows that s 2 is a biased estimator of σ 2 . (N/(N − 1))s 2 is an
unbiased estimator. However when N is large, the diﬀerence is negligable.
This is an example of an asymptotically unbiased estimator whose bias
goes to 0 as N goes to inﬁnity.
The mean square error can be rewritten as follows—d is short for d(X):
E (d − θ)2
E (d − E[d] + E[d] − θ)2

=

E (d − E[d])2 + (E[d] − θ)2 + 2(E[d] − θ)(d − E[d])

=

E (d − E[d])2 + E (E[d] − θ)2 + 2E [(E[d] − θ)(d − E[d])]

=
(4.11)

=
=

E (d − E[d])2 + (E[d] − θ)2 + 2(E[d] − θ)E[d − E[d]]

=

r (d, θ)

E (d − E[d])2 + (E[d] − θ)2
var iance

variance

(4.12)

4.4

bias 2

The two equalities follow because E[d] is a constant and therefore E[d]−
θ also is a constant, and because E[d − E[d]] = E[d] − E[d] = 0. In
equation 4.11, the ﬁrst term is the variance that measures how much, on
average, di vary around the expected value (going from one dataset to
another), and the second term is the bias that measures how much the
expected value varies from the correct value θ (ﬁgure 4.1). We then write
error as the sum of these two terms, the variance and the square of the
bias:
r (d, θ) = Var(d) + (bθ (d))2

The Bayes’ Estimator
Sometimes, before looking at a sample, we (or experts of the application)
may have some prior information on the possible value range that a parameter, θ, may take. This information is quite useful and should be
used, especially when the sample is small. The prior information does
not tell us exactly what the parameter value is (otherwise we would not

67

4.4 The Bayes’ Estimator

variance
di
θ

E[ d]

bias
Figure 4.1 θ is the parameter to be estimated. di are several estimates (denoted
by ‘×’) over diﬀerent samples Xi . Bias is the diﬀerence between the expected
value of d and θ. Variance is how much di are scattered around the expected
value. We would like both to be small.

need the sample), and we model this uncertainty by viewing θ as a random variable and by deﬁning a prior density for it, p(θ). For example, let
us say we are told that θ is approximately normal and with 90 percent
conﬁdence, θ lies between 5 and 9, symmetrically around 7. Then we can
write p(θ) to be normal with mean 7 and because
θ−μ
< 1.64}
σ
P {μ − 1.64σ < θ < μ + 1.64σ }
P {−1.64 <

prior density

posterior density

(4.13)

=

0.9

=

0.9

we take 1.64σ = 2 and use σ = 2/1.64. We can thus assume p(θ) ∼
N (7, (2/1.64)2 ).
The prior density, p(θ), tells us the likely values that θ may take before
looking at the sample. We combine this with what the sample data tells
us, namely, the likelihood density, p(X|θ), using Bayes’ rule, and get the
posterior density of θ, which tells us the likely θ values after looking at
the sample:
p(θ|X) =

p(X|θ)p(θ)
=
p(X)

p(X|θ)p(θ)
p(X|θ )p(θ )dθ

For estimating the density at x, we have
p(x|X)

=

p(x, θ|X)dθ

=

p(x|θ, X)p(θ|X)dθ

=

p(x|θ)p(θ|X)dθ

68

4

Parametric Methods

p(x|θ, X) = p(x|θ) because once we know θ, the suﬃcient statistics,
we know everything about the distribution. Thus we are taking an average
over predictions using all values of θ, weighted by their probabilities. If
we are doing a prediction in the form, y = g(x|θ), as in regression, then
we have
y=

maximum a
posteriori estimate

(4.14)

g(x|θ)p(θ|X)dθ

Evaluating the integrals may be quite diﬃcult, except in cases where
the posterior has a nice form. When the full integration is not feasible,
we reduce it to a single point. If we can assume that p(θ|X) has a narrow peak around its mode, then using the maximum a posteriori (MAP)
estimate will make the calculation easier:
θMAP = arg max p(θ|X)
θ

thus replacing a whole density with a single point, getting rid of the integral and using as
p(x|X)

=

p(x|θMAP )

yMAP

=

g(x|θMAP )

If we have no prior reason to favor some values of θ, then the prior
density is ﬂat and the posterior will have the same form as the likelihood, p(X|θ), and the MAP estimate will be equivalent to the maximum
likelihood estimate (section 4.2) where we have
(4.15)
Bayes’ estimator

(4.16)

θML = arg max p(X|θ)
θ

Another possibility is the Bayes’ estimator, which is deﬁned as the expected value of the posterior density
θBayes = E[θ|X] =

θp(θ|X)dθ

The reason for taking the expected value is that the best estimate of
a random variable is its mean. Let us say θ is the variable we want to
predict with E[θ] = μ. It can be shown that if c, a constant value, is our
estimate of θ, then
E[(θ − c)2 ]
(4.17)

=

E[(θ − μ + μ − c)2 ]

=

E[(θ − μ)2 ] + (μ − c)2

69

4.5 Parametric Classiﬁcation

which is minimum if c is taken as μ. In the case of a normal density, the
mode is the expected value and if p(θ|X) is normal, then θBayes = θMAP .
2
As an example, let us suppose xt ∼ N (θ, σ 2 ) and θ ∼ N (μ0 , σ0 ),
2
where μ0 , σ0 , and σ 2 are known:
p(X|θ)

=

p(θ)

=

1
(2π )N/2 σ N
√

exp −

− θ)2
2σ 2

t (x

t

1
(θ − μ0 )2
exp −
2
2π σ0
2σ0

It can be shown that p(θ|X) is normal with
(4.18)

E[θ|X] =

2
1/σ0
N/σ 2
m+
μ
2
2 0
N/σ 2 + 1/σ0
N/σ 2 + 1/σ0

Thus the Bayes’ estimator is a weighted average of the prior mean μ0
and the sample mean m, with weights being inversely proportional to
their variances. As the sample size N increases, the Bayes’ estimator gets
closer to the sample average, using more the information provided by the
2
sample. When σ0 is small, that is, when we have little prior uncertainty
regarding the correct value of θ, or when N is small, our prior guess μ0
has a higher eﬀect.
Note that both MAP and Bayes’ estimators reduce the whole posterior
density to a single point and lose information unless the posterior is
unimodal and makes a narrow peak around these points. With computation getting cheaper, we can use a Monte Carlo approach that generates
samples from the posterior density (Andrieu et al. 2003). There also are
approximation methods one can use to evaluate the full integral. We are
going to discuss Bayesian estimation in more detail in chapter 14.

4.5

Parametric Classiﬁcation
We saw in chapter 3 that using the Bayes’ rule, we can write the posterior
probability of class Ci as

(4.19)

P (Ci |x) =

p(x|Ci )P (Ci )
=
p(x)

p(x|Ci )P (Ci )
K
k=1

p(x|Ck )P (Ck )

and use the discriminant function
gi (x) = p(x|Ci )P (Ci )

70

4

Parametric Methods

or equivalently
(4.20)

gi (x) = log p(x|Ci ) + log P (Ci )
If we can assume that p(x|Ci ) are Gaussian

(4.21)

1
(x − μi )2
exp −
p(x|Ci ) = √
2π σi
2σi2
equation 4.20 becomes

(4.22)

gi (x) = −

1
(x − μi )2
log 2π − log σi −
+ log P (Ci )
2
2σi2

Let us see an example: Assume we are a car company selling K different cars, and for simplicity, let us say that the sole factor that aﬀects
a customer’s choice is his or her yearly income, which we denote by x.
Then P (Ci ) is the proportion of customers who buy car type i. If the
yearly income distributions of such customers can be approximated with
a Gaussian, then p(x|Ci ), the probability that a customer who bought car
type i has income x, can be taken N (μi , σi2 ), where μi is the mean income
of such customers and σi2 is their income variance.
When we do not know P (Ci ) and p(x|Ci ), we estimate them from a sample and plug in their estimates to get the estimate for the discriminant
function. We are given a sample
(4.23)

X = {xt , r t }N
t=1
where x ∈

(4.24)

1
0

rit =

is one-dimensional and r ∈ {0, 1}K such that
if x t ∈ Ci
if x t ∈ Ck , k = i

For each class separately, the estimates for the means and variances
are (relying on equation 4.8)
(4.25)

mi

=

(4.26)

si2

=

t

xt rit
t
t ri

t (x

t

− mi )2 rit
t
t ri

and the estimates for the priors are (relying on equation 4.6)
(4.27)

ˆ
P (Ci ) =

t
t ri

N

71

4.5 Parametric Classiﬁcation

(a) Likelihoods
0.4

p(x|Ci)

0.3
0.2
0.1
0
−10

−8

−6

−4

−2

0
x

2

4

6

8

10

4

6

8

10

(b) Posteriors with equal priors
1

p(Ci|x)

0.8
0.6
0.4
0.2
0
−10

−8

−6

−4

−2

0
x

2

Figure 4.2 (a) Likelihood functions and (b) posteriors with equal priors for two
classes when the input is one-dimensional. Variances are equal and the posteriors intersect at one point, which is the threshold of decision.

Plugging these estimates into equation 4.22, we get
(4.28)

gi (x) = −

1
(x − mi )2
ˆ
log 2π − log si −
+ log P (Ci )
2
2si2

The ﬁrst term is a constant and can be dropped because it is common
in all gi (x). If the priors are equal, the last term can also be dropped. If
we can further assume that variances are equal, we can write
(4.29)

gi (x) = −(x − mi )2
and thus we assign x to the class with the nearest mean:
Choose Ci if |x − mi | = min |x − mk |
k

With two adjacent classes, the midpoint between the two means is the
threshold of decision (see ﬁgure 4.2).
g1 (x)

=

g2 (x)

72

4

Parametric Methods

(a) Likelihoods

i

p(x|C )

0.4
0.2
0
−10

−8

−6

−4

−2

0
2
4
x
(b) Posteriors with equal priors

−8

−6

−4

−2

−8

−6

−4

−2

6

8

10

4

6

8

10

4

6

8

10

i

p(C |x)

1
0.5
0
−10

0
2
x
(c) Expected risks

i

R(α |x)

1
0.5
0
−10

0
x

2

Figure 4.3 (a) Likelihood functions and (b) posteriors with equal priors for two
classes when the input is one-dimensional. Variances are unequal and the posteriors intersect at two points. In (c), the expected risks are shown for the two
classes and for reject with λ = 0.2 (section 3.3).

(x − m1 )2

=

x

=

(x − m2 )2
m1 + m 2
2

When the variances are diﬀerent, there are two thresholds (see ﬁgure 4.3), which can be calculated easily (exercise 4). If the priors are
diﬀerent, this has the eﬀect of moving the threshold of decision toward
the mean of the less likely class.
Here we use the maximum likelihood estimators for the parameters
but if we have some prior information about them, for example, for the
means, we can use a Bayesian estimate of p(x|Ci ) with prior on μi .
One note of caution is necessary here: When x is continuous, we should
not immediately rush to use Gaussian densities for p(x|Ci ). The classiﬁcation algorithm—that is, the threshold points—will be wrong if the densities are not Gaussian. In statistical literature, tests exist to check for

73

4.6 Regression

normality, and such a test should be used before assuming normality.
In the case of one-dimensional data, the easiest test is to plot the histogram and to check visually whether the density is bell-shaped, namely,
unimodal and symmetric around the center.
This is the likelihood-based approach to classiﬁcation where we use
data to estimate the densities separately, calculate posterior densities
using Bayes’ rule, and then get the discriminant. In later chapters, we
discuss the discriminant-based approach where we bypass the estimation
of densities and directly estimate the discriminants.

4.6

Regression
In regression, we would like to write the numeric output, called the dependent variable, as a function of the input, called the independent variable.
We assume that the numeric output is the sum of a deterministic function
of the input and random noise:
r = f (x) +
where f (x) is the unknown function, which we would like to approximate
by our estimator, g(x|θ), deﬁned up to a set of parameters θ. If we
assume that is zero mean Gaussian with constant variance σ 2 , namely,
∼ N (0, σ 2 ), and placing our estimator g(·) in place of the unknown
function f (·), we have (ﬁgure 4.4)

(4.30)

p(r |x) ∼ N (g(x|θ), σ 2 )
We again use maximum likelihood to learn the parameters θ. The pairs
(xt , r t ) in the training set are drawn from an unknown joint probability
density p(x, r ), which we can write as
p(x, r ) = p(r |x)p(x)
p(r |x) is the probability of the output given the input, and p(x) is the
input density. Given an iid sample X = {xt , r t }N , the log likelihood is
t=1
N

L(θ|X)

=

p(xt , r t )

log
t=1
N

=

N

p(r t |xt ) + log

log
t=1

p(xt )
t=1

74

4

Parametric Methods

Figure 4.4 Regression assumes 0 mean Gaussian noise added to the model;
here, the model is linear.

We can ignore the second term since it does not depend on our estimator, and we have
N

(4.31)

L(θ|X)

=
=
=

1
[r t − g(xt |θ)]2
exp −
2σ 2
2πσ
t=1
⎤
⎡
N
N
1
1
t
t
2⎦
exp ⎣−
[r − g(x |θ)]
log √
2σ 2 t=1
2πσ
log

√

√
−N log( 2πσ ) −

1
2σ 2

N

[r t − g(xt |θ)]2
t=1

The ﬁrst term is independent of the parameters θ and can be dropped,
as can the factor 1/σ 2 . Maximizing this is equivalent to minimizing
N

(4.32)

least squares
estimate

linear regression

E(θ|X) =

1
[r t − g(xt |θ)]2
2 t=1

which is the most frequently used error function, and θ that minimize it
are called the least squares estimates. This is a transformation frequently
done in statistics: When the likelihood l contains exponents, instead of
maximizing l, we deﬁne an error function, E = − log l, and minimize it.
In linear regression, we have a linear model
g(xt |w1 , w0 ) = w1 xt + w0

75

4.6 Regression

and taking the derivative of the sum of squared errors (equation 4.32)
with respect to w1 and w0 , we have two equations in two unknowns
rt

=

xt

Nw0 + w1

t

t
t t

=

r x
t

(xt )2

xt + w1

w0
t

t

which can be written in vector-matrix form as Aw = y where
A=

polynomial
regression

N

xt
t 2
t (x )
t

t

xt

, w=

w0
w1

, y=

rt
t t
tr x
t

and can be solved as w = A−1 y.
In the general case of polynomial regression, the model is a polynomial
in x of order k
g(xt |wk , . . . , w2 , w1 , w0 ) = wk (xt )k + · · · + w2 (xt )2 + w1 xt + w0
The model is still linear with respect to the parameters and taking the
derivatives, we get k+1 equations in k+1 unknowns, which can be written
in vector matrix form Aw = y where we have
⎡
⎤
t
t 2
t k
···
N
tx
t (x )
t (x )
⎢
t 2
t 3
t k+1 ⎥
⎢ t xt
⎥
···
t (x )
t (x )
t (x )
⎢
⎥
⎥
A = ⎢ .
⎢ .
⎥
⎣ .
⎦
t k
t k+1
t k+2
t 2k
···
t (x )
t (x )
t (x )
t (x )
⎤
⎤
⎡
⎡
t
w0
tr
⎥
⎥
⎢
⎢
⎥
⎢ w1 ⎥
⎢ t r t xt
⎥
⎥
⎢
⎢
t
t 2 ⎥
⎢ w2 ⎥
⎢ t r (x )
⎥, y = ⎢
⎥
w = ⎢
⎥
⎥
⎢ .
⎢ .
⎥
⎥
⎢ .
⎢ .
⎦
⎦
⎣ .
⎣ .
t
t k
wk
r (x )
t
We can
⎡
1
⎢
⎢ 1
⎢
D=⎢ .
⎢ .
⎣ .
1

write A = DT D and y = DT r where
⎤
⎡
x1 (x1 )2 · · · (x1 )k
r1
⎢ 2
2
2 2
2 k ⎥
⎢ r
x
(x )
· · · (x ) ⎥
⎥
⎢
⎥,r = ⎢ .
⎥
⎢ .
⎦
⎣ .
N
N 2
N k
x
rN
(x )
· · · (x )

and we can then solve for the parameters as
(4.33)

w = (DT D)−1 DT r

⎤
⎥
⎥
⎥
⎥
⎥
⎦

76

4

relative square
error

(4.34)

coefficient of
determination

Parametric Methods

Assuming Gaussian distributed error and maximizing likelihood corresponds to minimizing the sum of squared errors. Another measure is the
relative square error (RSE):
ERSE =

t [r

− g(xt |θ)]2
t
2
t (r − r )

t

If ERSE is close to 1, then our prediction is as good as predicting by
the average; as it gets closer to 0, we have better ﬁt. If ERSE is close to
1, this means that using a model based on input x does not work better
than using the average which would be our estimator if there were no x;
if ERSE is close to 0, input x helps.
To check whether regression makes a good ﬁt, a measure is the coeﬃcient of determination that is
R 2 = 1 − ERSE
and for regression to be considered useful, we require R 2 to be close to
1.
Remember that for best generalization, we should adjust the complexity of our learner model to the complexity of the data. In polynomial
regression, the complexity parameter is the order of the ﬁtted polynomial, and therefore we need to ﬁnd a way to choose the best order that
minimizes the generalization error, that is, tune the complexity of the
model to best ﬁt the complexity of the function inherent in the data.

4.7

Tuning Model Complexity: Bias/Variance Dilemma
Let us say that a sample X = {xt , r t } is drawn from some unknown joint
probability density p(x, r ). Using this sample, we construct our estimate
g(·). The expected square error (over the joint density) at x can be written
as (using equation 4.17)

(4.35)

E[(r − g(x))2 |x] = E[(r − E[r |x])2 |x] + (E[r |x] − g(x))2
noise

squar ed er r or

The ﬁrst term on the right is the variance of r given x; it does not
depend on g(·) or X. It is the variance of noise added, σ 2 . This is the
part of error that can never be removed, no matter what estimator we use.
The second term quantiﬁes how much g(x) deviates from the regression
function, E[r |x]. This does depend on the estimator and the training set.

77

4.7 Tuning Model Complexity: Bias/Variance Dilemma

It may be the case that for one sample, g(x) may be a very good ﬁt; and
for some other sample, it may make a bad ﬁt. To quantify how well an
estimator g(·) is, we average over possible datasets.
The expected value (average over samples X, all of size N and drawn
from the same joint density p(r , x)) is (using equation 4.11)
(4.36)

EX [(E[r |x]−g(x))2 |x] = (E[r |x] − EX [g(x)])2 + EX [(g(x) − EX [g(x)])2 ]
bias

var iance

As we discussed before, bias measures how much g(x) is wrong disregarding the eﬀect of varying samples, and variance measures how much
g(x) ﬂuctuate around the expected value, E[g(x)], as the sample varies.
We want both to be small.
Let us see a didactic example: To estimate the bias and the variance,
we generate a number of datasets Xi = {xt , rit }, i = 1, . . . , M, from some
i
known f (·) with added noise, use each dataset to form an estimator gi (·),
and calculate bias and variance. Note that in real life, we cannot do this
because we do not know f (·) or the parameters of the added noise. Then
E[g(x)] is estimated by the average over gi (·):
g(x) =

1
M

M

gi (x)
i=1

Estimated bias and variance are
Bias2 (g)
Variance(g)

=

1
N

=

1
NM

[g(xt ) − f (xt )]2
t

[gi (xt ) − g(xt )]2
t

i

Let us see some models of diﬀerent complexity: The simplest is a constant ﬁt
gi (x) = 2
This has no variance because we do not use the data and all gi (x) are the
same. But the bias is high, unless of course f (x) is close to 2 for all x. If
we take the average of r t in the sample
rit /N

gi (x) =
t

instead of the constant 2, this decreases the bias because we would expect the average in general to be a better estimate. But this increases the

78

4

(a) Function and data

Parametric Methods

(b) Order 1

5

5

0

0

−5

0

1

2

3

4

5

−5

0

1

(c) Order 3

2

3

4

5

4

5

(d) Order 5

5

5

0

0

−5

0

1

2

3

4

5

−5

0

1

2

3

Figure 4.5 (a) Function, f (x) = 2 sin(1.5x), and one noisy (N (0, 1)) dataset
sampled from the function. Five samples are taken, each containing twenty instances. (b), (c), (d) are ﬁve polynomial ﬁts, namely, gi (·), of order 1, 3, and 5.
For each case, dotted line is the average of the ﬁve ﬁts, namely, g(·).

bias/variance
dilemma

variance because the diﬀerent samples Xi would have diﬀerent average
values. Normally in this case the decrease in bias would be larger than
the increase in variance, and error would decrease.
In the context of polynomial regression, an example is given in ﬁgure 4.5. As the order of the polynomial increases, small changes in the
dataset cause a greater change in the ﬁtted polynomials; thus variance
increases. But a complex model on the average allows a better ﬁt to the
underlying function; thus bias decreases (see ﬁgure 4.6). This is called
the bias/variance dilemma and is true for any machine learning system
and not only for polynomial regression (Geman, Bienenstock, and Doursat 1992). To decrease bias, the model should be ﬂexible, at the risk of

79

4.7 Tuning Model Complexity: Bias/Variance Dilemma

4

3.5

3

Error

2.5

2
Error

1.5
Bias

1

Variance

0.5

0
1

2

3
Order

4

5

Figure 4.6 In the same setting as that of ﬁgure 4.5, using one hundred models
instead of ﬁve, bias, variance, and error for polynomials of order 1 to 5. Order
1 has the smallest variance. Order 5 has the smallest bias. As the order is
increased, bias decreases but variance increases. Order 3 has the minimum error.

underfitting
overfitting

having high variance. If the variance is kept low, we may not be able to
make a good ﬁt to data and have high bias. The optimal model is the one
that has the best trade-oﬀ between the bias and the variance.
If there is bias, this indicates that our model class does not contain
the solution; this is underﬁtting. If there is variance, the model class is
too general and also learns the noise; this is overﬁtting. If g(·) is of the
same hypothesis class with f (·), for example, a polynomial of the same
order, we have an unbiased estimator, and estimated bias decreases as
the number of models increase. This shows the error-reducing eﬀect of
choosing the right model (which we called inductive bias in chapter 2—
the two uses of “bias” are diﬀerent but not unrelated). As for variance, it
also depends on the size of the training set; the variability due to sample
decreases as the sample size increases. To sum up, to get a small value of
error, we should have the proper inductive bias (to get small bias in the
statistical sense) and have a large enough dataset so that the variability
of the model can be constrained with the data.

80

4

Parametric Methods

Note that when the variance is large, bias is low: this indicates that g(x)
is a good estimator. So to get a small value of error, we can take a large
number of high-variance models and use their average as our estimator.
We will discuss such approaches for model combination in chapter 17.

4.8

cross-validation

regularization

(4.37)

Model Selection Procedures
There are a number of procedures we can use to ﬁne-tune model complexity.
In practice, the method we use to ﬁnd the optimal complexity is crossvalidation. We cannot calculate bias and variance for a model, but we can
calculate the total error. Given a dataset, we divide it into two parts as
training and validation sets, train candidate models of diﬀerent complexities, and test their error on the validation set left out during training.
As the model complexity increases, training error keeps decreasing. The
error on the validation set decreases up to a certain level of complexity,
then stops decreasing or does not decrease further signiﬁcantly, or even
increases if there is signiﬁcant noise. This “elbow” corresponds to the
optimal complexity level (see ﬁgure 4.7).
In real life, we cannot calculate bias and hence error as we do in ﬁgure 4.6; the validation error in ﬁgure 4.7 is an estimate of that except
that it also contains noise: Even if we have the right model that there
is no bias and large enough data that variance is negligable, there may
still be nonzero validation error. Note that the validation error of ﬁgure 4.7 is not as V-shaped as the error of ﬁgure 4.6 because the former
uses more training data and we know that we can constrain variance with
more data. Indeed we see in ﬁgure 4.5(d) that even the ﬁfth-order polynomial behaves like a third-order where there is data; for example, at the
two extremes where there are fewer data points, it is not as accurate.
Another approach that is used frequently is regularization (Breiman
1998). In this approach, we write an augmented error function
E = error on data + λ · model complexity
This has a second term that penalizes complex models with large variance, where λ gives the weight of this penalty. When we minimize the
augmented error function instead of the error on data only, we penalize complex models and thus decrease variance. If λ is taken too large,
only very simple models are allowed and we risk introducing bias. λ is
optimized using cross-validation.

81

4.8 Model Selection Procedures

(a) Data and fitted polynomials
5

0

−5

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

(b) Error vs. polynomial order
3
Training
Validation

2.5
2
1.5
1
0.5

1

2

3

4

5

6

7

8

Figure 4.7 In the same setting as that of ﬁgure 4.5, training and validation
sets (each containing 50 instances) are generated. (a) Training data and ﬁtted
polynomials of order from 1 to 8. (b) Training and validation errors as a function
of the polynomial order. The “elbow” is at 3.

AIC
BIC

Another way we can view equation 4.37 is by regarding E as the error
on new test data. The ﬁrst term on the right is the training error and the
second is an optimism term estimating the discrepancy between training
and test error (Hastie, Tibshirani, and Friedman 2001). Methods such
as Akaike’s information criterion (AIC) and Bayesian information criterion
(BIC) work by estimating this optimism and adding it to the training error
to estimate test error, without any need for validation. The magnitude of
this optimism term increases linearly with d, the number of inputs (here,
it is k+1), and decreases as N, training set size, increases; it also increases
with σ 2 , the variance of the noise added (which we can estimate from the
error of a low-bias model). For models that are not linear, d should be

82

4

structural risk
minimization

minimum
description length

Bayesian model
selection

(4.38)

Parametric Methods

replaced with the “eﬀective” number of parameters.
Structural risk minimization (SRM) (Vapnik 1995) uses a set of models
ordered in terms of their complexities. An example is polynomials of increasing order. The complexity is generally given by the number of free
parameters. VC dimension is another measure of model complexity. In
equation 4.37, we can have a set of decreasing λi to get a set of models
ordered in increasing complexity. Model selection by SRM then corresponds to ﬁnding the model simplest in terms of order and best in terms
of empirical error on the data.
Minimum description length (MDL) (Rissanen 1978; Grünwald 2007)
uses an information theoretic measure. Kolmogorov complexity of a dataset
is deﬁned as the shortest description of the data. If the data is simple,
it has a short complexity; for example, if it is a sequence of ‘0’s, we can
just write ‘0’ and the length of the sequence. If the data is completely
random, then we cannot have any description of the data shorter than
the data itself. If a model is appropriate for the data, then it has a good
ﬁt to the data, and instead of the data, we can send/store the model description. Out of all the models that describe the data, we want to have
the simplest model so that it lends itself to the shortest description. So
we again have a trade-oﬀ between how simple the model is and how well
it explains the data.
Bayesian model selection is used when we have some prior knowledge
about the appropriate class of approximating functions. This prior knowledge is deﬁned as a prior distribution over models, p(model). Given the
data and assuming a model, we can calculate p(model|data) using Bayes’
rule:
p(model|data) =

p(data|model)p(model)
p(data)

p(model|data) is the posterior probability of the model given our prior
subjective knowledge about models, namely, p(model), and the objective support provided by the data, namely, p(data|model). We can then
choose the model with the highest posterior probability, or take an average over all models weighted by their posterior probabilities. If we take
the log of equation 4.38, we get
(4.39)

log p(model|data) = log p(data|model) + log p(model) − c
which has the form of equation 4.37; the log likelihood of the data is the
training error and the log of the prior is the penalty term. For example,

83

4.8 Model Selection Procedures

5
4
3
2
1
0
−1
−2
−3
−4
−5
0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

Figure 4.8 In the same setting as that of ﬁgure 4.5, polynomials of order
1 to 4 are ﬁtted. The magnitude of coeﬃcients increase as the order of
the polynomial increases. They are as follows: 1 : [−0.0769, 0.0016]T , 2 :
[0.1682, −0.6657, 0.0080]T , 3 : [0.4238, −2.5778, 3.4675, −0.0002]T , 4 :
[−0.1093, 1.4356, −5.5007, 6.0454, −0.0019]T .

if we have a regression model and use the prior p(w) ∼ N (0, 1/λ), the
MAP corresponds to the minimum of
(4.40)

[r t − g(xt |w)]2 + λ

E=
t

wi2
i

That is, we look for wi that both decrease error and are also as close as
possible to 0, and the reason we want them close to 0 is then because the
ﬁtted polynomial will be smoother. As the polynomial order increases, to
get a better ﬁt to the data, the function will go up and down which will
mean coeﬃcients moving away from 0 (see ﬁgure 4.8); when we add this
penalty, we force a ﬂatter, smoother ﬁt. How much we penalize depends
on λ, which is the inverse of the variance of the prior, that is, how much
we expect the weights a priori to be away from 0. That is, having such a
prior is equivalent to forcing parameters to be close to 0. We are going to
talk about this in more detail in chapter 14.
That is, when the prior is chosen such that we give higher probabilities
to simpler models (following Occam’s razor), the Bayesian approach, regularization, SRM, and MDL are equivalent. Cross-validation is diﬀerent
from all other methods for model selection in that it makes no prior assumption about the model. If there is a large enough validation dataset,

84

4

Parametric Methods

it is the best approach. The other models become useful when the data
sample is small.

4.9

Notes
A good source on the basics of maximum likelihood and Bayesian estimation is Ross 1987. Many pattern recognition textbooks discuss classiﬁcation with parametric models (e.g., MacLachlan 1992; Devroye, Györﬁ, and
Lugosi 1996; Webb 1999; Duda, Hart, and Stork 2001). Tests for checking
univariate normality can be found in Rencher 1995.
Geman, Bienenstock, and Doursat (1992) discuss bias and variance decomposition for several learning models, which we discuss in later chapters. Bias/variance decomposition is for sum of squared loss and is for
regression; such a nice additive splitting of error into bias, variance and
noise is not possible for 0/1 loss, because in classiﬁcation, there is error
only if we accidentally move to the other side of the boundary. For a
two-class problem, if the correct posterior is 0.7 and if our estimate is
0.8, there is no error; we have error only if our estimate is less than 0.5.
Various researchers proposed diﬀerent deﬁnitions of bias and variance
for classiﬁcation; see Friedman 1997 for a review.

4.10

Exercises
1. Write the code that generates a Bernoulli sample with given parameter p, and
ˆ
the code that calculates p from the sample.
2. Write the log likelihood for a multinomial sample and show equation 4.6.
3. Write the code that generates a normal sample with given μ and σ , and the
code that calculates m and s from the sample. Do the same using the Bayes’
estimator assuming a prior distribution for μ.
2
2
4. Given two normal distributions p(x|C1 ) ∼ N (μ1 , σ1 ) and p(x|C2 ) ∼ N (μ2 , σ2 )
and P (C1 ) and P (C2 ), calculate the Bayes’ discriminant points analytically.

5. What is the likelihood ratio
p(x|C1 )
p(x|C2 )
in the case of Gaussian densities?
6. For a two-class problem, generate normal samples for two classes with diﬀerent variances, then use parametric classiﬁcation to estimate the discriminant
points. Compare these with the theoretical values.

4.11 References

85

7. Assume a linear model and then add 0-mean Gaussian noise to generate a
sample. Divide your sample into two as training and validation sets. Use
linear regression using the training half. Compute error on the validation set.
Do the same for polynomials of degrees 2 and 3 as well.
8. When the training set is small, the contribution of variance to error may be
more than that of bias and in such a case, we may prefer a simple model even
though we know that it is too simple for the task. Can you give an example?
9. Let us say, given the samples Xi = {xt , rit }, we deﬁne gi (x) = ri1 , namely, our
i
estimate for any x is the r value of the ﬁrst instance in the (unordered) dataset
Xi . What can you say about its bias and variance, as compared with gi (x) = 2
and gi (x) = t rit /N? What if the sample is ordered, so that gi (x) = mint rit ?
10. In equation 4.40, what is the eﬀect of changing λ on bias and variance?

4.11

References
Andrieu, C., N. de Freitas, A. Doucet, and M. I. Jordan. 2003. “An Introduction
to MCMC for Machine Learning.” Machine Learning 50: 5–43.
Breiman, L. 1998. “Bias-Variance, Regularization, Instability and Stabilization.”
In Neural Networks and Machine Learning, ed. C. M. Bishop, 27–56. Berlin:
Springer.
Devroye, L., L. Györﬁ, and G. Lugosi. 1996. A Probabilistic Theory of Pattern
Recognition. New York: Springer.
Duda, R. O., P. E. Hart, and D. G. Stork. 2001. Pattern Classiﬁcation, 2nd ed.
New York: Wiley.
Friedman, J. H. 1997. “On Bias, Variance, 0/1-Loss and the Curse of Dimensionality.” Data Mining and Knowledge Discovery 1: 55–77.
Geman, S., E. Bienenstock, and R. Doursat. 1992. “Neural Networks and the
Bias/Variance Dilemma.” Neural Computation 4: 1–58.
Grünwald, P. D. 2007. The Minimum Description Length Principle. Cambridge,
MA: MIT Press.
Hastie, T., R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical
Learning: Data Mining, Inference, and Prediction. New York: Springer.
McLachlan, G. J. 1992. Discriminant Analysis and Statistical Pattern Recognition.
New York: Wiley.
Rencher, A. C. 1995. Methods of Multivariate Analysis. New York: Wiley.
Rissanen, J. 1978. “Modeling by Shortest Data Description.” Automatica 14:
465–471.

86

4

Parametric Methods

Ross, S. M. 1987. Introduction to Probability and Statistics for Engineers and
Scientists. New York: Wiley.
Vapnik, V. 1995. The Nature of Statistical Learning Theory. New York: Springer.
Webb, A. 1999. Statistical Pattern Recognition. London: Arnold.

5

Multivariate Methods

In chapter 4, we discussed the parametric approach to classiﬁcation and regression. Now, we generalize this to the multivariate
case where we have multiple inputs and where the output, which
is class code or continuous output, is a function of these multiple inputs. These inputs may be discrete or numeric. We will see how such
functions can be learned from a labeled multivariate sample and
also how the complexity of the learner can be ﬁne-tuned to the data
at hand.

5.1

Multivariate Data
I n m an y a p p l i c a t i o n s , several measurements are made on each individual or event generating an observation vector. The sample may be
viewed as a data matrix
⎡
⎤
1
1
1
X1 X2 · · · X d
⎢ 2
2
2 ⎥
⎢ X1 X2 · · · X d ⎥
⎢
⎥
⎥
X=⎢ .
⎢ .
⎥
⎣ .
⎦
N
N
N
X1 X2 · · · X d

input
feature
attribute
observation
example
instance

where the d columns correspond to d variables denoting the result of
measurements made on an individual or event. These are also called inputs, features, or attributes. The N rows correspond to independent and
identically distributed observations, examples, or instances on N individuals or events.
For example, in deciding on a loan application, an observation vector
is the information associated with a customer and is composed of age,
marital status, yearly income, and so forth, and we have N such past

88

5 Multivariate Methods

customers. These measurements may be of diﬀerent scales, for example,
age in years and yearly income in monetary units. Some like age may be
numeric, and some like marital status may be discrete.
Typically these variables are correlated. If they are not, there is no need
for a multivariate analysis. Our aim may be simpliﬁcation, that is, summarizing this large body of data by means of relatively few parameters.
Or our aim may be exploratory, and we may be interested in generating
hypotheses about data. In some applications, we are interested in predicting the value of one variable from the values of other variables. If the
predicted variable is discrete, this is multivariate classiﬁcation, and if it
is numeric, this is a multivariate regression problem.

5.2
mean vector

(5.1)

Parameter Estimation
The mean vector μ is deﬁned such that each of its elements is the mean
of one column of X:
E[x] = μ = [μ1 , . . . , μd ]T
The variance of Xi is denoted as σi2 , and the covariance of two variables
Xi and Xj is deﬁned as

(5.2)

covariance matrix

σij ≡ Cov(Xi , Xj ) = E[(Xi − μi )(Xj − μj )] = E[Xi Xj ] − μi μj
with σij = σji , and when i = j, σii = σi2 . With d variables, there are d
variances and d(d − 1)/2 covariances, which are generally represented as
a d × d matrix, named the covariance matrix, denoted as Σ, whose (i, j)th
element is σij :
⎤
⎡
2
σ1 σ12 · · · σ1d
⎥
⎢
2
⎢ σ21 σ2 · · · σ2d ⎥
⎥
⎢
⎥
Σ=⎢ .
⎥
⎢ .
⎦
⎣ .
2
σd1 σd2 · · · σd
The diagonal terms are the variances, the oﬀ-diagonal terms are the
covariances, and the matrix is symmetric. In vector-matrix notation

(5.3)

Σ ≡ Cov(X) = E[(X − μ)(X − μ)T ] = E[XX T ] − μμT
If two variables are related in a linear way, then the covariance will be
positive or negative depending on whether the relationship has a positive

89

5.3 Estimation of Missing Values

correlation

(5.4)

sample mean

(5.5)
sample covariance

or negative slope. But the size of the relationship is diﬃcult to interpret
because it depends on the units in which the two variables are measured.
The correlation between variables Xi and Xj is a statistic normalized between −1 and +1, deﬁned as
Corr(Xi , Xj ) ≡ ρij =

If two variables are independent, then their covariance, and hence their
correlation, is 0. However, the converse is not true: the variables may be
dependent (in a nonlinear way), and their correlation may be 0.
Given a multivariate sample, estimates for these parameters can be
calculated: The maximum likelihood estimator for the mean is the sample
mean, m. Its ith dimension is the average of the ith column of X:
N
t
t=1 x

m=

N

si2

=

(5.7)

sij

=

(5.8)

with mi =

N
t
t=1 xi

N

, i = 1, . . . , d

The estimator of Σ is S, the sample covariance matrix, with entries

(5.6)

sample correlation

σij
σi σj

N
t
t=1 (xi

− mi )2
N
N
t
t
t=1 (xi − mi )(xj − mj )
N

These are biased estimates, but if in an application the estimates vary
signiﬁcantly depending on whether we divide by N or N − 1, we are in
serious trouble anyway.
The sample correlation coeﬃcients are
rij =

sij
si sj

and the sample correlation matrix R contains rij .

5.3

imputation

Estimation of Missing Values
Frequently, values of certain variables may be missing in observations.
The best strategy is to discard those observations all together, but generally we do not have large enough samples to be able to aﬀord this and
we do not want to lose data as the non-missing entries do contain information. We try to ﬁll in the missing entries by estimating them. This is
called imputation.

90

5 Multivariate Methods

In mean imputation, for a numeric variable, we substitute the mean (average) of the available data for that variable in the sample. For a discrete
variable, we ﬁll in with the most likely value, that is, the value most often
seen in the data.
In imputation by regression, we try to predict the value of a missing
variable from other variables whose values are known for that case. Depending on the type of the missing variable, we deﬁne a separate regression or classiﬁcation problem that we train by the data points for
which such values are known. If many diﬀerent variables are missing, we
take the means as the initial estimates and the procedure is iterated until
predicted values stabilize. If the variables are not highly correlated, the
regression approach is equivalent to mean imputation.
Depending on the context, however, sometimes the fact that a certain
attribute value is missing may be important. For example, in a credit
card application, if the applicant does not declare his or her telephone
number, that may be a critical piece of information. In such cases, this is
represented as a separate value to indicate that the value is missing and
is used as such.

5.4

Multivariate Normal Distribution
In the multivariate case where x is d-dimensional and normal distributed,
we have

(5.9)

p(x) =

1
1
exp − (x − μ)T Σ−1 (x − μ)
(2π )d/2 |Σ|1/2
2

and we write x ∼ Nd (μ, Σ) where μ is the mean vector and Σ is the
covariance matrix (see ﬁgure 5.1). Just as
(x − μ)2
= (x − μ)(σ 2 )−1 (x − μ)
σ2
Mahalanobis
distance

(5.10)

is the squared distance from x to μ in standard deviation units, normalizing for diﬀerent variances, in the multivariate case the Mahalanobis
distance is used:
(x − μ)T Σ−1 (x − μ)
(x −μ)T Σ−1 (x −μ) = c 2 is the d-dimensional hyperellipsoid centered at
μ, and its shape and orientation are deﬁned by Σ. Because of the use of
the inverse of Σ, if a variable has a larger variance than another, it receives

91

5.4 Multivariate Normal Distribution

0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0

x

x

2

1

Figure 5.1 Bivariate normal distribution.

less weight in the Mahalanobis distance. Similarly, two highly correlated
variables do not contribute as much as two less correlated variables. The
use of the inverse of the covariance matrix thus has the eﬀect of standardizing all variables to unit variance and eliminating correlations.
Let us consider the bivariate case where d = 2 for visualization purposes (see ﬁgure 5.2). When the variables are independent, the major
axes of the density are parallel to the input axes. The density becomes
an ellipse if the variances are diﬀerent. The density rotates depending on
the sign of the covariance (correlation). The mean vector is μT = [μ1 , μ2 ],
and the covariance matrix is usually expressed as
Σ=

2
σ1
ρσ1 σ2

ρσ1 σ2
2
σ2

The joint bivariate density can be expressed in the form (see exercise 1)
(5.11)

z-normalization

p(x1 , x2 ) =

1
2π σ1 σ2 1 − ρ 2

exp −

1
2
z 2 − 2ρz1 z2 + z2
2(1 − ρ 2 ) 1

where zi = (xi − μi )/σi , i = 1, 2, are standardized variables; this is called
z-normalization. Remember that
2
2
z1 + 2ρz1 z2 + z2 = constant

92

5 Multivariate Methods

Cov(x ,x )=0, Var(x )=Var(x )
1

2

Cov(x ,x )=0, Var(x )>Var(x )
1 2

1

2

x2

1 2

x1
Cov(x ,x )>0
1 2

Cov(x ,x )<0
1 2

Figure 5.2 Isoprobability contour plot of the bivariate normal distribution. Its
center is given by the mean, and its shape and orientation depend on the covariance matrix.

for |ρ| < 1, is the equation of an ellipse. When ρ > 0, the major axis of
the ellipse has a positive slope and if ρ < 0, the major axis has a negative
slope.
In the expanded Mahalanobis distance of equation 5.11, each variable
is normalized to have unit variance, and there is the cross-term that corrects for the correlation between the two variables.
The density depends on ﬁve parameters: the two means, the two variances, and the correlation. Σ is nonsingular, and hence positive deﬁnite,
provided that variances are nonzero and |ρ| < 1. If ρ is +1 or −1, the
two variables are linearly related, the observations are eﬀectively onedimensional, and one of the two variables can be disposed of. If ρ = 0,
then the two variables are independent, the cross-term disappears, and
we get a product of two univariate densities.
In the multivariate case, a small value of |Σ| indicates samples are close
to μ, just as in the univariate case where a small value of σ 2 indicates

5.4 Multivariate Normal Distribution

(5.12)

93

samples are close to μ. Small |Σ| may also indicate that there is high
correlation between variables. Σ is a symmetric positive deﬁnite matrix;
this is the multivariate way of saying that Var(X) > 0. If not so, Σ is
singular and its determinant is 0. This is either due to linear dependence
between the dimensions or because one of the dimensions has variance
0. In such a case, dimensionality should be reduced to a get a positive
deﬁnite matrix; methods for this are discussed in chapter 6.
If x ∼ Nd (μ, Σ), then each dimension of x is univariate normal. (The
converse is not true: Each Xi may be univariate normal and X may not
be multivariate normal.) Actually any k < d subset of the variables is
k-variate normal.
A special, naive case is where the components of x are independent
and Cov(Xi , Xj ) = 0, for i = j, and Var(Xi ) = σi2 , ∀i. Then the covariance
matrix is diagonal and the joint density is the product of the individual
univariate densities:
⎤
⎡
d
d
xi − μi 2 ⎦
1
⎣− 1
p(x) =
pi (xi ) =
exp
d
2 i=1
σi
(2π )d/2 i=1 σi
i=1
Now let us see another property we make use of in later chapters. Let
us say x ∼ Nd (μ, Σ) and w ∈ d , then
w T x = w1 x1 + w2 x2 + · · · + wd xd ∼ N (w T μ, w T Σw)
given that
=

w T E[x] = w T μ

=

E[(w T x − w T μ)2 ] = E[(w T x − w T μ)(w T x − w T μ)]

=
(5.14)

E[w T x]
Var(w T x)

(5.13)

E[w T (x − μ)(x − μ)T w] = w T E[(x − μ)(x − μ)T ]w

=

w T Σw

That is, the projection of a d-dimensional normal on the vector w is
univariate normal. In the general case, if W is a d × k matrix with rank
k < d, then the k-dimensional WT x is k-variate normal:
(5.15)

WT x ∼ Nk (WT μ, WT ΣW)
That is, if we project a d-dimensional normal distribution to a space
that is k-dimensional, then it projects to a k-dimensional normal.

94

5 Multivariate Methods

5.5

Multivariate Classiﬁcation
When x ∈ d , if the class-conditional densities, p(x|Ci ), are taken as
normal density, Nd (μi , Σi ), we have

(5.16)

p(x|Ci ) =

1
(2π )d/2 |Σi |1/2

1
exp − (x − μi )T Σ−1 (x − μi )
i
2

The main reason for this is its analytical simplicity (Duda, Hart, and Stork
2001). Besides, the normal density is a model for many naturally occurring phenomena in that examples of most classes can be seen as mildly
changed versions of a single prototype, μi , and the covariance matrix,
Σi , denotes the amount of noise in each variable and the correlations of
these noise sources. While real data may not often be exactly multivariate normal, it is a useful approximation. In addition to its mathematical
tractability, the model is robust to departures from normality as is shown
in many works (e.g., McLachlan 1992). However, one clear requirement is
that the sample of a class should form a single group; if there are multiple
groups, one should use a mixture model (chapter 7).
Let us say we want to predict the type of a car that a customer would be
interested in. Diﬀerent cars are the classes and x are observable data of
customers, for example, age and income. μi is the vector of mean age and
income of customers who buy car type i and Σi is their covariance matrix:
2
2
σi1 and σi2 are the age and income variances, and σi12 is the covariance
of age and income in the group of customers who buy car type i.
When we deﬁne the discriminant function as
gi (x) = log p(x|Ci ) + log P (Ci )
and assuming p(x|Ci ) ∼ Nd (μi , Σi ), we have
(5.17)

gi (x) = −

1
d
1
log 2π − log |Σi | − (x − μi )T Σ−1 (x − μi ) + log P (Ci )
i
2
2
2

Given a training sample for K ≥ 2 classes, X = {x t , r t }, where rit = 1
if x t ∈ Ci and 0 otherwise, estimates for the means and covariances are
found using maximum likelihood separately for each class:
(5.18)

ˆ
P (Ci )

=

mi

=

Si

=

t
t ri

N
t t
t ri x
t
t ri
t
t
t ri (x

− m i )(x t − m i )T
t
t ri

5.5 Multivariate Classiﬁcation

95

These are then plugged into the discriminant function to get the estimates for the discriminants. Ignoring the ﬁrst constant term, we have
(5.19)

gi (x) = −

1
1
ˆ
log |Si | − (x − m i )T S−1 (x − m i ) + log P (Ci )
i
2
2

Expanding this, we get
gi (x) = −
quadratic
discriminant

(5.20)

1
1 T −1
ˆ
log |Si | −
x Si x − 2x T S−1 m i + m T S−1 m i + log P (Ci )
i
i i
2
2

which deﬁnes a quadratic discriminant (see ﬁgure 5.3) that can also be
written as
gi (x) = x T Wi x + w T x + wi0
i
where
Wi

=

wi

=

wi0

=

1
− S−1
2 i
S−1 m i
i
1
1
ˆ
− m T S−1 m i − log |Si | + log P (Ci )
i i
2
2

The number of parameters to be estimated are K · d for the means and
K · d(d + 1)/2 for the covariance matrices. When d is large and samples
are small, Si may be singular and inverses may not exist. Or, |Si | may be
nonzero but too small, in which case it will be unstable; small changes in
Si will cause large changes in S−1 . For the estimates to be reliable on small
i
samples, one may want to decrease dimensionality, d, by redesigning the
feature extractor and select a subset of the features or somehow combine
existing features. We discuss such methods in chapter 6.
Another possibility is to pool the data and estimate a common covariance matrix for all classes:
(5.21)

ˆ
P (Ci )Si

S=
i

In this case of equal covariance matrices, equation 5.19 reduces to
(5.22)

1
ˆ
gi (x) = − (x − m i )T S−1 (x − m i ) + log P (Ci )
2
The number of parameters is K · d for the means and d(d + 1)/2 for
the shared covariance matrix. If the priors are equal, the optimal decision
rule is to assign input to the class whose mean’s Mahalanobis distance to
the input is the smallest. As before, unequal priors shift the boundary

96

5 Multivariate Methods

1

p( x|C )

0.1

0.05

0

x

x

x

x

2

1

0.5

1

p(C | x)

1

0

2

1

Figure 5.3 Classes have diﬀerent covariance matrices. Likelihood densities and
the posterior probability for one of the classes (top). Class distributions are
indicated by isoprobability contours and the discriminant is drawn (bottom).

97

5.5 Multivariate Classiﬁcation

Figure 5.4 Covariances may be arbitary but shared by both classes.

linear discriminant

(5.23)

toward the less likely class. Note that in this case, the quadratic term
x T S−1 x cancels since it is common in all discriminants, and the decision
boundaries are linear, leading to a linear discriminant (ﬁgure 5.4) that can
be written as
gi (x) = w T x + wi0
i
where
wi
wi0

naive Bayes’
classifier

=
=

S−1 m i
1
ˆ
− m T S−1 m i + log P (Ci )
2 i

Decision regions of such a linear classiﬁer are convex; namely, when
two points are chosen arbitrarily in one decision region and are connected
by a straight line, all the points on the line will lie in the region.
Further simplication may be possible by assuming all oﬀ-diagonals of
the covariance matrix to be 0, thus assuming independent variables. This
is the naive Bayes’ classiﬁer where p(xj |Ci ) are univariate Gaussian. S
and its inverse are diagonal, and we get
d

(5.24)

1
gi (x) = −
2 j=1

xt − mij
j
sj

2

ˆ
+ log P (Ci )

The term (xt −mij )/sj has the eﬀect of normalization and measures the
j
distance in terms of standard deviation units. Geometrically speaking,

98

5 Multivariate Methods

Figure 5.5 All classes have equal, diagonal covariance matrices, but variances
are not equal.

Euclidean distance

(5.25)

nearest mean
classifier
template matching

classes are hyperellipsoidal and, because the covariances are zero, are
axis-aligned (see ﬁgure 5.5). The number of parameters is K · d for the
means and d for the variances. Thus the complexity of S is reduced from
O(d 2 ) to O(d).
Simplifying even further, if we assume all variances to be equal, the
Mahalanobis distance reduces to Euclidean distance. Geometrically, the
distribution is shaped spherically, centered around the mean vector m i
(see ﬁgure 5.6). Then |S| = s 2d and S−1 = (1/s 2 )I. The number of parameters in this case is K · d for the means and 1 for s 2 .
gi (x) = −

2

ˆ
+ log P (Ci ) = −

1
2s 2

d

ˆ
(xt − mij )2 + log P (Ci )
j
j=1

If the priors are equal, we have gi (x) = − x − m i 2 . This is named the
nearest mean classiﬁer because it assigns the input to the class of the
nearest mean. If each mean is thought of as the ideal prototype or template for the class, this is a template matching procedure. This can be
expanded as
gi (x)

(5.26)

x − mi
2s 2

2

= −(x − m i )T (x − m i )

=

− x − mi

=

−(x T x − 2m T x + m T m i )
i
i

5.6 Tuning Complexity

99

Figure 5.6 All classes have equal, diagonal covariance matrices of equal variances on both dimensions.

The ﬁrst term, x T x, is shared in all gi (x) and can be dropped, and we
can write the discriminant function as
(5.27)

gi (x) = w T x + wi0
i
where w i = m i and wi0 = −(1/2) m i 2 . If all m i have similar norms,
then this term can also be ignored and we can use

(5.28)

gi (x) = m T x
i
When the norms of m i are comparable, dot product can also be used
as the similarity measure instead of the (negative) Euclidean distance.
We can actually think of ﬁnding the best discriminant function as the
task of ﬁnding the best distance function. This can be seen as another
approach to classiﬁcation: Instead of learning the discriminant functions,
gi (x), we want to learn the suitable distance function D(x 1 , x 2 ), such that
for any x 1 , x 2 , x 3 , where x 1 and x 2 belong to the same class, and x 1 and
x 3 belong to two diﬀerent classes, we would like to have
D(x 1 , x 2 ) < D(x 1 , x 3 )

5.6

Tuning Complexity
In table 5.1, we see how the number of parameters of the covariance
matrix may be reduced, trading oﬀ the comfort of a simple model with

100

5 Multivariate Methods

Table 5.1 Reducing variance through simplifying assumptions.

Assumption
Shared, Hyperspheric
Shared, Axis-aligned
Shared, Hyperellipsoidal
Diﬀerent, Hyperellipsoidal

regularized
discriminant
analysis

Covariance matrix
Si = S = s 2 I
Si = S, with sij = 0
Si = S
Si

No. of parameters
1
d
d(d + 1)/2
K · (d(d + 1)/2)

generality. This is another example of bias/variance dilemma. When
we make simplifying assumptions about the covariance matrices and decrease the number of parameters to be estimated, we risk introducing
bias (see ﬁgure 5.7). On the other hand, if no such assumption is made
and the matrices are arbitrary, the quadratic discriminant may have large
variance on small datasets. The ideal case depends on the complexity of
the problem represented by the data at hand and the amount of data we
have. When we have a small dataset, even if the covariance matrices are
diﬀerent, it may be better to assume a shared covariance matrix; a single
covariance matrix has fewer parameters and it can be estimated using
more data, that is, instances of all classes. This corresponds to using
linear discriminants, which is very frequently used in classiﬁcation and
which we discuss in more detail in chapter 10.
Note that when we use Euclidean distance to measure similarity, we
are assuming that all variables have the same variance and that they are
independent. In many cases, this does not hold; for example, age and
yearly income are in diﬀerent units, and are dependent in many contexts.
In such a case, the inputs may be separately z-normalized in a preprocessing stage (to have zero mean and unit variance), and then Euclidean
distance can be used. On the other hand, sometimes even if the variables
are dependent, it may be better to assume that they are independent
and to use the naive Bayes’ classiﬁer, if we do not have enough data to
calculate the dependency accurately.
Friedman (1989) proposed a method that combines all these as special cases, named regularized discriminant analysis (RDA). We remember
that regularization corresponds to approaches where one starts with high
variance and constrains toward lower variance, at the risk of increasing
bias. In the case of parametric classiﬁcation with Gaussian densities, the

101

5.6 Tuning Complexity

Population likelihoods and posteriors
4

y

3
2
1
0

0

2
4
x
Arbitrary covar.

Shared covar.
3

2

2

y

4

3

y

4

1
0

1
0

2
x
Diag. covar.

0

4

4

0

2
x

4

2

y

3

2

2
x
Equal var.

4

3

y

4

0

1
0

1
0

2
x

4

0

Figure 5.7 Diﬀerent cases of the covariance matrices ﬁtted to the same data
lead to diﬀerent boundaries.

covariance matrices can be written as a weighted average of the three
special cases:
(5.29)

Si = ασ 2 I + βS + (1 − α − β)Si
When α = β = 0, this leads to a quadratic classiﬁer. When α = 0 and
β = 1, the covariance matrices are shared, and we get linear classiﬁers.
When α = 1 and β = 0, the covariance matrices are diagonal with σ 2 on
the diagonals, and we get the nearest mean classiﬁer. In between these

102

5 Multivariate Methods

extremes, we get a whole variety of classiﬁers where α, β are optimized
by cross-validation.
Another approach to regularization, when the dataset is small, is one
that uses a Bayesian approach by deﬁning priors on μi and Si or that uses
cross-validation to choose the best of the four cases given in table 5.1.

5.7

Discrete Features
In some applications, we have discrete attributes taking one of n diﬀerent
values. For example, an attribute may be color ∈ {red, blue, green, black},
or another may be pixel ∈ {on, oﬀ}. Let us say xj are binary (Bernoulli)
where
pij ≡ p(xj = 1|Ci )
If xj are independent binary variables, we have
d

xj

pij (1 − pij )(1−xj )

p(x|Ci ) =
j=1

This is another example of the naive Bayes’ classiﬁer where p(xj |Ci )
are Bernoulli. The discriminant function is
gi (x)

=

log p(x|Ci ) + log P (Ci )

=

(5.30)

xj log pij + (1 − xj ) log(1 − pij ) + log P (Ci )
j

which is linear. The estimator for pij is
(5.31)
document
categorization
bag of words

ˆ
pij =

t

xt rit
j
t
t ri

This approach is used in document categorization, an example of which
is classifying news reports into various categories, such as, politics, sports,
fashion, and so forth. In the bag of words representation, we choose a
priori d words that we believe give information regarding the class (Manning and Schütze 1999). For example, in news classiﬁcation, words such
as “missile,” “athlete,” and “couture” are useful, rather than ambiguous
words such as “model,” or even “runway.” In this representation, each
text is a d-dimensional binary vector where xj is 1 if word j occurs in
the document and is 0 otherwise. Note that this representation loses all
ordering information of words, and hence the name bag of words.

5.8 Multivariate Regression

spam filtering

103

ˆ
After training, pij estimates the probability that word j occurs in document type i. Words whose probabilities are similar for diﬀerent classes
do not convey much information; for them to be useful, we would want
the probability to be high for one class (or few) and low for all others; we
are going to talk about this type of feature selection in chapter 6. Another
example application of document categorization is spam ﬁltering where
there are two classes of emails as spam and legitimate. In bioinformatics,
too, inputs are generally sequences of discrete items, whether base-pairs
or amino acids.
In the general case, instead of binary features, let us say we have the
multinomial xj chosen from the set {v1 , v2 , . . . , vnj }. We deﬁne new 0/1
dummy variables as
t
zjk =

1
0

if xt = vk
j
otherwise

Let pijk denote the probability that xj belonging to Ci takes value vk :
pijk ≡ p(zjk = 1|Ci ) = p(xj = vk |Ci )
If the attributes are independent, we have
d

(5.32)

nj

p(x|Ci ) =

zj k

pijk
j=1 k=1

The discriminant function is then
(5.33)

gi (x) =

zjk log pijk + log P (Ci )
j

k

The maximum likelihood estimator for pijk is
(5.34)

ˆ
pijk =

t

t
zjk rit
t
t ri

which can be plugged into equation 5.33 to give us the discriminant.

5.8
multivariate linear
regression

Multivariate Regression
In multivariate linear regression, the numeric output r is assumed to be
written as a linear function, that is, a weighted sum, of several input
variables, x1 , . . . , xd , and noise. Actually in statistical literature, this is

104

5 Multivariate Methods

called multiple regression; statisticians use the term multivariate when
there are multiple outputs. The multivariate linear model is
(5.35)

= w0 + w1 xt + w2 xt + · · · + wd xt +
1
2
d

r t = g(x t |w0 , w1 , . . . , wd ) +

As in the univariate case, we assume to be normal with mean 0 and
constant variance, and maximizing the likelihood is equivalent to minimizing the sum of squared errors:
(5.36)

E(w0 , w1 , . . . , wd |X) =

1
2

(r t − w0 − w1 xt − w2 xt − · · · − wd xt )2
1
2
d
t

Taking the derivative with respect to the parameters, wj , j = 0, . . . , d,
we get the normal equations:
rt

(5.37)

=

xt + w2
1

Nw0 + w1

t

t

xt r t
1

=

xt + w1
1

w0

t

t

xt r t
2

=

xt xt + · · · + wd
1 2
t

xt xt
1 2

+ w1

t

xt
d
t

(xt )2 + w2
1
t

xt
2

w0

t

xt + · · · + wd
2
t

t

(xt )2
2

+ w2

t

xt xt
1 d
xt xt
2 d

+ · · · + wd

t

t

.
.
.
xt r t
d

=

xt + w1
d

w0

t

t

xt xt + w2
d 1
t

xt xt + · · · + wd
d 2
t

Let us deﬁne the following vectors and matrix:
⎤
⎤
⎡
⎡
⎡
1 x1 x1 · · · x1
w0
r1
1
2
d
⎥
⎢
⎢
⎢ 2
2
2
2 ⎥
⎢ 1 x1 x2 · · · xd ⎥
⎢ w1 ⎥
⎢ r
⎥
⎥
⎢
⎢
⎢
⎥,w = ⎢ . ⎥,r = ⎢ .
X=⎢ .
⎥
⎢ .
⎢ . ⎥
⎢ .
⎦
⎣ .
⎣ . ⎦
⎣ .
N
N
N
1 x1 x2 · · · xd
wd ,
rN

(xt )2
d
t

⎤
⎥
⎥
⎥
⎥
⎥
⎦

Then the normal equations can be written as
(5.38)

XT Xw = XT r
and we can solve for the parameters as

(5.39)

multivariate
polynomial
regression

w = (XT X)−1 XT r
This method is the same as we used for polynomial regression using
one input. The two problems are the same if we deﬁne the variables as
x1 = x, x2 = x2 , . . . , xk = xk . This also gives us a hint as to how we can do
multivariate polynomial regression if necessary (exercise 7), but unless d

5.9 Notes

105

is small, in multivariate regression, we rarely use polynomials of an order
higher than linear.
Actually using higher-order terms of inputs as additional inputs is
only one possibility; we can deﬁne any nonlinear function of the original inputs using basis functions. For example, we can deﬁne new inputs
x2 = sin(x), x3 = exp(x2 ) if we believe that such a transformation is
useful. Then, using a linear model in this new augmented space will correspond to a nonlinear model in the original space. The same calculation
will still be valid; we need only replace X with the data matrix after the basis functions are applied. As we will see later under various guises (e.g.,
multilayer perceptrons, support vector machines, Gaussian processes),
this type of generalizing the linear model is frequently used.
One advantage of linear models is that after the regression, looking at
the wj , j = 1, . . . , d, values, we can extract knowledge: First, by looking at
the signs of wj , we can see whether xj have a positive or negative eﬀect
on the output. Second, if all xj are in the same range, by looking at the
absolute values of wj , we can get an idea about how important a feature
is, rank the features in terms of their importances, and even remove the
features whose wj are close to 0.
When there are multiple outputs, this can equivalently be deﬁned as a
set of independent single-output regression problems.

5.9

Notes
A good review text on linear algebra is Strang 1988. Harville 1997 is another excellent book that looks at matrix algebra from a statistical point
of view.
One inconvenience with multivariate data is that when the number of
dimensions is large, one cannot do a visual analysis. There are methods
proposed in the statistical literature for displaying multivariate data; a
review is given in Rencher 1995. One possibility is to plot variables two
by two as bivariate scatter plots: If the data is multivariate normal, then
the plot of any two variables should be roughly linear; this can be used
as a visual test of multivariate normality. Another possibility that we
discuss in chapter 6 is to project them to one or two dimensions and
display there.
Most work on pattern recognition is done assuming multivariate normal densities. Sometimes such a discriminant is even called the Bayes’

106

5 Multivariate Methods

optimal classiﬁer, but this is generally wrong; it is only optimal if the
densities are indeed multivariate normal and if we have enough data to
calculate the correct parameters from the data. Rencher 1995 discusses
tests for assessing multivariate normality as well as tests for checking for
equal covariance matrices. McLachlan 1992 discusses classiﬁcation with
multivariate normals and compares linear and quadratic discriminants.
One obvious restriction of multivariate normals is that it does not allow for data where some features are discrete. A variable with n possible values can be converted into n dummy 0/1 variables, but this increases dimensionality. One can do a dimensionality reduction in this
n-dimensional space by a method explained in chapter 6 and thereby not
increase dimensionality. Parametric classiﬁcation for such cases of mixed
features is discussed in detail in McLachlan 1992.

5.10

Exercises
1. Show equation 5.11.
2. Generate a sample from a multivariate normal density N (μ, Σ), calculate m
and S, and compare them with μ and Σ. Check how your estimates change as
the sample size changes.
3. Generate samples from two multivariate normal densities N (μi , Σi ), i = 1, 2,
and calculate the Bayes’ optimal discriminant for the four cases in table 5.1.
4. For a two-class problem, for the four cases of Gaussian densities in table 5.1,
derive
log

P (C1 |x)
P (C2 |x)

5. Another possibility using Gaussian densities is to have them all diagonal but
allow them to be diﬀerent. Derive the discriminant for this case.
6. Let us say in two dimensions, we have two classes with exactly the same
mean. What type of boundaries can be deﬁned?
7. Let us say we have two variables x1 and x2 and we want to make a quadratic
ﬁt using them, namely,
f (x1 , x2 ) = w0 + w1 x1 + w2 x2 + w3 x1 x2 + w4 (x1 )2 + w5 (x2 )2
How can we ﬁnd wi , i = 0, . . . , 5, given a sample of X = {xt , xt , r t }?
1
2
8. In regression we saw that ﬁtting a quadratic is equivalent to ﬁtting a linear
model with an extra input corresponding to the square of the input. Can we
also do this in classiﬁcation?

5.11 References

107

9. In document clustering, ambiguity of words can be decreased by taking the
context into account, for example, by considering pairs of words, as in “cocktail party” vs. “party elections.” Discuss how this can be implemented.

5.11

References
Duda, R. O., P. E. Hart, and D. G. Stork. 2001. Pattern Classiﬁcation, 2nd ed.
New York: Wiley.
Friedman, J. H. 1989. “Regularized Discriminant Analysis.” Journal of American
Statistical Association 84: 165–175.
Harville, D. A. 1997. Matrix Algebra from a Statistician’s Perspective. New York:
Springer.
Manning, C. D., and H. Schütze. 1999. Foundations of Statistical Natural Language Processing. Cambridge, MA: MIT Press.
McLachlan, G. J. 1992. Discriminant Analysis and Statistical Pattern Recognition.
New York: Wiley.
Rencher, A. C. 1995. Methods of Multivariate Analysis. New York: Wiley.
Strang, G. 1988. Linear Algebra and its Applications, 3rd ed. New York: Harcourt Brace Jovanovich.

6

Dimensionality Reduction

The complexity of any classiﬁer or regressor depends on the number
of inputs. This determines both the time and space complexity and
the necessary number of training examples to train such a classiﬁer or regressor. In this chapter, we discuss feature selection methods that choose a subset of important features pruning the rest and
feature extraction methods that form fewer, new features from the
original inputs.

6.1

Introduction
I n a n a p p l i c a t i o n , whether it is classiﬁcation or regression, observation data that we believe contain information are taken as inputs and fed
to the system for decision making. Ideally, we should not need feature
selection or extraction as a separate process; the classiﬁer (or regressor)
should be able to use whichever features are necessary, discarding the
irrelevant. However, there are several reasons why we are interested in
reducing dimensionality as a separate preprocessing step:
In most learning algorithms, the complexity depends on the number of
input dimensions, d, as well as on the size of the data sample, N, and
for reduced memory and computation, we are interested in reducing
the dimensionality of the problem. Decreasing d also decreases the
complexity of the inference algorithm during testing.
When an input is decided to be unnecessary, we save the cost of extracting it.
Simpler models are more robust on small datasets. Simpler models

110

6 Dimensionality Reduction

have less variance, that is, they vary less depending on the particulars
of a sample, including noise, outliers, and so forth.
When data can be explained with fewer features, we get a better idea
about the process that underlies the data and this allows knowledge
extraction.
When data can be represented in a few dimensions without loss of
information, it can be plotted and analyzed visually for structure and
outliers.

feature selection

feature extraction

6.2
subset selection

forward selection

There are two main methods for reducing dimensionality: feature selection and feature extraction. In feature selection, we are interested in
ﬁnding k of the d dimensions that give us the most information and we
discard the other (d − k) dimensions. We are going to discuss subset
selection as a feature selection method.
In feature extraction, we are interested in ﬁnding a new set of k dimensions that are combinations of the original d dimensions. These
methods may be supervised or unsupervised depending on whether or
not they use the output information. The best known and most widely
used feature extraction methods are Principal Components Analysis (PCA)
and Linear Discriminant Analysis (LDA), which are both linear projection
methods, unsupervised and supervised respectively. PCA bears much
similarity to two other unsupervised linear projection methods, which we
also discuss—namely, Factor Analysis (FA) and Multidimensional Scaling
(MDS). As examples of nonlinear dimensionality reduction, we are going
to see Isometric feature mapping (Isomap) and Locally Linear Embedding
(LLE).

Subset Selection
In subset selection, we are interested in ﬁnding the best subset of the
set of features. The best subset contains the least number of dimensions
that most contribute to accuracy. We discard the remaining, unimportant
dimensions. Using a suitable error function, this can be used in both
regression and classiﬁcation problems. There are 2d possible subsets
of d variables, but we cannot test for all of them unless d is small and
we employ heuristics to get a reasonable (but not optimal) solution in
reasonable (polynomial) time.
There are two approaches: In forward selection, we start with no vari-

6.2 Subset Selection

backward selection

(6.1)

111

ables and add them one by one, at each step adding the one that decreases the error the most, until any further addition does not decrease
the error (or decreases it only sightly). In backward selection, we start
with all variables and remove them one by one, at each step removing
the one that decreases the error the most (or increases it only slightly),
until any further removal increases the error signiﬁcantly. In either case,
checking the error should be done on a validation set distinct from the
training set because we want to test the generalization accuracy. With
more features, generally we have lower training error, but not necessarily
lower validation error.
Let us denote by F , a feature set of input dimensions, xi , i = 1, . . . , d.
E(F ) denotes the error incurred on the validation sample when only the
inputs in F are used. Depending on the application, the error is either the
mean square error or misclassiﬁcation error.
In sequential forward selection, we start with no features: F = ∅. At
each step, for all possible xi , we train our model on the training set and
calculate E(F ∪ xi ) on the validation set. Then, we choose that input xj
that causes the least error
j = arg min E(F ∪ xi )
i

and we
(6.2)

add xj to F if E(F ∪ xj ) < E(F )
We stop if adding any feature does not decrease E. We may even decide to stop earlier if the decrease in error is too small, where there is a
user-deﬁned threshold that depends on the application constraints, trading oﬀ the importance of error and complexity. Adding another feature
introduces the cost of observing the feature, as well as making the classiﬁer/regressor more complex.
This process may be costly because to decrease the dimensions from d
to k, we need to train and test the system d +(d −1)+(d −2)+· · ·+(d −k)
times, which is O(d 2 ). This is a local search procedure and does not
guarantee ﬁnding the optimal subset, namely, the minimal subset causing
the smallest error. For example, xi and xj by themselves may not be good
but together may decrease the error a lot, but because this algorithm
is greedy and adds attributes one by one, it may not be able to detect
this. It is possible to generalize and add multiple features at a time,
instead of a single one, at the expense of more computation. We can

112

6 Dimensionality Reduction

floating search

(6.3)

also backtrack and check which previously added feature can be removed
after a current addition, thereby increasing the search space, but this
increases complexity. In ﬂoating search methods (Pudil, Novovi˘ová, and
c
Kittler 1994), the number of added features and removed features can
also change at each step.
In sequential backward selection, we start with F containing all features
and do a similar process except that we remove one attribute from F as
opposed to adding to it, and we remove the one that causes the least
error
j = arg min E(F − xi )
i

and we
(6.4)

remove xj from F if E(F − xj ) < E(F )
We stop if removing a feature does not decrease the error. To decrease
complexity, we may decide to remove a feature if its removal causes only
a slight increase in error.
All the variants possible for forward search are also possible for backward search. The complexity of backward search has the same order of
complexity as forward search, except that training a system with more
features is more costly than training a system with fewer features, and
forward search may be preferable especially if we expect many useless
features.
Subset selection is supervised in that outputs are used by the regressor
or classiﬁer to calculate the error, but it can be used with any regression
or classiﬁcation method. In the particular case of multivariate normals
for classiﬁcation, remember that if the original d-dimensional class densities are multivariate normal, then any subset is also multivariate normal
and parametric classiﬁcation can still be used with the advantage of k × k
covariance matrices instead of d × d.
In an application like face recognition, feature selection is not a good
method for dimensionality reduction because individual pixels by themselves do not carry much discriminative information; it is the combination of values of several pixels together that carry information about the
face identity. This is done by feature extraction methods that we discuss
next.

6.3 Principal Components Analysis

6.3

113

Principal Components Analysis
In projection methods, we are interested in ﬁnding a mapping from the
inputs in the original d-dimensional space to a new (k < d)-dimensional
space, with minimum loss of information. The projection of x on the
direction of w is

(6.5)
principal
components
analysis

z = wT x
Principal components analysis (PCA) is an unsupervised method in that
it does not use the output information; the criterion to be maximized is
the variance. The principal component is w 1 such that the sample, after
projection on to w 1 , is most spread out so that the diﬀerence between
the sample points becomes most apparent. For a unique solution and to
make the direction the important factor, we require w 1 = 1. We know
from equation 5.14 that if z1 = w T x with Cov(x) = Σ, then
1
Var(z1 ) = w T Σw 1
1
We seek w 1 such that Var(z1 ) is maximized subject to the constraint
that w T w 1 = 1. Writing this as a Lagrange problem, we have
1

(6.6)

max w T Σw 1 − α(w T w 1 − 1)
1
1
w1
Taking the derivative with respect to w 1 and setting it equal to 0, we
have
2Σw 1 − 2αw 1 = 0, and therefore Σw 1 = αw 1
which holds if w 1 is an eigenvector of Σ and α the corresponding eigenvalue. Because we want to maximize
w T Σw 1 = αw T w 1 = α
1
1
we choose the eigenvector with the largest eigenvalue for the variance
to be maximum. Therefore the principal component is the eigenvector
of the covariance matrix of the input sample with the largest eigenvalue,
λ1 = α.
The second principal component, w 2 , should also maximize variance,
be of unit length, and be orthogonal to w 1 . This latter requirement is so
that after projection z2 = w T x is uncorrelated with z1 . For the second
2
principal component, we have

(6.7)

max w T Σw 2 − α(w T w 2 − 1) − β(w T w 1 − 0)
2
2
2
w2

114

6 Dimensionality Reduction

Taking the derivative with respect to w 2 and setting it equal to 0, we
have
(6.8)

2Σw 2 − 2αw 2 − βw 1 = 0
Premultiply by w T and we get
1
2w T Σw 2 − 2αw T w 2 − βw T w 1 = 0
1
1
1
Note that w T w 2 = 0. w T Σw 2 is a scalar, equal to its transpose w T Σw 1
1
1
2
where, because w 1 is the leading eigenvector of Σ, Σw 1 = λ1 w 1 . Therefore
w T Σw 2 = w T Σw 1 = λ1 w T w 1 = 0
1
2
2
Then β = 0 and equation 6.8 reduces to
Σw 2 = αw 2
which implies that w 2 should be the eigenvector of Σ with the second
largest eigenvalue, λ2 = α. Similarly, we can show that the other dimensions are given by the eigenvectors with decreasing eigenvalues.
Because Σ is symmetric, for two diﬀerent eigenvalues, the eigenvectors
are orthogonal. If Σ is positive deﬁnite (x T Σx > 0, for all nonnull x), then
all its eigenvalues are positive. If Σ is singular, then its rank, the eﬀective
dimensionality, is k with k < d and λi , i = k + 1, . . . , d are 0 (λi are sorted
in descending order). The k eigenvectors with nonzero eigenvalues are
the dimensions of the reduced space. The ﬁrst eigenvector (the one with
the largest eigenvalue), w 1 , namely, the principal component, explains
the largest part of the variance; the second explains the second largest;
and so on.
We deﬁne

(6.9)

z = WT (x − m)
where the k columns of W are the k leading eigenvectors of S, the estimator to Σ. We subtract the sample mean m from x before projection
to center the data on the origin. After this linear transformation, we get
to a k-dimensional space whose dimensions are the eigenvectors, and the
variances over these new dimensions are equal to the eigenvalues (see
ﬁgure 6.1). To normalize variances, we can divide by the square roots of
the eigenvalues.

115

6.3 Principal Components Analysis

z2

x2

z1
z2

x1

z1

Figure 6.1 Principal components analysis centers the sample and then rotates
the axes to line up with the directions of highest variance. If the variance on z2
is too small, it can be ignored and we have dimensionality reduction from two to
one.

Let us see another derivation: We want to ﬁnd a matrix W such that
when we have z = WT x (assume without loss of generality that x are already centered), we will get Cov(z) = D where D is any diagonal matrix;
that is, we would like to get uncorrelated zi .
If we form a (d × d) matrix C whose ith column is the normalized
eigenvector c i of S, then CT C = I and

(Sc 1 , Sc 2 , . . . , Sc d )C T
(λ1 c 1 , λ2 c 2 , . . . , λd c d )CT

=

(6.11)

S(c 1 , c 2 , . . . , c d )CT

=

spectral
decomposition

SCCT

=

(6.10)

=
=

λ1 c 1 c T + · · · + λd c d c T
1
d

=

S

CDCT

where D is a diagonal matrix whose diagonal elements are the eigenvalues, λ1 , . . . , λd . This is called the spectral decomposition of S. Since C is
orthogonal and CCT = CT C = I, we can multiply on the left by CT and on
the right by C to obtain
CT SC = D
We know that if z = WT x, then Cov(z) = WT SW, which we would like
to be equal to a diagonal matrix. Then from equation 6.11, we see that

116

6 Dimensionality Reduction

proportion of
variance

we can set W = C.
Let us see an example to get some intuition (Rencher 1995): Assume
we are given a class of students with grades on ﬁve courses and we want
to order these students. That is, we want to project the data onto one
dimension, such that the diﬀerence between the data points become most
apparent. We can use PCA. The eigenvector with the highest eigenvalue
is the direction that has the highest variance, that is, the direction on
which the students are most spread out. This works better than taking
the average because we take into account correlations and diﬀerences in
variances.
In practice even if all eigenvalues are greater than 0, if |S| is small, red
membering that |S| = i=1 λi , we understand that some eigenvalues have
little contribution to variance and may be discarded. Then, we take into
account the leading k components that explain more than, for example,
90 percent, of the variance. When λi are sorted in descending order, the
proportion of variance explained by the k principal components is
λ1 + λ2 + · · · + λk
λ1 + λ2 + · · · + λk + · · · + λd

scree graph

If the dimensions are highly correlated, there will be a small number of
eigenvectors with large eigenvalues and k will be much smaller than d and
a large reduction in dimensionality may be attained. This is typically the
case in many image and speech processing tasks where nearby inputs (in
space or time) are highly correlated. If the dimensions are not correlated,
k will be as large as d and there is no gain through PCA.
Scree graph is the plot of variance explained as a function of the number of eigenvectors kept (see ﬁgure 6.2). By visually analyzing it, one can
also decide on k. At the “elbow,” adding another eigenvector does not
signiﬁcantly increase the variance explained.
Another possibility is to ignore the eigenvectors whose eigenvalues are
less than the average input variance. Given that i λi = i si2 (equal
to the trace of S, denoted as tr(S)), the average eigenvalue is equal to
the average input variance. When we keep only the eigenvectors with
eigenvalues greater than the average eigenvalue, we keep only those that
have variance higher than the average input variance.
If the variances of the original xi dimensions vary considerably, they
aﬀect the direction of the principal components more than the correlations, so a common procedure is to preprocess the data so that each
dimension has mean 0 and unit variance, before using PCA. Or, one may

117

6.3 Principal Components Analysis

(a) Scree graph for Optdigits

Eigenvalues

200

100

0

0

10

20

30
40
Eigenvectors

50

60

70

50

60

70

(b) Proportion of variance explained
1

Prop. of var.

0.8
0.6
0.4
0.2
0

0

10

20

30
40
Eigenvectors

Figure 6.2 (a) Scree graph. (b) Proportion of variance explained is given for the
Optdigits dataset from the UCI Repository. This is a handwritten digit dataset
with ten classes and sixty-four dimensional inputs. The ﬁrst twenty eigenvectors
explain 90 percent of the variance.

use the eigenvectors of the correlation matrix, R, instead of the covariance matrix, S, for the correlations to be eﬀective and not the individual
variances.
PCA explains variance and is sensitive to outliers: A few points distant
from the center would have a large eﬀect on the variances and thus the
eigenvectors. Robust estimation methods allow calculating parameters in
the presence of outliers. A simple method is to calculate the Mahalanobis
distance of the data points, discarding the isolated data points that are
far away.
If the ﬁrst two principal components explain a large percentage of the
variance, we can do visual analysis: We can plot the data in this two di-

118

6 Dimensionality Reduction

Optdigits after PCA
30
2
20

3
3

3

Second eigenvector

3

0

3

3

3

10

7

7
7
77
7

−10

−20

3

7
7
9

8

7

2

2
2 2
2
2
3
2
2
23 5 3
9

9

8
8 5 5
88
8

5 8 5
7 9
9
9
7
1 1
1
19
1
4
1
9 14
7
94 1 9
1
4
49 4
4

8
2
1

8 0
0
00 1

−20

−10

6
0

6 6
6
6
6
6
6

4
4
4

−30

−30

0

0

4

−40
−40

0
0

0
First eigenvector

10

4

20

30

40

Figure 6.3 Optdigits data plotted in the space of two principal components.
Only the labels of a hundred data points are shown to minimize the ink-to-noise
ratio.

eigenfaces
eigendigits

mensional space (ﬁgure 6.3) and search visually for structure, groups,
outliers, normality, and so forth. This plot gives a better pictorial description of the sample than a plot of any two of the original variables.
By looking at the dimensions of the principal components, we can also
try to recover meaningful underlying variables that describe the data. For
example, in image applications where the inputs are images, the eigenvectors can also be displayed as images and can be seen as templates for
important features; they are typically named “eigenfaces,” “eigendigits,”
and so forth (Turk and Pentland 1991).
When d is large, calculating, storing, and processing S may be tedious.
It is possible to calculate the eigenvectors and eigenvalues directly from
data without explicitly calculating the covariance matrix (Chatﬁeld and

6.3 Principal Components Analysis

119

Collins 1980).
We know from equation 5.15 that if x ∼ Nd (μ, Σ), then after projection
WT x ∼ Nk (WT μ, WT ΣW). If the sample contains d-variate normals, then
it projects to k-variate normals allowing us to do parametric discrimination in this hopefully much lower dimensional space. Because zj are
uncorrelated, the new covariance matrices will be diagonal, and if they
are normalized to have unit variance, Euclidean distance can be used in
this new space, leading to a simple classiﬁer.
Instance x t is projected to the z-space as
z t = WT (x t − μ)
When W is an orthogonal matrix such that WWT = I, it can be backprojected to the original space as
x t = Wz t + μ
ˆ

reconstruction
error

x t is the reconstruction of x t from its representation in the z-space.
ˆ
It is known that among all orthogonal linear projections, PCA minimizes
the reconstruction error, which is the distance between the instance and
its reconstruction from the lower dimensional space:
xt − x
ˆ

(6.12)

2

t

Karhunen-Loève
expansion

common principal
components

The reconstruction error depends on how many of the leading components are taken into account. In a visual recognition application—for
example, face recognition—displaying x t allows a visual check for inforˆ
mation loss during PCA.
PCA is unsupervised and does not use output information. It is a onegroup procedure. However, in the case of classiﬁcation, there are multiple
groups. Karhunen-Loève expansion allows using class information; for example, instead of using the covariance matrix of the whole sample, we can
estimate separate class covariance matrices, take their average (weighted
by the priors) as the covariance matrix, and use its eigenvectors.
In common principal components (Flury 1988), we assume that the principal components are the same for each class whereas the variances of
these components diﬀer for diﬀerent classes:
Si = CDi CT
This allows pooling data and is a regularization method whose complexity is less than that of a common covariance matrix for all classes,

120

6 Dimensionality Reduction

flexible
discriminant
analysis

6.4

while still allowing diﬀerentiation of Si . A related approach is ﬂexible
discriminant analysis (Hastie, Tibshirani, and Buja 1994), which does a
linear projection to a lower-dimensional space where all features are uncorrelated and then uses a minimum distance classiﬁer.

Factor Analysis
In PCA, from the original dimensions xi , i = 1, . . . , d, we form a new set
of variables z that are linear combinations of xi :
z = WT (x − μ)

factor analysis
latent factors

In factor analysis (FA), we assume that there is a set of unobservable,
latent factors zj , j = 1, . . . , k, which when acting in combination generate
x. Thus the direction is opposite that of PCA (see ﬁgure 6.4). The goal is
to characterize the dependency among the observed variables by means
of a smaller number of factors.
Suppose there is a group of variables that have high correlation among
themselves and low correlation with all the other variables. Then there
may be a single underlying factor that gave rise to these variables. If the
other variables can be similarly grouped into subsets, then a few factors
can represent these groups of variables. Though factor analysis always
partitions the variables into factor clusters, whether the factors mean
anything, or really exist, is open to question.
FA, like PCA, is a one-group procedure and is unsupervised. The aim is
to model the data in a smaller dimensional space without loss of information. In FA, this is measured as the correlation between variables.
As in PCA, we have a sample X = {x t }t drawn from some unknown
probability density with E[x] = μ and Cov(x) = Σ. We assume that
the factors are unit normals, E[zj ] = 0, Var(zj ) = 1, and are uncorrelated, Cov(zi , zj ) = 0, i = j. To explain what is not explained by the
factors, there is an added source for each input which we denote by i .
It is assumed to be zero-mean, E[ i ] = 0, and have some unknown variance, Var( i ) = ψi . These speciﬁc sources are uncorrelated among themselves, Cov( i , j ) = 0, i = j, and are also uncorrelated with the factors,
Cov( i , zj ) = 0, ∀i, j.
FA assumes that each input dimension, xi , i = 1, . . . , d, can be written
as a weighted sum of the k < d factors, zj , j = 1, . . . , k, plus the residual

121

6.4 Factor Analysis

Figure 6.4 Principal components analysis generates new variables that are linear combinations of the original input variables. In factor analysis, however,
we posit that there are factors that when linearly combined generate the input
variables.

term (see ﬁgure 6.5):
xi − μi

=

vi1 z1 + vi2 z2 + · · · + vik zk +

i , ∀i

= 1, . . . , d

k

(6.13)

xi − μi

=

vij zj +

i

j=1

This can be written in vector-matrix form as
(6.14)

x − μ = Vz +
where V is the d × k matrix of weights, called factor loadings. From now
on, we are going to assume that μ = 0 without loss of generality; we can
always add μ after projection. Given that Var(zj ) = 1 and Var( i ) = ψi

(6.15)

2
2
2
Var(xi ) = vi1 + vi2 + · · · + vik + ψi
k
2
j=1 vij

is the part of the variance explained by the common factors and
ψi is the variance speciﬁc to xi .
In vector-matrix form, we have
Cov(Vz + )
Cov(Vz) + Cov( )

=
(6.17)

Σ = Cov(x)

=
=

(6.16)

VCov(z)VT + Ψ

=

VVT + Ψ

122

z2

x2

6 Dimensionality Reduction

z1

x1

Figure 6.5 Factors are independent unit normals that are stretched, rotated,
and translated to make up the inputs.

where Ψ is a diagonal matrix with ψi on the diagonals. Because the factors are uncorrelated unit normals, we have Cov(z) = I. With two factors,
for example,
Cov(x1 , x2 ) = v11 v21 + v12 v22
If x1 and x2 have high covariance, then they are related through a factor. If it is the ﬁrst factor, then v11 and v21 will both be high; if it is the
second factor, then v12 and v22 will both be high. In either case, the sum
v11 v21 + v12 v22 will be high. If the covariance is low, then x1 and x2 depend on diﬀerent factors and in the products in the sum, one term will
be high and the other will be low and the sum will be low.
We see that
Cov(x1 , z2 ) = Cov(v12 z2 , z2 ) = v12 Var(z2 ) = v12
Thus Cov(x, z) = V, and we see that the loadings represent the correlations of variables with the factors.
Given S, the estimator of Σ, we would like to ﬁnd V and Ψ such that
S = VVT + Ψ
If there are only a few factors, that is, if V has few columns, then we
have a simpliﬁed structure for S, as V is d × k and Ψ has d values, thus
reducing the number of parameters from d 2 to d · k + d.
Since Ψ is diagonal, covariances are represented by V. Note that PCA
does not allow a separate Ψ and it tries to account for both the covariances and the variances. When all ψi are equal, namely, Ψ = ψI, we get

6.4 Factor Analysis

probabilistic PCA

123

probabilistic PCA (Tipping and Bishop 1999) and the conventional PCA is
when ψi are 0.
Let us now see how we can ﬁnd the factor loadings and the speciﬁc
variances: Let us ﬁrst ignore Ψ . Then, from its spectral decomposition,
we know that we have
S = CDCT = CD1/2 D1/2 CT = (CD1/2 )(CD1/2 )T
where we take only k of the eigenvectors by looking at the proportion of
variance explained so that C is the d × k matrix of eigenvectors and D1/2
is the k × k diagonal matrix with the square roots of the eigenvalues on
its diagonals. Thus we have

(6.18)

V = CD1/2
We can ﬁnd ψj from equation 6.15 as
k

(6.19)

ψi = si2 −

2
vij
j=1

Note that when V is multiplied with any orthogonal matrix—namely,
having the property TTT = I—that is another valid solution and thus the
solution is not unique.
S = (VT)(VT)T = VTTT VT = VIVT = VVT
If T is an orthogonal matrix, the distance to the origin does not change.
If z = Tx, then
z T z = (Tx)T (Tx) = x T TT Tx = x T x
Multiplying with an orthogonal matrix has the eﬀect of rotating the
axes and allows us to choose the set of axes most interpretable (Rencher
1995). In two dimensions,
T=

cos φ
sin φ

− sin φ
cos φ

rotates the axes by φ. There are two types of rotation: In orthogonal
rotation the factors are still orthogonal after the rotation, and in oblique
rotation the factors are allowed to become correlated. The factors are
rotated to give the maximum loading on as few factors as possible for
each variable, to make the factors interpretable. However, interpretability
is subjective and should not be used to force one’s prejudices on the data.

124

6 Dimensionality Reduction

There are two uses of factor analysis: It can be used for knowledge
extraction when we ﬁnd the loadings and try to express the variables
using fewer factors. It can also be used for dimensionality reduction
when k < d. We already saw how the ﬁrst one is done. Now, let us see
how factor analysis can be used for dimensionality reduction.
When we are interested in dimensionality reduction, we need to be able
to ﬁnd the factor scores, zj , from xi . We want to ﬁnd the loadings wji
such that
d

(6.20)

zj =

wji xi +

= 1, . . . , k

i, j

i=1

where xi are centered to have mean 0. In vector form, for observation t,
this can be written as
z t = WT x t + , ∀t = 1, . . . , N
This is a linear model with d inputs and k outputs. Its transpose can
be written as
(z t )T = (x t )T W +

T

, ∀t = 1, . . . , N

Given that we have a sample of N observations, we write
(6.21)

Z = XW + Ξ
where Z is N × k of factors, X is N × d of (centered) observations, and Ξ
is N × k of zero-mean noise. This is multivariate linear regression with
multiple outputs, and we know from section 5.8 that W can be found as
W = (XT X)−1 XT Z
but we do not know Z; it is what we would like to calculate. We multiply
and divide both sides by N − 1 and obtain
W

=
=

(6.22)

=

(N − 1)(XT X)−1
XT X
N −1

−1

XT Z
N−1

XT Z
N −1

S−1 V

and placing equation 6.22 in equation 6.21, we write
(6.23)

Z = XW = XS−1 V

6.5 Multidimensional Scaling

125

assuming that S is nonsingular. One can use R instead of S when xi are
normalized to have unit variance.
For dimensionality reduction, FA oﬀers no advantage over PCA except the interpretability of factors allowing the identiﬁcation of common
causes, a simple explanation, and knowledge extraction. For example, in
the context of speech recognition, x corresponds to the acoustic signal,
but we know that it is the result of the (nonlinear) interaction of a small
number of articulators, namely, jaw, tongue, velum, lips, and mouth,
which are positioned appropriately to shape the air as it comes out of
the lungs and generate the speech sound. If a speech signal could be
transformed to this articulatory space, then recognition would be much
easier. Using such generative models is one of the current research directions for speech recognition; in chapter 16, we will discuss how such
models can be represented as a graphical model.

6.5

multidimensional
scaling

Multidimensional Scaling
Let us say for N points, we are given the distances between pairs of
points, dij , for all i, j = 1, . . . , N. We do not know the exact coordinates
of the points, their dimensionality, or how the distances are calculated.
Multidimensional scaling (MDS) is the method for placing these points in
a low—for example, two-dimensional—space such that the Euclidean distance between them there is as close as possible to dij , the given distances
in the original space. Thus it requires a projection from some unknown
dimensional space to, for example, two dimensions.
In the archetypical example of multidimensional scaling, we take the
road travel distances between cities, and after applying MDS, we get an
approximation to the map. The map is distorted such that in parts of
the country with geographical obstacles like mountains and lakes where
the road travel distance deviates much from the direct bird-ﬂight path
(Euclidean distance), the map is stretched out to accommodate longer
distances (see ﬁgure 6.6). The map is centered on the origin, but the solution is still not unique. We can get any rotated or mirror image version.
MDS can be used for dimensionality reduction by calculating pairwise
Euclidean distances in the d-dimensional x space and giving this as input
to MDS, which then projects it to a lower-dimensional space so as to
preserve these distances.
Let us say we have a sample X = {x t }N as usual, where x t ∈ d . For
t=1

126

6 Dimensionality Reduction

2000
Helsinki
1500

Moscow

1000
Dublin
London
500
Berlin

Paris
0
Zurich
−500

Lisbon

Madrid

−1000

Istanbul
Rome

−1500

Athens

−2000
−2500

−2000

−1500

−1000

−500

0

500

1000

1500

2000

Figure 6.6 Map of Europe drawn by MDS. Pairwise road travel distances between these cities are given as input, and MDS places them in two dimensions
such that these distances are preserved as well as possible.

two points r and s, the squared Euclidean distance between them is
d
2
dr s

=

xr − xs

2

j=1

=

(6.24)

d

(xr − xs )2 =
j
j

=

d

(xr )2 − 2
j
j=1

d

xr xs +
j j
j=1

(xs )2
j
j=1

br r + bss − 2br s

where br s is deﬁned as
d

(6.25)

xr xs
j j

br s =
j=1

To constrain the solution, we center the data at the origin and assume
N

xt = 0, ∀j = 1, . . . , d
j
t=1

127

6.5 Multidimensional Scaling

Then, summing up equation 6.24 on r , s, and both r , s, and deﬁning
N

(xt )2
j

btt =

T =

t

t=1

j

we get
2
dr s

=

T + Nbss

2
dr s

=

Nbr r + T

2
dr s

=

2NT

r

s

r

s

When we deﬁne
2
d•s =

1
N

r

2
2
dr s , dr • =

1
N

s

2
2
dr s , d•• =

1
N2

2
dr s
r

s

and using equation 6.24, we get
(6.26)

br s =

1 2
2
2
2
(d + d•s − d•• − dr s )
2 r•

Having now calculated br s and knowing that B = XXT as deﬁned in
equation 6.25, we look for an approximation. We know from the spectral
decomposition that X = CD1/2 can be used as an approximation for X,
where C is the matrix whose columns are the eigenvectors of B and D1/2
is a diagonal matrix with square roots of the eigenvalues on the diagonals.
Looking at the eigenvalues of B, we decide on a dimensionality k lower
than d (and N), as we did in PCA and FA. Let us say c j are the eigenvectors
with λj as the corresponding eigenvalues. Note that c j is N-dimensional.
Then we get the new dimensions as
(6.27)

t
t
zj = λj cj , j = 1, . . . , k, t = 1, . . . , N

That is, the new coordinates of instance t are given by the tth elements
of the eigenvectors, c j , j = 1, . . . , k, after normalization.
It has been shown (Chatﬁeld and Collins 1980) that the eigenvalues of
XXT (N × N) are the same as those of XT X (d × d) and the eigenvectors
are related by a simple linear transformation. This shows that PCA does
the same work with MDS and does it more cheaply. PCA done on the correlation matrix rather than the covariance matrix equals doing MDS with
standardized Euclidean distances where each variable has unit variance.

128

6 Dimensionality Reduction

In the general case, we want to ﬁnd a mapping z = g(x|θ), where
z ∈ k , x ∈ d , and g(x|θ) is the mapping function from d to k dimensions deﬁned up to a set of parameters θ. Classical MDS we discussed
previously corresponds to a linear transformation
(6.28)

Sammon mapping

z = g(x|W) = WT x
but in a general case, a nonlinear mapping can also be used; this is called
Sammon mapping. The normalized error in mapping is called the Sammon stress and is deﬁned as

r ,s

( z r − z s − x r − x s )2
xr − x s 2

r ,s

E(θ|X)

( g(x r |θ) − g(x s |θ) − xr − x s )2
xr − xs 2

=
=

(6.29)

One can use any regression method for g(·|θ) and estimate θ to minimize the stress on the training data X. If g(·) is nonlinear in x, this will
then correspond to a nonlinear dimensionality reduction.
In the case of classiﬁcation, one can include class information in the
distance (see Webb 1999) as
dr s = (1 − α)dr s + αcr s
where cr s is the “distance” between the classes x r and x s belong to. This
interclass distance should be supplied subjectively and α is optimized
using cross-validation.

6.6
linear discriminant
analysis

(6.30)

Linear Discriminant Analysis
Linear discriminant analysis (LDA) is a supervised method for dimensionality reduction for classiﬁcation problems. We start with the case where
there are two classes, then generalize to K > 2 classes.
Given samples from two classes C1 and C2 , we want to ﬁnd the direction, as deﬁned by a vector w, such that when the data are projected onto
w, the examples from the two classes are as well separated as possible.
As we saw before,
z = wT x
is the projection of x onto w and thus is a dimensionality reduction from
d to 1.

129

x2

6.6 Linear Discriminant Analysis

m2
m1
m2
s2 2

w

m1
s1 2
x1

Figure 6.7 Two-dimensional, two-class data projected on w.

m 1 and m1 are the means of samples from C1 before and after projection, respectively. Note that m 1 ∈ d and m1 ∈ . We are given a sample
X = {x t , r t } such that r t = 1 if x t ∈ C1 and r t = 0 if x t ∈ C2 .
m1
(6.31)

=

t

m2

=

t

w T xt r t
= w T m1
t
tr
w T x t (1 − r t )
= w T m2
t
t (1 − r )

The scatter of samples from C1 and C2 after projection are

scatter
2
s1

(w T x t − m1 )2 r t

=
t

(6.32)

2
s2

(w T x t − m2 )2 (1 − r t )

=
t

Fisher’s linear
discriminant

(6.33)

After projection, for the two classes to be well separated, we would like
the means to be as far apart as possible and the examples of classes be
scattered in as small a region as possible. So we want |m1 − m2 | to be
2
2
large and s1 + s2 to be small (see ﬁgure 6.7). Fisher’s linear discriminant
is w that maximizes
J(w) =

(m1 − m2 )2
2
2
s1 + s2

130

6 Dimensionality Reduction

Rewriting the numerator, we get
(m1 − m2 )2

between-class
scatter matrix

(w T m 1 − w T m 2 )2

=

w T (m 1 − m 2 )(m 1 − m 2 )T w

=

(6.34)

=

w T SB w

where SB = (m 1 − m2 )(m 1 − m 2 )T is the between-class scatter matrix. The
denominator is the sum of scatter of examples of classes around their
means after projection and can be rewritten as
2
s1

(w T x t − m1 )2 r t

=
t

w T (x t − m 1 )(x t − m 1 )T wr t

=
t

(6.35)

=

w T S1 w

where
(6.36)

r t (x t − m 1 )(x t − m 1 )T

S1 =
t

within-class
scatter matrix

is the within-class scatter matrix for C1 . S1 / t r t is the estimator of Σ1 .
2
Similarly, s2 = w T S2 w with S2 = t (1 − r t )(x t − m 2 )(x t − m 2 )T , and we
get
2
2
s1 + s2 = w T S W w
2
2
where SW = S1 + S2 is the total within-class scatter. Note that s1 + s2
divided by the total number of samples is the variance of the pooled
data. Equation 6.33 can be rewritten as

(6.37)

J(w) =

w T SB w
|w T (m 1 − m 2 )|2
=
w T SW w
w T SW w

Taking the derivative of J with respect to w and setting it equal to 0, we
get
w T (m 1 − m 2 )
w T SW w

2(m 1 − m 2 ) −

w T (m 1 − m 2 )
SW w
w T SW w

=0

Given that w T (m 1 − m 2 )/w T SW w is a constant, we have
(6.38)

w = cS−1 (m 1 − m 2 )
W
where c is some constant. Because it is the direction that is important for
us and not the magnitude, we can just take c = 1 and ﬁnd w.

6.6 Linear Discriminant Analysis

131

Remember that when p(x|Ci ) ∼ N (μi , Σ), we have a linear discriminant where w = Σ−1 (μ1 − μ2 ), and we see that Fisher’s linear discriminant is optimal if the classes are normally distributed. Under the same
assumption, a threshold, w0 , can also be calculated to separate the two
classes. But Fisher’s linear discriminant can be used even when the classes
are not normal. We have projected the samples from d dimensions to one,
and any classiﬁcation method can be used afterward.
In the case of K > 2 classes, we want to ﬁnd the matrix W such that
(6.39)

z = WT x
where z is k-dimensional and W is d × k. The within-class scatter matrix
for Ci is

(6.40)

rit (x t − m i )(x t − m i )T

Si =
t

where rit = 1 if x t ∈ Ci and 0 otherwise. The total within-class scatter is
K

(6.41)

SW =

Si
i=1

When there are K > 2 classes, the scatter of the means is calculated as
how much they are scattered around the overall mean
(6.42)

m=

1
K

K

mi
i=1

and the between-class scatter matrix is
K

(6.43)

Ni (m i − m)(m i − m)T

SB =
i=1

with Ni = t rit . The between-class scatter matrix after projection is
WT SB W and the within-class scatter matrix after projection is WT SW W.
These are both k × k matrices. We want the ﬁrst scatter to be large, that
is, after the projection, in the new k-dimensional space we want class
means to be as far apart from each other as possible. We want the second scatter to be small, that is, after the projection, we want samples
from the same class to be as close to their mean as possible. For a scatter
(or covariance) matrix, a measure of spread is the determinant, remembering that the determinant is the product of eigenvalues and that an

132

6 Dimensionality Reduction

Optdigits after LDA
4
7
3

7

7

4

7
7
77 7
7
7
7 7

2

91
1
3

0
3
3
−1

1

3

3
3 33
2

3

2

9

3

8
5
2

5
2
2 3 55
2 22

4

9

8

66
6

8 8
88
8

66
6

8
0
0

−3
0
0
0

−4

−1.5

4

6

2

−2

4

8

2

−5
−2.5

4

4
4

1
9
4
9
9 9 1
9
1
1
1 44
5
1
9
1 9
9
1

2
−2

4

1

4

−1

−0.5

0

0.5

0
0
0 0

0

1

1.5

2

2.5

Figure 6.8 Optdigits data plotted in the space of the ﬁrst two dimensions found
by LDA. Comparing this with ﬁgure 6.3, we see that LDA, as expected, leads to a
better separation of classes than PCA. Even in this two-dimensional space (there
are nine altogether), we can discern separate clouds for diﬀerent classes.

eigenvalue gives the variance along its eigenvector (component). Thus we
are interested in the matrix W that maximizes
(6.44)

J(W) =

|WT SB W|
|WT SW W|

The largest eigenvectors of S−1 SB are the solution. SB is the sum of K
W
matrices of rank 1, namely, (m i − m)(m i − m)T , and only K − 1 of them
are independent. Therefore, SB has a maximum rank of K − 1 and we
take k = K − 1. Thus we deﬁne a new lower, (K − 1)-dimensional space
where the discriminant is then to be constructed (see ﬁgure 6.8). Though
LDA uses class separability as its goodness criterion, any classiﬁcation
method can be used in this new space for estimating the discriminants.

6.7 Isomap

133

We see that to be able to apply LDA, SW should be invertible. If this
is not the case, we can ﬁrst use PCA to get rid of singularity and then
apply LDA to its result; however, we should make sure that PCA does not
reduce dimensionality so much that LDA does not have anything left to
work on.

6.7

geodesic distance
isometric feature
mapping

Isomap
Principal component analysis (PCA), which we discussed in section 6.3,
works when the data lies in a linear subspace. However, this may not
hold in many applications. Take, for example, face recognition where a
face is represented as a two-dimensional, say 100 × 100 image. In this
case, each face is a point in 10, 000 dimensions. Now let us say that
we take a series of pictures as a person slowly rotates his or her head
from right to left. The sequence of face images we capture follows a
trajectory in the 10, 000-dimensional space and this is not linear. Now
consider the faces of many people. The trajectories of all their faces as
they rotate their faces deﬁne a manifold in the 10, 000-dimensional space,
and this is what we want to model. The similarity between two faces
cannot simply be written in terms of the sum of the pixel diﬀerences,
and hence Euclidean distance is not a good metric. It may even be the
case that images of two diﬀerent people with the same pose have smaller
Euclidean distance between them than the images of two diﬀerent poses
of the same person. This is not what we want. What should count is
the distance along the manifold, which is called the geodesic distance.
Isometric feature mapping (Isomap) (Tenenbaum, de Silva, and Langford
2000) estimates this distance and applies multidimensional scaling (MDS)
(section 6.5), using it for dimensionality reduction.
Isomap uses the geodesic distances between all pairs of data points.
For neighboring points that are close in the input space, Euclidean distance can be used; for small changes in pose, the manifold is locally
linear. For faraway points, geodesic distance is approximated by the sum
of the distances between the points along the way over the manifold.
This is done by deﬁning a graph whose nodes correspond to the N data
points and whose edges connect neighboring points (those with distance
less than some or one of the n nearest) with weights corresponding
to Euclidean distances. The geodesic distance between any two points is
calculated as the length of the shortest path between the corresponding

134

6 Dimensionality Reduction

Figure 6.9 Geodesic distance is calculated along the manifold as opposed to
the Euclidean distance that does not use this information. After multidimensional scaling, these two instances from two classes will be mapped to faraway
positions in the new space, though they are close in the original space.

two nodes. For two points that are not close by, we need to hop over a
number of intermediate points along the way, and therefore the distance
will be the distance along the manifold, approximated as the sum of local
Euclidean distances (see ﬁgure 6.9).
Two nodes r and s are connected if xr − x s < (while making sure
that the graph is connected), or if x s is one of the n neighbors of x r (while
making sure that the distance matrix is symmetric), and we set the edge
length to x r − x s . For any two nodes r and s, dr s is the length of the
shortest path between them. We then apply MDS on dr s to reduce dimensionality to k by observing the proportion of variance explained. This will
have the eﬀect of placing r and s that are far apart in the geodesic space
also far in the new k-dimensional space even if they are close in terms of
Euclidean distance in the original d-dimensional space.
It is clear that the graph distances provide a better approximation as
the number of points increases, though there is the trade-oﬀ of longer
execution time; if time is critical, one can subsample and use a subset
of “landmark points” to make the algorithm faster. The parameter
needs to be carefully tuned; if it is too small, there may be more than
one connected component, and if it is too large, “shortcut” edges may be
added that corrupt the low-dimensional embedding (Balasubramanian et
al. 2002).
One problem with Isomap, as with MDS, is that it places the N points in
a low-dimensional space, but it does not learn a general mapping function
that will allow mapping a new test point; the new point should be added

135

6.8 Locally Linear Embedding

to the dataset and the whole algorithm needs to be run once more using
N + 1 instances.

6.8
locally linear
embedding

(6.45)

Locally Linear Embedding
Locally linear embedding (LLE) recovers global nonlinear structure from
locally linear ﬁts (Roweis and Saul 2000). The idea is that each local
patch of the manifold can be approximated linearly and given enough
data, each point can be written as a linear, weighted sum of its neighbors
(again either deﬁned using a given number of neighbors, n, or distance
threshold, ). Given x r and its neighbors x s ) in the original space, one
(r
can ﬁnd the reconstruction weights Wr s that minimize the error function
E w (W|X) =

Wr s x s )
(r

xr −
r

2

s

using least squares subject to Wr r = 0, ∀r and s Wr s = 1.
The idea in LLE is that the reconstruction weights Wr s reﬂect the intrinsic geometric properties of the data that we expect to be also valid
for local patches of the manifold, that is, the new space we are mapping
the instances to (see ﬁgure 6.10). The second step of LLE is hence to now
keep the weights Wr s ﬁxed and let the new coordinates z r take whatever values they need respecting the interpoint constraints given by the
weights:
(6.46)

E z (Z|W) =

zr −
r

Wr s z s

2

s

Nearby points in the original, d-dimensional space should remain nearby
and similarly colocated with respect to one another in the new, k-dimensional
space. Equation 6.46 can be rewritten as
(6.47)

E z (Z|W) =

Mr s (z r )T z s
r ,s

where
(6.48)

Mr s = δr s − Wr s − Wsr +

Wir Wis
i

M is sparse (only a small percentage of data points are neighbors of a
data point: n
N), symmetric, and positive semideﬁnite. As in other
dimensionality reduction methods, we require that the data be centered
at the origin, E[z] = 0, and that the new coordinates be uncorrelated

136

6 Dimensionality Reduction

Figure 6.10 Local linear embedding ﬁrst learns the constraints in the original
space and next places the points in the new space respecting those constraints.
The constraints are learned using the immediate neighbors (shown with continuous lines) but also propagate to second-order neighbors (shown dashed).

and unit length: Cov(z) = I. The solution to equation 6.47 subject to
these two constraints is given by the k + 1 eigenvectors with the smallest
eigenvalues; we ignore the lowest one and the other k eigenvectors give
us the new coordinates.
Because the n neighbors span a space of dimensionality n − 1 (you
need distances to three points to uniquely specify your location in two
dimensions), LLE can reduce dimensionality up to k ≤ n−1. It is observed
(Saul and Roweis 2003) that some margin between k and n is necessary to
obtain a good embedding. Note that if n (or ) is small, the graph (that is
constructed by connecting each instance to its neighbors) may no longer
be connected and it may be necessary to run LLE separately on separate
components to ﬁnd separate manifolds in diﬀerent parts of the input
space. On the other hand, if n (or ) is taken large, some neighbors may
be too far for the local linearity assumption to hold and this may corrupt
the embedding. It is possible to use diﬀerent n (or ) in diﬀerent parts
of the input space based on some prior knowledge, but how this can be
done is open to research (Saul and Roweis 2003).
As with Isomap, LLE solution is the set of new coordinates for the N
points, but we do not learn a mapping and hence cannot ﬁnd z for a new
x . There are two solutions to this:

137

6.8 Locally Linear Embedding

1. Using the same idea, one can ﬁnd the n neighbors of x in the original
d-dimensional space and ﬁrst learn the reconstruction weights w j that
minimizes
(6.49)

E w (w|X) = x −

w s xs

2

s

and then use them to reconstruct z in the new k-dimensional space:
(6.50)

w s zs

z =
s

Note that this approach can also be used to interpolate from an Isomap
(or MDS) solution. The drawback however is the need to store the
whole set of {x t , z t }N .
t=1
2. Using X = {x t , z t }N as a training set, one can train any regressor,
t=1
g(x t |θ)—for example, a multilayer perceptron (chapter 11)—as a generalizer to approximate z t from x t , whose parameters θ is learned to
minimize the regression error:
(6.51)

z t − g(x t |θ)

E(θ|X) =

2

t

Once training is done, we can calculate z = g(x |θ). The model g(·)
should be carefully chosen to be able to learn the mapping. There
may no longer be a unique optimum and hence there are all the usual
problems related to minimization, that is, initialization, local optima,
convergence, and so on.
In both Isomap and LLE, there is local information that is propagated
over neighbors to get a global solution. In Isomap, the geodesic distance
is the sum of local distances; in LLE, the ﬁnal opimization in placing z t
takes into account all local Wr s values. Let us say a and b are neighbors
and b and c are neighbors. Though a and c may not be neighbors, there
is dependence between a and c either through the graph, dac = dab + dbc ,
or the weights Wab and Wbc . In both algorithms, the global nonlinear
organization is found by integrating local linear constraints that overlap
partially.

138

6 Dimensionality Reduction

6.9

wrappers

Notes
A survey of feature selection algorithms is given in Devijer and Kittler
1982. Feature subset selection algorithms are also known as the wrapper approach, where the feature selection is thought to “wrap” around
the learner it uses as a subroutine (Kohavi and John 1997). Subset selection in regression is discussed in Miller 1990. The forward and backward
search procedures we discussed are local search procedures. Fukunaga
and Narendra (1977) proposed a branch and bound procedure. At considerable more expense, one can use a stochastic procedure like simulated
annealing or genetic algorithms to search more widely in the the search
space.
There are also ﬁltering algorithms for feature selection where heuristic
measures are used to calculate the “relevance” of a feature in a preprocessing stage without actually using the learner. For example, in the case
of classiﬁcation, instead of training a classiﬁer and testing it at each step,
one can use a separability measure, like the one used in linear discriminant analysis, to measure the quality of the new space in separating
classes from each other (McLachlan 1992). With the cost of computation
going down, it is best to include the learner in the loop because there
is no guarantee that the heuristic used by the ﬁlter will match the bias
of the learner that uses the features; no heuristic can replace the actual
validation accuracy. A survey of feature selection methods is given by
Guyon and Elisseeﬀ (2003).
Projection methods work with numeric inputs, and discrete variables
should be represented by 0/1 dummy variables, whereas subset selection
can use discrete inputs directly. Finding the eigenvectors and eigenvalues
is quite straightforward; an example of a code is given in Press et al. 1992.
Factor analysis was introduced by the British psychologist Charles Spearman to ﬁnd the single factor for intelligence which explains the correlation between scores on various intelligence tests. The existence of such
a single factor, called g, is highly disputed. More information on multidimensional scaling can be found in Cox and Cox 1994.
The projection methods we discussed are batch procedures in that they
require that the whole sample be given before the projection directions
are found. Mao and Jain (1995) discuss online procedures for doing PCA
and LDA, where instances are given one by one and updates are done
as new instances arrive. Another possibility in doing a nonlinear projection is when the estimator in Sammon mapping is taken as a nonlinear

6.10 Exercises

139

function, for example, a multilayer perceptron (section 11.11) (Mao and
Jain 1995). It is also possible but much harder to do nonlinear factor analysis. When the models are nonlinear, it is diﬃcult to come up with the
right nonlinear model. One also needs to use complicated optimization
and approximation methods to solve for the model parameters.
For more information, one can refer to the Isomap homepage that
is at http://web.mit.edu/cocosci/isomap/isomap.html and the LLE
homepage is at http://www.cs.toronto.edu/∼roweis/lle/. Both contain links to related publications and example code.
Just as we implement polynomial regression by using linear regression
where we consider high-order terms as additional inputs (section 5.8),
another way to do nonlinear dimensionality reduction is to ﬁrst map to
a new space by using nonlinear basis functions and then use a linear
method there. In chapter 13 where we will discuss kernel methods, we
will see how this can be done eﬃciently.
There is a trade-oﬀ between feature extraction and decision making.
If the feature extractor is good, the task of the classiﬁer (or regressor)
becomes trivial, for example, when the class code is extracted as a new
feature from the existing features. On the other hand, if the classiﬁer
is good enough, then there is no need for feature extraction; it does its
automatic feature selection or combination internally. We live between
these two ideal worlds.
There exist algorithms that do some feature selection internally, though
in a limited way. Decision trees (chapter 9) do feature selection while
generating the decision tree, and multilayer perceptrons (chapter 11) do
nonlinear feature extraction in the hidden nodes. We expect to see more
development along this line in embedding feature extraction in the actual
step of classiﬁcation/regression.

6.10

Exercises
1. Assuming that the classes are normally distributed, in subset selection, when
one variable is added or removed, how can the new discriminant be calculated
quickly? For example, how can the new S−1 be calculated from S−1 ?
new
old
2. Using Optdigits from the UCI repository, implement PCA. For various number
of eigenvectors, reconstruct the digit images and calculate the reconstruction
error (equation 6.12).
3. Plot the map of your state/country using MDS, given the road travel distances
as input.

140

6 Dimensionality Reduction

4. In Sammon mapping, if the mapping is linear, namely, g(x|W) = WT x, how
can W that minimizes the Sammon stress be calculated?
5. Redo exercise 3, this time using Isomap where two cities are connected only
if there is a direct road between them that does not pass through any other
city.
6. In Isomap, instead of using Euclidean distance, we can also use Mahalanobis
distance between neighboring points. What are the advantages and disadvantages of this approach, if any?
7. Draw two-class, two-dimensional data such that (a) PCA and LDA ﬁnd the
same direction and (b) PCA and LDA ﬁnd totally diﬀerent directions.
8. Multidimensional scaling can work as long as we have the pairwise distances
between objects. We do not actually need to represent the objects as vectors at all as long as we have some measure of similarity. Can you give an
example?
9. How can we incorporate class information into Isomap and LLE such that
instances of the same class are mapped to nearby locations in the new space?
10. In factor analysis, how can we ﬁnd the remaining ones if we already know
some of the factors?
11. Discuss an application where there are hidden factors (not necessarily linear)
and where factor analysis would be expected to work well.

6.11

References
Balasubramanian, M., E. L. Schwartz, J. B. Tenenbaum, V. de Silva, and J. C.
Langford. 2002. “The Isomap Algorithm and Topological Stability.” Science
295: 7.
Chatﬁeld, C., and A. J. Collins. 1980. Introduction to Multivariate Analysis.
London: Chapman and Hall.
Cox, T. F., and M. A. A. Cox. 1994. Multidimensional Scaling. London: Chapman
and Hall.
Devijer, P. A., and J. Kittler. 1982. Pattern Recognition: A Statistical Approach.
New York: Prentice-Hall.
Flury, B. 1988. Common Principal Components and Related Multivariate Models.
New York: Wiley.
Fukunaga, K., and P. M. Narendra. 1977. “A Branch and Bound Algorithm for
Feature Subset Selection.” IEEE Transactions on Computers C-26: 917–922.
Guyon, I., and A. Elisseeﬀ. 2003. “An Introduction to Variable and Feature
Selection.” Journal of Machine Learning Research 3: 1157–1182.

6.11 References

141

Hastie, T. J., R. J. Tibshirani, and A. Buja. 1994. “Flexible Discriminant Analysis
by Optimal Scoring.” Journal of the American Statistical Association 89: 1255–
1270.
Kohavi, R., and G. John. 1997. “Wrappers for Feature Subset Selection.” Artiﬁcial Intelligence 97: 273–324.
Mao, J., and A. K. Jain. 1995. “Artiﬁcial Neural Networks for Feature Extraction
and Multivariate Data Projection.” IEEE Transactions on Neural Networks 6:
296–317.
McLachlan, G. J. 1992. Discriminant Analysis and Statistical Pattern Recognition.
New York: Wiley.
Miller, A. J. 1990. Subset Selection in Regression. London: Chapman and Hall.
Press, W. H., B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. 1992. Numerical Recipes in C. Cambridge, UK: Cambridge University Press.
Pudil, P., J. Novovi˘ová, and J. Kittler. 1994. “Floating Search Methods in Feature
c
Selection.” Pattern Recognition Letters 15: 1119–1125.
Rencher, A. C. 1995. Methods of Multivariate Analysis. New York: Wiley.
Roweis, S. T., and L. K. Saul. 2000. “Nonlinear Dimensionality Reduction by
Locally Linear Embedding.” Science 290: 2323–2326.
Saul, K. K., and S. T. Roweis. 2003. “Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds.” Journal of Machine Learning
Research 4: 119–155.
Tenenbaum, J. B., V. de Silva, and J. C. Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” Science 290: 2319–
2323.
Tipping, M. E., and C. M. Bishop. 1999. “Probabilistic Principal Components
Analysis.” Journal of the Royal Statistical Society Series B 61: 611–622.
Turk, M., and A. Pentland. 1991. “Eigenfaces for Recognition.” Journal of
Cognitive Neuroscience 3: 71–86.
Webb, A. 1999. Statistical Pattern Recognition. London: Arnold.

7

Clustering

In the parametric approach, we assumed that the sample comes
from a known distribution. In cases when such an assumption is
untenable, we relax this assumption and use a semiparametric approach that allows a mixture of distributions to be used for estimating the input sample. Clustering methods allow learning the mixture
parameters from data. In addition to probabilistic modeling, we discuss vector quantization and hierarchical clustering.

7.1

Introduction
I n c h a p te r s 4 and 5, we discussed the parametric method for density
estimation where we assumed that the sample X is drawn from some
parametric family, for example, Gaussian. In parametric classiﬁcation,
this corresponds to assuming a certain density for the class densities
p(x|Ci ). The advantage of any parametric approach is that given a model,
the problem reduces to the estimation of a small number of parameters,
which, in the case of density estimation, are the suﬃcient statistics of the
density, for example, the mean and covariance in the case of Gaussian
densities.
Though parametric approaches are used quite frequently, assuming a
rigid parametric model may be a source of bias in many applications
where this assumption does not hold. We thus need more ﬂexible models.
In particular, assuming Gaussian density corresponds to assuming that
the sample, for example, instances of a class, forms one single group in
the d-dimensional space, and as we saw in chapter 5, the center and the
shape of this group is given by the mean and the covariance respectively.
In many applications, however, the sample is not one group; there may

144

7

semiparametric
density estimation

7.2
mixture density

Clustering

be several groups. Consider the case of optical character recognition:
There are two ways of writing the digit 7; the American writing is ‘7’,
whereas the European writing style has a horizontal bar in the middle (to
tell it apart from the European ‘1’, which keeps the small stroke on top in
handwriting). In such a case, when the sample contains examples from
both continents, the class for the digit 7 should be represented as the
disjunction of two groups. If each of these groups can be represented by
a Gaussian, the class can be represented by a mixture of two Gaussians,
one for each writing style.
A similar example is in speech recognition where the same word can be
uttered in diﬀerent ways, due to diﬀerent pronounciation, accent, gender,
age, and so forth. Thus when there is not a single, universal prototype,
all these diﬀerent ways should be represented in the density to be statistically correct.
We call this approach semiparametric density estimation, as we still
assume a parametric model for each group in the sample. We discuss
the nonparametric approach in chapter 8, which is used when there is no
structure to the data and even a mixture model is not applicable. In this
chapter, we focus on density estimation and defer supervised learning to
chapter 12.

Mixture Densities
The mixture density is written as
k

(7.1)

p(x) =

p(x|Gi )P (Gi )
i=1

mixture
components
groups
clusters
component
densities
mixture
proportions

where Gi are the mixture components. They are also called group or clusters. p(x|Gi ) are the component densities and P (Gi ) are the mixture proportions. The number of components, k, is a hyperparameter and should
be speciﬁed beforehand. Given a sample and k, learning corresponds to
estimating the component densities and proportions. When we assume
that the component densities obey a parametric model, we need only
estimate their parameters. If the component densities are multivariate
Gaussian, we have p(x|Gi ) ∼ N (μi , Σi ), and Φ = {P (Gi ), μi , Σi }k are the
i=1
parameters that should be estimated from the iid sample X = {x t }t .

7.3 k-Means Clustering

145

Parametric classiﬁcation is a bona ﬁde mixture model where groups,
Gi , correspond to classes, Ci , component densities p(x|Gi ) correspond to
class densities p(x|Ci ), and P (Gi ) correspond to class priors, P (Ci ):
K

p(x) =

p(x|Ci )P (Ci )
i=1

In this supervised case, we know how many groups there are and learning the parameters is trivial because we are given the labels, namely,
which instance belongs to which class (component). We remember from
chapter 5 that when we are given the sample X = {x t , r t }N , where rit = 1
t=1
if x t ∈ Ci and 0 otherwise, the parameters can be calculated using maximum likelihood. When each class is Gaussian distributed, we have a
Gaussian mixture, and the parameters are estimated as
ˆ
P (Ci )

=

mi

=

Si

(7.2)

=

t
t ri

N
t t
t ri x
t
t ri
t
t
t ri (x

− m i )(x t − m i )T
t
t ri

The diﬀerence in this chapter is that the sample is X = {x t }t : We have
an unsupervised learning problem. We are given only x t and not the labels
r t , that is, we do not know which x t comes from which component. So we
should estimate both: First, we should estimate the labels, rit , the component that a given instance belongs to; and, second, once we estimate the
labels, we should estimate the parameters of the components given the
set of instances belonging to them. We are ﬁrst going to discuss a simple
algorithm, k-means clustering, for this purpose and later on show that it
is a special case of the Expectation-Maximization algorithm.

7.3

color quantization

k-Means Clustering
Let us say we have an image that is stored with 24 bits/pixel and can have
up to 16 million colors. Assume we have a color screen with 8 bits/pixel
that can display only 256 colors. We want to ﬁnd the best 256 colors
among all 16 million colors such that the image using only the 256 colors
in the palette looks as close as possible to the original image. This is color
quantization where we map from high to lower resolution. In the general

146

7

vector
quantization

reference vectors

Clustering

case, the aim is to map from a continuous space to a discrete space; this
process is called vector quantization.
Of course we can always quantize uniformly, but this wastes the colormap by assigning entries to colors not existing in the image, or would
not assign extra entries to colors frequently used in the image. For example, if the image is a seascape, we expect to see many shades of blue
and maybe no red. So the distribution of the colormap entries should
reﬂect the original density as close as possible placing many entries in
high-density regions, discarding regions where there is no data.
Let us say we have a sample of X = {x t }N . We have k reference
t=1
vectors, m j , j = 1, . . . , k. In our example of color quantization, x t are the
image pixel values in 24 bits and m j are the color map entries also in 24
bits, with k = 256.
Assume for now that we somehow have the m j values; we will discuss
how to learn them shortly. Then in displaying the image, given the pixel,
x t , we represent it with the most similar entry, m i in the color map,
satisfying
xt − m i = min x t − m j
j

codebook vectors
code words

compression

reconstruction
error

(7.3)

That is, instead of the original data value, we use the closest value we
have in the alphabet of reference vectors. m i are also called codebook
vectors or code words, because this is a process of encoding/decoding
(see ﬁgure 7.1): Going from x t to i is a process of encoding the data using
the codebook of m i , i = 1, . . . , k and, on the receiving end, generating m i
from i is decoding. Quantization also allows compression: For example,
instead of using 24 bits to store (or transfer over a communication line)
each x t , we can just store/transfer its index i in the colormap using 8 bits
to index any one of 256, and we get a compression rate of almost 3; there
is also the color map to store/transfer.
Let us see how we can calculate m i : When x t is represented by m i , there
is an error that is proportional to the distance, x t − m i . For the new
image to look like the original image, we should have these distances as
small as possible for all pixels. The total reconstruction error is deﬁned
as
E({mi }k |X) =
i=1

t
bi xt − m i
t

i

2

147

7.3 k-Means Clustering

Encoder

Decoder

x

Find closest

i

Communication
line

i

x' =mi

mi

mi

.
.
.

.
.
.

Figure 7.1 Given x, the encoder sends the index of the closest code word and
the decoder generates the code word with the received index as x . Error is
x − x 2.

where
(7.4)

k-means clustering

(7.5)

t
bi =

1
0

if x t − m i = minj x t − m j
otherwise

The best reference vectors are those that minimize the total reconstruct
tion error. bi also depend on m i , and we cannot solve this optimization
problem analytically. We have an iterative procedure named k-means
clustering for this: First, we start with some m i initialized randomly.
t
Then at each iteration, we ﬁrst use equation 7.4 and calculate bi for all
t
x t , which are the estimated labels; if bi is 1, we say that x t belongs to the
group of m i . Then, once we have these labels, we minimize equation 7.3.
Taking its derivative with respect to m i and setting it to 0, we get
mi =

t

t
bi x t
t
t bi

The reference vector is set to the mean of all the instances that it represents. Note that this is the same as the formula for the mean in equat
tion 7.2, except that we place the estimated labels bi in place of the labels
t
ri . This is an iterative procedure because once we calculate the new m i ,
t
bi change and need to be recalculated, which in turn aﬀect m i . These two
steps are repeated until m i stabilize (see ﬁgure 7.2). The pseudocode of
the k-means algorithm is given in ﬁgure 7.3.
One disadvantage is that this is a local search procedure, and the ﬁnal m i highly depend on the initial m i . There are various methods for
initialization:
One can simply take randomly selected k instances as the initial m i .

148

7

Clustering

After 1 iteration

k−means: Initial

10

10

0

0

x

x

2

20

2

20

−10

−10

−20

−20

−30
−40

−20

0
x1

20

−30
−40

40

−20

After 2 iterations

0
x1

20

40

20

40

After 3 iterations

10

10

0

0

x

x

2

20

2

20

−10

−10

−20

−20

−30
−40

−20

0
x

1

20

40

−30
−40

−20

0
x

1

Figure 7.2 Evolution of k-means. Crosses indicate center positions. Data points
are marked depending on the closest center.

The mean of all data can be calculated and small random vectors may
be added to the mean to get the k initial m i .
One can calculate the principal component, divide its range into k
equal intervals, partitioning the data into k groups, and then take the
means of these groups as the initial centers.

leader cluster
algorithm

After convergence, all the centers should cover some subset of the data
instances and be useful; therefore, it is best to initialize centers where we
believe there is data.
There are also algorithms for adding new centers incrementally or deleting empty ones. In leader cluster algorithm, an instance that is far away
from existing centers (deﬁned by a threshold value) causes the creation
of a new center at that point (we discuss such a neural network algorithm,

7.4 Expectation-Maximization Algorithm

149

Initialize m i , i = 1, . . . , k, for example, to k random x t
Repeat
For all x t ∈ X
1 if x t − m i = minj xt − m j
t
bi ←
0 otherwise
For all m i , i = 1, . . . , k
t
t
m i ← t bi x t / t bi
Until m i converge
Figure 7.3 k-means algorithm.

ART, in chapter 12). Or, a center that covers a large number of instances
t
( t bi /N > θ) can be split into two (by adding a small random vector to
one of the two copies to make them diﬀerent). Similarly, a center that
covers too few instances can be removed and restarted from some other
part of the input space.
k-means algorithm is for clustering, that is, for ﬁnding groups in the
data, where the groups are represented by their centers, which are the
typical representatives of the groups. Vector quantization is one application of clustering, but clustering is also used for preprocessing before a
t
later stage of classiﬁcation or regression. Given x t , when we calculate bi ,
we do a mapping from the original space to the k-dimensional space, that
is, to one of the corners of the k-dimensional hypercube. A regression or
discriminant function can then be learned in this new space; we discuss
such methods in chapter 12.

7.4

Expectation-Maximization Algorithm
In k-means, we approached clustering as the problem of ﬁnding codebook
vectors that minimize the total reconstruction error. In this section, our
approach is probabilistic and we look for the component density parameters that maximize the likelihood of the sample. Using the mixture model
of equation 7.1, the log likelihood given the sample X = {x t }t is
L(Φ|X)

=

p(xt |Φ)

log
t

k

(7.6)

=

p(x t |Gi )P (Gi )

log
t

i=1

150

7

ExpectationMaximization

Clustering

where Φ includes the priors P (Gi ) and also the suﬃcient statistics of the
component densities p(x t |Gi ). Unfortunately, we cannot solve for the
parameters analytically and need to resort to iterative optimization.
The Expectation-Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977; Redner and Walker 1984) is used in maximum likelihood estimation where the problem involves two sets of random variables of which
one, X, is observable and the other, Z, is hidden. The goal of the algorithm is to ﬁnd the parameter vector Φ that maximizes the likelihood of
the observed values of X, L(Φ|X). But in cases where this is not feasible, we associate the extra hidden variables Z and express the underlying
model using both, to maximize the likelihood of the joint distribution of
X and Z, the complete likelihood Lc (Φ|X, Z).
Since the Z values are not observed, we cannot work directly with the
complete data likelihood Lc ; instead, we work with its expectation, Q,
given X and the current parameter values Φl , where l indexes iteration.
This is the expectation (E) step of the algorithm. Then in the maximization
(M) step, we look for the new parameter values, Φl+1 , that maximize this.
Thus
E-step

:

Q(Φ|Φl ) = E[Lc (Φ|X, Z)|X, Φl ]

M-step

:

Φl+1 = arg max Q(Φ|Φl )
Φ

Dempster, Laird, and Rubin (1977) proved that an increase in Q implies
an increase in the incomplete likelihood
L(Φl+1 |X) ≥ L(Φl |X)
In the case of mixtures, the hidden variables are the sources of observations, namely, which observation belongs to which component. If
these were given, for example, as class labels in a supervised setting, we
would know which parameters to adjust to ﬁt that data point. The EM
algorithm works as follows: in the E-step we estimate these labels given
our current knowledge of components, and in the M-step we update our
component knowledge given the labels estimated in the E-step. These two
t
steps are the same as the two steps of k-means; calculation of bi (E-step)
and reestimation of m i (M-step).
t
t
We deﬁne a vector of indicator variables z t = {z1 , . . . , zk } where zit = 1
t
if x belongs to cluster Gi , and 0 otherwise. z is a multinomial distribu-

151

7.4 Expectation-Maximization Algorithm

tion from k categories with prior probabilities πi , shorthand for P (Gi ).
Then
k

(7.7)

zt

P (z t ) =

πi i
i=1

The likelihood of an observation x t is equal to its probability speciﬁed by
the component that generated it:
k

(7.8)

t

p(xt |z t ) =

pi (x t )zi
i=1

pi (x t ) is shorthand for p(x t |Gi ). The joint density is
p(xt , z t ) = P (z t )p(xt |z t )
and the complete data likelihood of the iid sample X is
Lc (Φ|X, Z)

=

p(x t , z t |Φ)

log
t

log p(x t , z t |Φ)

=
t

log P (z t |Φ) + log p(x t |z t , Φ)

=
t

zit [log πi + log pi (x t |Φ)]

=
t

i

E-step: We deﬁne
Q(Φ|Φl )

≡

E log P (X, Z)|X, Φl

=

E Lc (Φ|X, Z)|X, Φl )
E[zit |X, Φl ][log πi + log pi (x t |Φl )]

=
t

i

where
E[zit |X, Φl ]

=

E[zit |x t , Φl ]

=

P (zit = 1|x t , Φl )

=

p(x t |zit = 1, Φl )P (zit = 1|Φl )
p(xt |Φl )

=

x t are iid

pi (x t |Φl )πi
t
l
j pj (x |Φ )πj

zit is a 0/1 random variable
Bayes’ rule

152

7

p(x t |Gi , Φl )P (Gi )
t
l
j p(x |Gj , Φ )P (Gj )

=

P (Gi |x t , Φl ) ≡ ht
i

=

(7.9)

Clustering

We see that the expected value of the hidden variable, E[zit ], is the
posterior probability that x t is generated by component Gi . Because this
is a probability, it is between 0 and 1 and is a “soft” label, as opposed to
the 0/1 “hard” label of k-means.
M-step: We maximize Q to get the next set of parameter values Φl+1 :
Φl+1 = arg max Q(Φ|Φl )
Φ

which is
Q(Φ|Φl )

ht [log πi + log pi (x t |Φl )]
i

=
t

i

t

i

ht log πi +
i

=

(7.10)

ht log pi (x t |Φl )
i
t

i

The second term is independent of πi and using the constraint that
i πi = 1 as the Lagrangian, we solve for
⎛
⎞
ht log πi − λ ⎝
i

∇πi
t

i

π i − 1⎠ = 0
i

and get
(7.11)

πi =

ht
i
N
t

which is analogous to the calculation of priors in equation 7.2.
Similarly, the ﬁrst term of equation 7.10 is independent of the components and can be dropped while estimating the parameters of the components. We solve for
(7.12)

ht log pi (x t |Φ) = 0
i

∇Φ
t

i

ˆ
If we assume Gaussian components, pi (x t |Φ) ∼ N (m i , Si ), the M-step
is
(7.13)

m l+1
i

=

t

Sl+1
i

=

t

ht x t
i
ht
t i
ht (x t − m l+1 )(x t − m l+1 )T
i
i
i
t
t hi

153

7.4 Expectation-Maximization Algorithm

EM solution
20

15

10

5

x2

0

−5

−10

−15

−20

−25

−30
−40

−30

−20

−10
x1

0

10

20

Figure 7.4 Data points and the ﬁtted Gaussians by EM, initialized by one kmeans iteration of ﬁgure 7.2. Unlike in k-means, as can be seen, EM allows
estimating the covariance matrices. The data points labeled by greater hi , the
contours of the estimated Gaussian densities, and the separating curve of hi =
0.5 (dashed line) are shown.

where, for Gaussian components in the E-step, we calculate
(7.14)

ht =
i

πi |Si |−1/2 exp[−(1/2)(x t − m i )T S−1 (x t − m i )]
i
j

πj |Sj |−1/2 exp[−(1/2)(x t − m j )T S−1 (x t − m j )]
j

Again, the similarity between equations 7.13 and 7.2 is not accidental;
the estimated soft labels ht replace the actual (unknown) labels rit .
i
EM is initalized by k-means. After a few iterations of k-means, we get
the estimates for the centers m i and using the instances covered by each
t
center, we estimate the Si and t bi /N give us the πi . We run EM from
that point on, as shown in ﬁgure 7.4.
Just as in parametric classiﬁcation (section 5.5), with small samples and
large dimensionality we can regularize by making simplifying assumpˆ
tions. When pi (x t |Φ) ∼ N (m i , S), the case of a shared covariance matrix,

154

7

Clustering

equation 7.12 reduces to
(7.15)

min
m i ,S

ht (x t − m i )T S−1 (x t − m i )
i
t

i

ˆ
When pi (x t |Φ) ∼ N (m i , s 2 I), the case of a shared diagonal matrix, we
have
(7.16)

min
m i ,s

ht
i
t

i

xt − mi
s2

2

which is the reconstruction error we deﬁned in k-means clustering (equation 7.3). The diﬀerence is that now
(7.17)

ht =
i

exp −(1/2s 2 ) xt − m i
j

2

exp −(1/2s 2 ) x t − m j

2

t
is a probability between 0 and 1. bi of k-means clustering makes a hard
t
0/1 decision, whereas hi is a soft label that assigns the input to a cluster
t
with a certain probability. When ht are used instead of bi , an instance
i
contributes to the update of parameters of all components, to each proportional to that probability. This is especially useful if the instance is
close to the midpoint between two centers.
We thus see that k-means clustering is a special case of EM applied
to Gaussian mixtures where inputs are assumed independent with equal
and shared variances, all components have equal priors, and labels are
hardened. k-means thus pave the input density with circles, whereas EM
in the general case uses ellipses of arbitrary shapes, orientations, and
coverage proportions.

7.5

Mixtures of Latent Variable Models
When full covariance matrices are used with Gaussian mixtures, even if
there is no singularity, one risks overﬁtting if the input dimensionality
is high and the sample is small. To decrease the number of parameters,
assuming a common covariance matrix may not be right since clusters
may really have diﬀerent shapes. Assuming diagonal matrices is even
more risky because it removes all correlations.
The alternative is to do dimensionality reduction in the clusters. This
decreases the number of parameters while still capturing the correlations. The number of free parameters is controlled through the dimensionality of the reduced space.

7.6 Supervised Learning after Clustering

155

When we do factor analysis (section 6.4) in the clusters, we look for
latent or hidden variables or factors that generate the data in the clusters
(Bishop 1999):
(7.18)

mixtures of factor
analyzers

mixtures of
probabilistic
principal
component
analyzers

7.6

customer
segmentation

customer
relationship
management

p(xt |Gi ) ∼ N (m i , Vi VT + Ψ i )
i
where Vi and Ψ i are the factor loadings and speciﬁc variances of cluster
Gi . Rubin and Thayer (1982) give EM equations for factor analysis. It
is possible to extend this in mixture models to ﬁnd mixtures of factor
analyzers (Ghahramani and Hinton 1997). In the E-step, in equation 7.9,
we use equation 7.18, and in the M-step, we solve equation 7.12 for Vi
and Ψ i instead of Si . Similarly, one can also do PCA in groups, which
is called mixtures of probabilistic principal component analyzers (Tipping
and Bishop 1999).
We can of course use EM to learn Si and then do FA or PCA separately
in each cluster, but doing EM is better because it couples these two steps
of clustering and dimensionality reduction and does a soft partitioning.
An instance contributes to the calculation of the latent variables of all
groups, weighted by ht .
i

Supervised Learning after Clustering
Clustering, like the dimensionality reduction methods discussed in chapter 6, can be used for two purposes: it can be used for data exploration,
to understand the structure of data. Dimensionality reduction methods
are used to ﬁnd correlations between variables and thus group variables;
clustering methods, on the other hand, are used to ﬁnd similarities between instances and thus group instances.
If such groups are found, these may be named (by application experts)
and their attributes be deﬁned. One can choose the group mean as the
representative prototype of instances in the group, or the possible range
of attributes can be written. This allows a simpler description of the
data. For example, if the customers of a company seem to fall in one
of k groups, called segments, customers being deﬁned in terms of their
demographic attributes and transactions with the company, then a better
understanding of the customer base will be provided that will allow the
company to provide diﬀerent strategies for diﬀerent types of customers;
this is part of customer relationship management (CRM). Likewise, the
company will also be able to develop strategies for those customers who

156

7

distributed vs.
local
representation

mixture of mixtures

Clustering

do not fall in any large group, and who may require attention, for example, churning customers.
Frequently, clustering is also used as a preprocessing stage. Just like
the dimensionality reduction methods of chapter 6 allowed us to make
a mapping to a new space, after clustering, we also map to a new kdimensional space where the dimensions are hi (or bi at the risk of loss of
information). In a supervised setting, we can then learn the discriminant
or regression function in this new space. The diﬀerence from dimensionality reduction methods like PCA however is that k, the dimensionality of
the new space, can be larger than d, the original dimensionality.
When we use a method like PCA, where the new dimensions are combinations of the original dimensions, to represent any instance in the new
space, all dimensions contribute; that is, all zj are nonzero. In the case of
a method like clustering where the new dimensions are deﬁned locally,
there are many more new dimensions, bj , but only one (or if we use hj ,
few) of them have a nonzero value. In the former case, where there are
few dimensions but all contribute, we have a distributed representation;
in the latter case, where there are many dimensions but few contribute,
we have a local representation.
One advantage of preceding a supervised learner with unsupervised
clustering or dimensionality reduction is that the latter does not need
labeled data. Labeling the data is costly. We can use a large amount of
unlabeled data for learning the cluster parameters and then use a smaller
labeled data to learn the second stage of classiﬁcation or regression. Unsupervised learning is called “learning what normally happens” (Barrow
1989). When followed by a supervised learner, we ﬁrst learn what normally happens and then learn what that means. We discuss such methods
in chapter 12.
In the case of classiﬁcation, when each class is a mixture model composed of a number of components, the whole density is a mixture of
mixtures:
ki

p(x|Ci )

=

p(x|Gij )P (Gij )
j=1
K

p(x)

=

p(x|Ci )P (Ci )
i=1

where ki is the number of components making up p(x|Ci ) and Gij is the
component j of class i. Note that diﬀerent classes may need diﬀerent

7.7 Hierarchical Clustering

157

number of components. Learning the parameters of components is done
separately for each class (probably after some regularization) as we discussed previously. This is better than ﬁtting many components to data
from all classes and then labeling them later with classes.

7.7

hierarchical
clustering

Hierarchical Clustering
We discussed clustering from a probabilistic point of view as ﬁtting a
mixture model to the data, or in terms of ﬁnding code words minimizing
reconstruction error. There are also methods for clustering that only use
similarities of instances, without any other requirement on the data; the
aim is to ﬁnd groups such that instances in a group are more similar to
each other than instances in diﬀerent groups. This is the approach taken
by hierarchical clustering.
This needs the use of a similarity, or equivalently a distance, measure
deﬁned between instances. Generally Euclidean distance is used, where
one has to make sure that all attributes have the same scale. This is a
special case of the Minkowksi distance with p = 2:
⎤1/p
⎡
dm (x r , x s ) = ⎣

d

(xr − xs )p ⎦
j
j

j=1

City-block distance is easier to calculate:
d

|xr − xs |
j
j

dcb (x r , x s ) =
j=1

agglomerative
clustering
divisive clustering

single-link
clustering

(7.19)

An agglomerative clustering algorithm starts with N groups, each initially containing one training instance, merging similar groups to form
larger groups, until there is a single one. A divisive clustering algorithm
goes in the other direction, starting with a single group and dividing large
groups into smaller groups, until each group contains a single instance.
At each iteration of an agglomerative algorithm, we choose the two
closest groups to merge. In single-link clustering, this distance is deﬁned
as the smallest distance between all possible pair of elements of the two
groups:
d(Gi , Gj ) =

min
d(x r , x s )
x r ∈Gi ,xs ∈Gj

Consider a weighted, completely connected graph with nodes corresponding to instances and edges between nodes with weights equal to

158

7

complete-link
clustering

(7.20)

dendrogram

7.8

Clustering

the distances between the instances. Then the single-link method corresponds to constructing the minimal spanning tree of this graph.
In complete-link clustering, the distance between two groups is taken as
the largest distance between all possible pairs:
d(Gi , Gj ) =

max
d(x r , x s )
x r ∈Gi ,xs ∈Gj

These are the two most frequently used measures to choose the two
closest groups to merge. Other possibilities are the average-link method
that uses the average of distances between all pairs and the centroid distance that measures the distance between the centroids (means) of the
two groups.
Once an agglomerative method is run, the result is generally drawn as
a hierarchical structure known as the dendrogram. This is a tree where
leaves correspond to instances, which are grouped in the order in which
they are merged. An example is given in ﬁgure 7.5. The tree can be then
intersected at any level to get the wanted number of groups.
Single-link and complete-link methods calculate the distance between
groups diﬀerently that aﬀect the clusters and the dendrogram. In the
single-link method, two instances are grouped together at level h if the
distance between them is less than h, or if there is an intermediate sequence of instances between them such that the distance between consecutive instances is less than h. On the other hand, in the complete-link
method, all instances in a group have a distance less than h between
them. Single-link clusters may be elongated due to this “chaining” eﬀect.
(In ﬁgure 7.5, what if there were an instance halfway between e and c?)
Complete-link clusters tend to be more compact.

Choosing the Number of Clusters
Like any learning method, clustering also has its knob to adjust complexity; it is k, the number of clusters. Given any k, clustering will always ﬁnd
k centers, whether they really are meaningful groups, or whether they
are imposed by the method we use. There are various ways we can use to
ﬁne-tune k:
In some applications such as color quantization, k is deﬁned by the
application.

159

7.8 Choosing the Number of Clusters

b
a

3
c
e
d

f

2
h
1

a

b

e

c

d

f

Figure 7.5 A two-dimensional dataset and the dendrogram showing the result
of single-link clustering is shown. Note that leaves of the tree are ordered so
that no branches cross. The tree is then intersected at a desired value of h to get
the clusters.

Plotting the data in two dimensions using PCA may be used in uncovering the structure of data and the number of clusters in the data.
An incremental approach may also help: setting a maximum allowed
distance is equivalent to setting a maximum allowed reconstruction
error per instance.
In some applications, validation of the groups can be done manually
by checking whether clusters actually code meaningful groups of the
data. For example, in a data mining application, application experts
may do this check. In color quantization, we may inspect the image
visually to check its quality (despite the fact that our eyes and brain
do not analyze an image pixel by pixel).
Depending on what type of clustering method we use, we can plot the
reconstruction error or log likelihood as a function of k and look for the
“elbow.” After a large enough k, the algorithm will start dividing groups,
in which case there will not be a large decrease in the reconstruction error
or large increase in the log likelihood. Similarly, in hierarchical clustering,
by looking at the diﬀerences between levels in the tree, we can decide on
a good split.

160

7

7.9

fuzzy k-means

7.10

Clustering

Notes
Mixture models are frequently used in statistics. Dedicated textbooks are
those by Titterington, Smith, and Makov (1985) and McLachlan and Basford (1988). McLachlan and Krishnan (1997) discuss recent developments
in the EM algorithm, how its convergence can be accelerated, and various variants. In signal processing, k-means is called the Linde-Buzo-Gray
(LBG) algorithm (Gersho and Gray 1992). It is frequently used in both
statistics and signal processing in a large variety of applications and has
many variants, one of which is fuzzy k-means. The fuzzy membership of
an input to a component is also a number between 0 and 1 (Bezdek and
Pal 1995). Alpaydın (1998) compares k-means, fuzzy k-means, and EM on
Gaussian mixtures. A comparison of EM and other learning algorithms
for the learning of Gaussian mixture models is given by Xu and Jordan
(1996). On small data samples, an alternative to simplifying assumptions
is to use a Bayesian approach (Ormoneit and Tresp 1996). Moerland
(1999) compares mixtures of Gaussians and mixtures of latent variable
models on a set of classiﬁcation problems, showing the advantage of latent variable models empirically. A book on clustering methods is by Jain
and Dubes (1988) and survey articles are by Jain, Murty, and Flynn (1999)
and Xu and Wunsch (2005).

Exercises
1. In image compression, k-means can be used as follows: The image is divided
into nonoverlapping c ×c windows and these c 2 -dimensional vectors make up
the sample. For a given k, which is generally a power of two, we do k-means
clustering. The reference vectors and the indices for each window is sent over
the communication line. At the receiving end, the image is then reconstructed
by reading from the table of reference vectors using the indices. Write the
computer program that does this for diﬀerent values of k and c. For each
case, calculate the reconstruction error and the compression rate.
2. We can do k-means clustering, partition the instances, and then calculate Si
separately in each group. Why is this not a good idea?
3. Derive the M-step equations for S in the case of shared arbitrary covariance
matrix S (equation 7.15) and s 2 , in the case of shared diagonal covariance
matrix (equation 7.16).
4. Deﬁne a multivariate Bernoulli mixture where inputs are binary and derive
the EM equations.

7.11 References

161

5. In the mixture of mixtures approach for classiﬁcation, how can we ﬁne-tune
ki , the number of components for class Ci ?
6. How can we do hierarchical clustering with binary input vectors, for example,
for text clustering using the bag-of-words representation?
7. What are the similarities and diﬀerences between average-link clustering and
k-means?
8. In hierarchical clustering, how can we have locally adaptive distances? What
are the advantages and disadvantages of this?
9. How can we make k-means robust to outliers?
10. Having generated a dendrogram, can we “prune” it?

7.11

References
Alpaydın, E. 1998. “Soft Vector Quantization and the EM Algorithm.” Neural
Networks 11: 467–477.
Barrow, H. B. 1989. “Unsupervised Learning.” Neural Computation 1: 295–311.
Bezdek, J. C., and N. R. Pal. 1995. “Two Soft Relatives of Learning Vector
Quantization.” Neural Networks 8: 729–743.
Bishop, C. M. 1999. “Latent Variable Models.” In Learning in Graphical Models,
ed. M. I. Jordan, 371–403. Cambridge, MA: MIT Press.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood from
Incomplete Data via the EM Algorithm.” Journal of Royal Statistical Society B
39: 1–38.
Gersho, A., and R. M. Gray. 1992. Vector Quantization and Signal Compression.
Boston: Kluwer.
Ghahramani, Z., and G. E. Hinton. 1997. The EM Algorithm for Mixtures of Factor
Analyzers. Technical Report CRG TR-96-1, Department of Computer Science,
University of Toronto (revised Feb. 1997).
Jain, A. K., and R. C. Dubes. 1988. Algorithms for Clustering Data. New York:
Prentice Hall.
Jain, A. K., M. N. Murty, and P. J. Flynn. 1999. “Data Clustering: A Review.” ACM
Computing Surveys 31: 264–323.
McLachlan, G. J., and K. E. Basford. 1988. Mixture Models: Inference and Applications to Clustering. New York: Marcel Dekker.
McLachlan, G. J., and T. Krishnan. 1997. The EM Algorithm and Extensions. New
York: Wiley.

162

7

Clustering

Moerland, P. 1999. “A Comparison of Mixture Models for Density Estimation.”
In International Conference on Artiﬁcial Neural Networks, ed. D. Willshaw,
and A. Murray, 25–30. London, UK: IEE Press.
Ormoneit, D., and V. Tresp. 1996. “Improved Gaussian Mixture Density Estimates using Bayesian Penalty Terms and Network Averaging.” In Advances in
Neural Information Processing Systems 8, ed. D. S. Touretzky, M. C. Mozer,
and M. E. Hasselmo, 542–548. Cambridge, MA: MIT Press.
Redner, R. A., and H. F. Walker. 1984. “Mixture Densities, Maximum Likelihood
and the EM Algorithm.” SIAM Review 26: 195–239.
Rubin, D. B., and D. T. Thayer. 1982. “EM Algorithms for ML Factor Analysis.”
Psychometrika 47: 69–76.
Tipping, M. E., and C. M. Bishop. 1999. “Mixtures of Probabilistic Principal
Component Analyzers.” Neural Computation 11: 443–482.
Titterington, D. M., A. F. M. Smith, and E. E. Makov. 1985. Statistical Analysis of
Finite Mixture Distributions. New York: Wiley.
Xu, L., and M. I. Jordan. 1996. “On Convergence Properties of the EM Algorithm
for Gaussian Mixtures.” Neural Computation 8: 129–151.
Xu, R., and D. Wunsch II. 2005. “Survey of Clustering Algorithms.” IEEE Transactions on Neural Networks 16: 645–678.

8

Nonparametric Methods

In the previous chapters, we discussed the parametric and semiparametric approaches where we assumed that the data is drawn from
one or a mixture of probability distributions of known form. Now, we
are going to discuss the nonparametric approach that is used when
no such assumption can be made about the input density and the
data speaks for itself. We consider the nonparametric approaches
for density estimation, classiﬁcation, and regression and see how the
time and space complexity can be checked.

8.1

nonparametric
estimation

Introduction
I n p a r a m e tr i c methods, whether for density estimation, classiﬁcation, or regression, we assume a model valid over the whole input space.
In regression, for example, when we assume a linear model, we assume
that for any input, the output is the same linear function of the input.
In classiﬁcation when we assume a normal density, we assume that all
examples of the class are drawn from this same density. The advantage
of a parametric method is that it reduces the problem of estimating a
probability density function, discriminant, or regression function to estimating the values of a small number of parameters. Its disadvantage is
that this assumption does not always hold and we may incur a large error
if it does not.
If we cannot make such assumptions and cannot come up with a parametric model, one possibility is to use a semiparametric mixture model
as we saw in chapter 7 where the density is written as a disjunction of a
small number of parametric models.
In nonparametric estimation, all we assume is that similar inputs have

164

8

instance-based
memory-based
learning

Nonparametric Methods

similar outputs. This is a reasonable assumption: the world is smooth
and functions, whether they are densities, discriminants, or regression
functions, change slowly. Similar instances mean similar things. We all
love our neighbors because they are so much like us.
Therefore, our algorithm is composed of ﬁnding the similar past instances from the training set using a suitable distance measure and interpolating from them to ﬁnd the right output. Diﬀerent nonparametric
methods diﬀer in the way they deﬁne similarity or interpolate from the
similar training instances. In a parametric model, all of the training instances aﬀect the ﬁnal global estimate, whereas in the nonparametric
case, there is no single global model; local models are estimated as they
are needed, aﬀected only by the nearby training instances.
Nonparametric methods do not assume any a priori parametric form
for the underlying densities; in a looser interpretation, a nonparametric
model is not ﬁxed but its complexity depends on the size of the training
set, or rather, the complexity of the problem inherent in the data.
In machine learning literature, nonparametric methods are also called
instance-based or memory-based learning algorithms, since what they do
is store the training instances in a lookup table and interpolate from
these. This implies that all of the training instances should be stored
and storing all requires memory of O(N). Furthermore, given an input,
similar ones should be found, and ﬁnding them requires computation of
O(N). Such methods are also called lazy learning algorithms, because
unlike the eager parametric models, they do not compute a model when
they are given the training set but postpone the computation of the model
until they are given a test instance. In the case of a parametric approach,
the model is quite simple and has a small number of parameters, of order O(d), or O(d 2 ), and once these parameters are calculated from the
training set, we keep the model and no longer need the training set to
calculate the output. N is generally much larger than d (or d 2 ), and this
increased need for memory and computation is the disadvantage of the
nonparametric methods.
We start by estimating a density function, and discuss its use in classiﬁcation. We then generalize the approach to regression.

8.2 Nonparametric Density Estimation

8.2

165

Nonparametric Density Estimation
As usual in density estimation, we assume that the sample X = {xt }N is
t=1
ˆ
drawn independently from some unknown probability density p(·). p(·)
t
is our estimator of p(·). We start with the univariate case where x are
scalars and later generalize to the multidimensional case.
The nonparametric estimator for the cumulative distribution function,
F (x), at point x is the proportion of sample points that are less than or
equal to x

(8.1)

ˆ
F(x) =

#{xt ≤ x}
N

where #{xt ≤ x} denotes the number of training instances whose xt is
less than or equal to x. Similarly, the nonparametric estimate for the
density function can be calculated as
(8.2)

ˆ
p(x) =

1
h

#{xt ≤ x + h} − #{xt ≤ x}
N

h is the length of the interval and instances xt that fall in this interval are assumed to be “close enough.” The techniques given in this
chapter are variants where diﬀerent heuristics are used to determine the
instances that are close and their eﬀects on the estimate.

8.2.1
histogram

(8.3)

Histogram Estimator
The oldest and most popular method is the histogram where the input
space is divided into equal-sized intervals named bins. Given an origin xo
and a bin width h, the bins are the intervals [xo + mh, xo + (m + 1)h) for
positive and negative integers m and the estimate is given as
ˆ
p(x) =

#{xt in the same bin as x}
Nh

In constructing the histogram, we have to choose both an origin and
a bin width. The choice of origin aﬀects the estimate near boundaries
of bins, but it is mainly the bin width that has an eﬀect on the estimate:
with small bins, the estimate is spiky, and with larger bins, the estimate
is smoother (see ﬁgure 8.1). The estimate is 0 if no instance falls in a bin
and there are discontinuities at bin boundaries. Still, one advantage of
the histogram is that once the bin estimates are calculated and stored,
we do not need to retain the training set.

166

8

Nonparametric Methods

Histogram: h = 2
0.4
0.3
0.2
0.1
0

0

1

2

3

4
h=1

5

6

7

8

0

1

2

3

4
h = 0.5

5

6

7

8

0

1

2

3

4

5

6

7

8

0.4
0.3
0.2
0.1
0
0.8
0.6
0.4
0.2
0

Figure 8.1 Histograms for various bin lengths. ‘×’ denote data points.

naive estimator

(8.4)

The naive estimator (Silverman 1986) frees us from setting an origin. It
is deﬁned as
ˆ
p(x) =

#{x − h/2 < xt ≤ x + h/2}
Nh

and is equal to the histogram estimate where x is always at the center of
a bin of size h (see ﬁgure 8.2). The estimator can also be written as
N

(8.5)

ˆ
p(x) =

1
w
Nh t=1

x − xt
h

with the weight function deﬁned as
w (u) =

1
0

if |u| < 1/2
otherwise

This is as if each xt has a symmetric region of inﬂuence of size h around
it and contributes 1 for an x falling in its region. Then the nonparametric estimate is just the sum of inﬂuences of xt whose regions include x.
Because this region of inﬂuence is “hard” (0 or 1), the estimate is not a
continuous function and has jumps at xt ± h/2.

167

8.2 Nonparametric Density Estimation

Naive estimator: h = 2
0.4
0.3
0.2
0.1
0

0

1

2

3

4
h=1

5

6

7

8

0

1

2

3

4
h = 0.5

5

6

7

8

0

1

2

3

4

5

6

7

8

0.4
0.3
0.2
0.1
0
0.8
0.6
0.4
0.2
0

Figure 8.2 Naive estimate for various bin lengths.

8.2.2

kernel function

(8.6)
kernel estimator
Parzen windows

(8.7)

Kernel Estimator
To get a smooth estimate, we use a smooth weight function, called a
kernel function. The most popular is the Gaussian kernel:
K(u) = √

1
u2
exp −
2
2π

The kernel estimator, also called Parzen windows, is deﬁned as
N

ˆ
p(x) =

1
K
Nh t=1

x − xt
h

The kernel function K(·) determines the shape of the inﬂuences and
the window width h determines the width. Just like the naive estimate is
the sum of “boxes,” the kernel estimate is the sum of “bumps.” All the xt
have an eﬀect on the estimate at x, and this eﬀect decreases smoothly as
|x − xt | increases.
To simplify calculation, K(·) can be taken to be 0 if |x − xt | > 3h. There
exist other kernels easier to compute that can be used, as long as K(u) is
maximum for u = 0 and decreasing symmetrically as |u| increases.

168

8

Nonparametric Methods

Kernel estimator: h = 1
0.2
0.15
0.1
0.05
0

0

1

2

3

4
h = 0.5

5

6

7

8

0

1

2

3

4
h = 0.25

5

6

7

8

0

1

2

3

4

5

6

7

8

0.4
0.3
0.2
0.1
0
0.8
0.6
0.4
0.2
0

Figure 8.3 Kernel estimate for various bin lengths.

When h is small, each training instance has a large eﬀect in a small
region and no eﬀect on distant points. When h is larger, there is more
overlap of the kernels and we get a smoother estimate (see ﬁgure 8.3).
If K(·) is everywhere nonnegative and integrates to 1, namely, if it is a
ˆ
ˆ
legitimate density function, so will p(·) be. Furthermore, p(·) will inherit
all the continuity and diﬀerentiability properties of the kernel K(·), so
ˆ
that, for example, if K(·) is Gaussian, then p(·) will be smooth having all
the derivatives.
One problem is that the window width is ﬁxed across the entire input
space. Various adaptive methods have been proposed to tailor h as a
function of the density around x.

8.2.3

k-Nearest Neighbor Estimator
The nearest neighbor class of estimators adapts the amount of smoothing
to the local density of data. The degree of smoothing is controlled by k,
the number of neighbors taken into account, which is much smaller than

169

8.2 Nonparametric Density Estimation

k−nn estimator: k = 5
0.4
0.3
0.2
0.1
0

0

1

2

3

4
k=3

5

6

7

8

0

1

2

3

4
k=1

5

6

7

8

0

1

2

3

4

5

6

7

8

1

0.5

0
1

0.5

0

Figure 8.4 k-nearest neighbor estimate for various k values.

N, the sample size. Let us deﬁne a distance between a and b, for example,
|a − b|, and for each x, we deﬁne
d1 (x) ≤ d2 (x) ≤ · · · ≤ dN (x)

k-nearest neighbor
estimate

(8.8)

to be the distances arranged in ascending order, from x to the points
in the sample: d1 (x) is the distance to the nearest sample, d2 (x) is the
distance to the next nearest, and so on. If xt are the data points, then we
deﬁne d1 (x) = mint |x − xt |, and if i is the index of the closest sample,
namely, i = arg mint |x − xt |, then d2 (x) = minj=i |x − xj |, and so forth.
The k-nearest neighbor (k-nn) density estimate is
ˆ
p(x) =

k
2Ndk (x)

This is like a naive estimator with h = 2dk (x), the diﬀerence being that
instead of ﬁxing h and checking how many samples fall in the bin, we ﬁx
k, the number of observations to fall in the bin, and compute the bin size.
Where density is high, bins are small, and where density is low, bins are
larger (see ﬁgure 8.4).

170

8

Nonparametric Methods

The k-nn estimator is not continuous; its derivative has a discontinuity
at all 1 (x(j) + x(j+k) ) where x(j) are the order statistics of the sample. The
2
k-nn is not a probability density function since it integrates to ∞, not 1.
To get a smoother estimate, we can use a kernel function whose eﬀect
decreases with increasing distance
N

(8.9)

ˆ
p(x) =

1
K
Ndk (x) t=1

x − xt
dk (x)

This is like a kernel estimator with adaptive smoothing parameter h =
dk (x). K(·) is typically taken to be the Gaussian kernel.

8.3

Generalization to Multivariate Data
Given a sample of d-dimensional observations X = {x t }N , the multivarit=1
ate kernel density estimator is

(8.10)

ˆ
p(x) =

1
Nhd

N

K
t=1

x − xt
h

with the requirement that

d

K(x)dx = 1

The obvious candidate is the multivariate Gaussian kernel:
(8.11)

curse of
dimensionality

K(u) =

√

1
2π

d

exp −

u
2

2

However, care should be applied to using nonparametric estimates in
high-dimensional spaces because of the curse of dimensionality: Let us
say x is eight-dimensional, and we use a histogram with ten bins per
dimension, then there are 108 bins, and unless we have lots of data, most
of these bins will be empty and the estimates in there will be 0. In high
dimensions, the concept of “close” also becomes blurry so one should be
careful in choosing h.
For example, the use of the Euclidean norm in equation 8.11 implies
that the kernel is scaled equally on all dimensions. If the inputs are on
diﬀerent scales, they should be normalized to have the same variance.
Still, this does not take correlations into account and better results are

171

8.4 Nonparametric Classiﬁcation

achieved when the kernel has the same form as the underlying distribution
(8.12)

Hamming distance

K(u) =

1
(2π )d/2 |S|1/2

1
exp − u T S−1 u
2

where S is the sample covariance matrix. This corresponds to using Mahalanobis distance instead of the Euclidean distance.
It is also possible to have the distance metric local where S is calculated
from instances in the vicinity of x, for example, some k closest instances.
Note that S calculated locally may be singular and PCA (or LDA, in the
case of classiﬁcation) may be needed.
When the inputs are discrete, we can use Hamming distance, which
counts the number of nonmatching attributes
d

(8.13)

1(xj = xt )
j

HD(x, x t ) =
j=1

where
1(xj = xt ) =
j

1
0

if xj = xt
j
otherwise

HD(x, x t ) is then used in place of x − x t or (x − x t )T S−1 (x − x t ) for
kernel estimation or for ﬁnding the k closest neighbors.

8.4

Nonparametric Classiﬁcation
When used for classiﬁcation, we use the nonparametric approach to estimate the class-conditional densities, p(x|Ci ). The kernel estimator of the
class-conditional density is given as

(8.14)

ˆ
p(x|Ci ) =

1
Ni hd

N

K
t=1

x − xt
h

rit

where rit is 1 if x t ∈ Ci and 0 otherwise. Ni is the number of labeled
instances belonging to Ci : Ni = t rit . The MLE of the prior density is
ˆ
P (Ci ) = Ni /N. Then, the discriminant can be written as
gi (x)
(8.15)

=

ˆ
ˆ
p(x|Ci )P (Ci )

=

1
Nhd

N

K
t=1

x − xt
h

rit

172

8

Nonparametric Methods

and x is assigned to the class for which the discriminant takes its maximum. The common factor 1/(Nhd ) can be ignored. So each training
instance votes for its class and has no eﬀect on other classes; the weight
of vote is given by the kernel function K(·), typically giving more weight
to closer instances.
For the special case of k-nn estimator, we have
ki
Ni V k (x)
where ki is the number of neighbors out of the k nearest that belong to
Ci and V k (x) is the volume of the d-dimensional hypersphere centered
at x, with radius r = x − x (k) where x (k) is the k-th nearest observation
to x (among all neighbors from all classes of x): V k = r d cd with cd as
the volume of the unit sphere in d dimensions, for example, c1 = 2, c2 =
π , c3 = 4π /3, and so forth. Then

(8.16)

ˆ
p(x|Ci ) =

(8.17)

ˆ
P (Ci |x) =

k-nn classifier

discriminant
adaptive nearest
neighbor
nearest neighbor
classifier
Voronoi
tesselation

8.5

ˆ
ˆ
p(x|Ci )P (Ci )
ki
=
ˆ
k
p(x)

The k-nn classiﬁer assigns the input to the class having most examples
among the k neighbors of the input. All neighbors have equal vote, and
the class having the maximum number of voters among the k neighbors
is chosen. Ties are broken arbitrarily or a weighted vote is taken. k
is generally taken to be an odd number to minimize ties: confusion is
generally between two neighboring classes.
Again, the use of Euclidean distance corresponds to assuming uncorrelated inputs with equal variances, and when this is not the case a suitable
metric should be used. One example is discriminant adaptive nearest
neighbor (Hastie and Tibshirani 1996) where the optimal distance to separate classes is estimated locally.
A special case of k-nn is the nearest neighbor classiﬁer where k = 1 and
the input is assigned to the class of the nearest pattern. This divides the
space in the form of a Voronoi tesselation (see ﬁgure 8.5).

Condensed Nearest Neighbor
Time and space complexity of nonparametric methods are proportional
to the size of the training set, and condensing methods have been proposed to decrease the number of stored instances without degrading performance. The idea is to select the smallest subset Z of X such that when
Z is used in place of X, error does not increase (Dasarathy 1991).

173

x2

8.5 Condensed Nearest Neighbor

*
*

x1
Figure 8.5 Dotted lines are the Voronoi tesselation and the straight line is the
class discriminant. In condensed nearest neighbor, those instances that do not
participate in deﬁning the discriminant (marked by ‘*’) can be removed without
increasing the training error.

condensed nearest
neighbor

The best-known and earliest method is condensed nearest neighbor
where 1-nn is used as the nonparametric estimator for classiﬁcation (Hart
1968). 1-nn approximates the discriminant in a piecewise linear manner,
and only the instances that deﬁne the discriminant need be kept; an instance inside the class regions need not be stored as its nearest neighbor
is of the same class and its absence does not cause any error (on the
training set) (ﬁgure 8.5). Such a subset is called a consistent subset, and
we would like to ﬁnd the minimal consistent subset.
Hart proposed a greedy algorithm to ﬁnd Z (ﬁgure 8.6). The algorithm
starts with an empty Z and passing over the instances in X one by one in
a random order, checks whether they can be classiﬁed correctly by 1-nn
using the instances already stored in Z. If an instance is misclassiﬁed, it is
added to Z; if it is correctly classiﬁed, Z is unchanged. One should pass
over the training set a few times until no further instances are added.
The algorithm does a local search and depending on the order in which
the training instances are seen, diﬀerent subsets may be found, which
may have diﬀerent accuracies on the validation data. Thus it does not

174

8

Nonparametric Methods

Z←∅
Repeat
For all x ∈ X (in random order)
Find x ∈ Z such that x − x = minx j ∈Z x − x j
If class(x)=class(x ) add x to Z
Until Z does not change
Figure 8.6 Condensed nearest neighbor algorithm.

guarantee ﬁnding the minimal consistent subset, which is known to be
NP-complete (Wilfong 1992).
Condensed nearest neighbor is a greedy algorithm that aims to minimize training error and complexity, measured by the size of the stored
subset. We can write an augmented error function
(8.18)

E (Z|X) = E(X|Z) + λ|Z|
where E(X|Z) is the error on X storing Z. |Z| is the cardinality of Z, and
the second term penalizes complexity. As in any regularization scheme,
λ represents the trade-oﬀ between the error and complexity such that
for small λ, error becomes more important, and as λ gets larger, complex
models are penalized more. Condensed nearest neighbor is one method
to minimize equation 8.18, but other algorithms to optimize it can also
be devised.

8.6

Nonparametric Regression: Smoothing Models
In regression, given the training set X = {xt , r t } where r t ∈

, we assume

r t = g(xt ) +

smoother

In parametric regression, we assume a polynomial of a certain order
and compute its coeﬃcients that minimize the sum of squared error on
the training set. Nonparametric regression is used when no such polynomial can be assumed; we only assume that close x have close g(x)
values. As in nonparametric density estimation, given x, our approach
is to ﬁnd the neighborhood of x and average the r values in the neighˆ
borhood to calculate g (x). The nonparametric regression estimator is
also called a smoother and the estimate is called a smooth (Härdle 1990).

175

8.6 Nonparametric Regression: Smoothing Models

Regressogram smoother: h = 6
4
2
0
−2

0

1

2

3

4
h=3

5

6

7

8

0

1

2

3

4
h=1

5

6

7

8

0

1

2

3

4

5

6

7

8

4
2
0
−2
4
2
0
−2

Figure 8.7 Regressograms for various bin lengths. ‘×’ denote data points.

There are various methods for deﬁning the neighborhood and averaging
in the neighborhood, similar to methods in density estimation. We discuss the methods for the univariate x; they can be generalized to the
multivariate case in a straightforward manner using multivariate kernels,
as in density estimation.

8.6.1

regressogram

(8.19)

Running Mean Smoother
If we deﬁne an origin and a bin width and average the r values in the bin
as in the histogram, we get a regressogram (see ﬁgure 8.7)
ˆ
g (x) =

N
t
t
t=1 b(x, x )r
N
t
t=1 b(x, x )

where
b(x, xt ) =

running mean
smoother

1
0

if xt is the same bin with x
otherwise

Having discontinuities at bin boundaries is disturbing as is the need to
ﬁx an origin. As in the naive estimator, in the running mean smoother,

176

8

Nonparametric Methods

Running mean smoother: h = 6
4
2
0
−2

0

1

2

3

4
h=3

5

6

7

8

0

1

2

3

4
h=1

5

6

7

8

0

1

2

3

4

5

6

7

8

4
2
0
−2
4
2
0
−2

Figure 8.8 Running mean smooth for various bin lengths.

we deﬁne a bin symmetric around x and average in there (ﬁgure 8.8).

(8.20)

ˆ
g(x) =

N
t=1 w
N
t=1 w

x−xt
h
x−xt
h

rt

where
w (u) =

1
0

if |u| < 1
otherwise

This method is especially popular with evenly spaced data, for example,
time series. In applications where there is noise, one can use the median
of the r t in the bin instead of their mean.

8.6.2

kernel smoother

Kernel Smoother
As in the kernel estimator, we can use a kernel giving less weight to further points, and we get the kernel smoother (see ﬁgure 8.9):

177

8.6 Nonparametric Regression: Smoothing Models

Kernel smooth: h = 1
4
2
0
−2

0

1

2

3

4
h = 0.5

5

6

7

8

0

1

2

3

4
h = 0.25

5

6

7

8

0

1

2

3

4

5

6

7

8

4
2
0
−2
4
2
0
−2

Figure 8.9 Kernel smooth for various bin lengths.

(8.21)

ˆ
g (x) =

t

K
t

k-nn smoother

8.6.3

running line
smoother

locally weighted
running line
smoother

K

x−xt
h
x−xt
h

rt

Typically a Gaussian kernel K(·) is used. Instead of ﬁxing h, we can ﬁx
k, the number of neighbors, adapting the estimate to the density around
x, and get the k-nn smoother.

Running Line Smoother
Instead of taking an average and giving a constant ﬁt at a point, we can
take into account one more term in the Taylor expansion and calculate
a linear ﬁt. In the running line smoother, we can use the data points in
the neighborhood, as deﬁned by h or k, and ﬁt a local regression line (see
ﬁgure 8.10).
In the locally weighted running line smoother, known as loess, instead
of a hard deﬁnition of neighborhoods, we use kernel weighting such that
distant points have less eﬀect on error.

178

8

Nonparametric Methods

Running line smooth: h = 6
4
2
0
−2

0

1

2

3

4
h=3

5

6

7

8

0

1

2

3

4
h=1

5

6

7

8

0

1

2

3

4

5

6

7

8

4
2
0
−2
6
4
2
0
−2

Figure 8.10 Running line smooth for various bin lengths.

8.7

smoothing splines

How to Choose the Smoothing Parameter
In nonparametric methods, for density estimation or regression, the critical parameter is the smoothing parameter as used in bin width or kernel
spread h, or the number of neighbors k. The aim is to have an estimate
that is less variable than the data points. As we have discussed previously, one source of variability in the data is noise and the other is the
variability in the unknown underlying function. We should smooth just
enough to get rid of the eﬀect of noise—not less, not more. With too
large h or k, many instances contribute to the estimate at a point and we
also smooth the variability due to the function (oversmoothing); with too
small h or k, single instances have a large eﬀect, we do not even smooth
over the noise (undersmoothing). In other words, small h or k leads to
small bias but large variance. Larger h or k decreases variance but increases bias. Geman, Bienenstock, and Doursat (1992) discuss bias and
variance for nonparametric estimators.
This requirement is explicitly coded in a regularized cost function as
used in smoothing splines

179

8.7 How to Choose the Smoothing Parameter

Kernel estimator for two classes: h = 1
0.2
0.15
0.1
0.05
0

0

1

2

3

4

5

6

7

8

5

6

7

8

5

6

7

8

h = 0.5
0.4
0.3
0.2
0.1
0

0

1

2

3

4
h = 0.25

0.8
0.6
0.4
0.2
0

0

1

2

3

4

Figure 8.11 Kernel estimate for various bin lengths for a two-class problem.
Plotted are the conditional densities, p(x|Ci ). It seems that the top one oversmooths and the bottom undersmooths, but whichever is the best will depend
on where the validation data points are.

ˆ
r t − g(xt )

(8.22)
t

2

b

+λ

[ˆ (x)]2 dx
g

a

ˆ
The ﬁrst term is the error of ﬁt. [a, b] is the input range; g (·) is
ˆ
the curvature of the estimated function g(·) and as such measures the
variability. Thus the second term penalizes fast-varying estimates. λ
trades oﬀ variability and error where, for example, with large λ, we get
smoother estimates.
Cross-validation is used to tune h, k, or λ. In density estimation, we
choose the parameter value that maximizes the likelihood of the validation set. In a supervised setting, trying a set of candidates on the training
set (see ﬁgure 8.11), we choose the parameter value that minimizes the
error on the validation set.

180

8

8.8

case-based
reasoning

additive models

Nonparametric Methods

Notes
k-nearest neighbor and kernel-based estimation were proposed ﬁfty years
ago, but because of the need for large memory and computation, the
approach was not popular until recently (Aha, Kibler, and Albert 1991).
With advances in parallel processing and with memory and computation
getting cheaper, such methods have recently become more widely used.
Textbooks on nonparametric estimation are Silverman 1986 and Scott
1992. Dasarathy 1991 is a collection of many papers on k-nn and editing/condensing rules; Aha 1997 is a collection of more recent work.
The nonparametric methods are very easy to parallelize on a Single Instruction Multiple Data (SIMD) machine; each processor stores one training instance in its local memory and in parallel computes the kernel
function value for that instance (Stanﬁll and Waltz 1986). Multiplying
with a kernel function can be seen as a convolution, and we can use
Fourier transformation to calculate the estimate more eﬃciently (Silverman 1986). It has also been shown that spline smoothing is equivalent to
kernel smoothing.
The most critical factor in nonparametric estimation is the distance
metric used. With discrete attributes, we can simply use the Hamming
distance where we just sum up the number of nonmatching attributes.
More sophisticated distance functions are discussed in Wettschereck, Aha,
and Mohri 1997 and Webb 1999.
In artiﬁcial intelligence, the nonparametric approach is called casebased reasoning. The output is found by interpolating from known similar past “cases.” This also allows for some knowledge extraction: the
given output can be justiﬁed by listing these similar past cases.
Due to its simplicity, k-nn is the most widely used nonparametric classiﬁcation method and is quite successful in practice in a variety of applications. It has been shown (Cover and Hart 1967; reviewed in Duda, Hart,
and Stork 2001) that in the large sample case when N → ∞, the risk of
nearest neighbor (k = 1) is never worse than twice the Bayes’ risk (which
is the best that can be achieved), and, in that respect, it is said that “half
of the available information in an inﬁnite collection of classiﬁed samples
is contained in the nearest neighbor” (Cover and Hart 1967, 21). In the
case of k-nn, it has been shown that the risk asymptotes to the Bayes’
risk as k goes to inﬁnity.
Nonparametric regression is discussed in detail in Härdle 1990. Hastie
and Tibshirani (1990) discuss smoothing models and propose additive

8.9 Exercises

181

models where a multivariate function is written as a sum of univariate estimates. Locally weighted regression is discussed in Atkeson, Moore, and
Schaal 1997. These models bear much similarity to radial basis functions
and mixture of experts that we will discuss in chapter 12.
In the condensed nearest neighbor algorithm, we saw that we can keep
only a subset of the training instances, those that are close to the boundary, and we can deﬁne the discriminant using them only. This idea bears
much similarity to the support vector machines that we will discuss in
chapter 13. There we will also discuss various kernel functions to measure similarity between instances and how we can choose the best. Writing the prediction as a sum of the combined eﬀects of training instances
also underlies Gaussian processes (chapter 14), where a kernel function is
called a covariance function.

8.9

Exercises
1. How can we have a smooth histogram?
2. Show equation 8.17.
3. How does condensed nearest neighbor behave if k > 1?
4. In condensed nearest neighbor, an instance previously added to Z may no
longer be necessary after a later addition. How can we ﬁnd such instances
that are no longer necessary?
5. In a regressogram, instead of averaging in a bin and doing a constant ﬁt, one
can use the instances falling in a bin and do a linear ﬁt (see ﬁgure 8.12). Write
the code and compare this with the regressogram proper.
6. Write the error function for loess discussed in section 8.6.3.
7. Propose an incremental version of the running mean estimator, which, like
the condensed nearest neighbor, stores instances only when necessary.
8. Generalize kernel smoother to multivariate data.
9. In the running smoother, we can ﬁt a constant, a line, or a higher-degree
polynomial at a test point. How can we choose between them?
10. In the running mean smoother, additional to giving an estimate, can we also
calculate a conﬁdence interval indicating the variance (uncertainty) around
the estimate at that point?

182

8

Nonparametric Methods

Regressogram linear smoother: h = 6
4
2
0
−2

0

1

2

3

4
h=3

5

6

7

8

0

1

2

3

4
h=1

5

6

7

8

0

1

2

3

4

5

6

7

8

4
2
0
−2
−4
4
2
0
−2

Figure 8.12 Regressograms with linear ﬁts in bins for various bin lengths.

8.10

References
Aha, D. W., ed. 1997. Special Issue on Lazy Learning, Artiﬁcial Intelligence
Review 11(1–5): 7–423.
Aha, D. W., D. Kibler, and M. K. Albert. 1991. “Instance-Based Learning Algorithm.” Machine Learning 6: 37–66.
Atkeson, C. G., A. W. Moore, and S. Schaal. 1997. “Locally Weighted Learning.”
Artiﬁcial Intelligence Review 11: 11–73.
Cover, T. M., and P. E. Hart. 1967. “Nearest Neighbor Pattern Classiﬁcation.”
IEEE Transactions on Information Theory 13: 21–27.
Dasarathy, B. V. 1991. Nearest Neighbor Norms: NN Pattern Classiﬁcation Techniques. Los Alamitos, CA: IEEE Computer Society Press.
Duda, R. O., P. E. Hart, and D. G. Stork. 2001. Pattern Classiﬁcation, 2nd ed.
New York: Wiley.
Geman, S., E. Bienenstock, and R. Doursat. 1992. “Neural Networks and the
Bias/Variance Dilemma.” Neural Computation 4: 1–58.
Härdle, W. 1990. Applied Nonparametric Regression. Cambridge, UK: Cambridge University Press.

8.10 References

183

Hart, P. E. 1968. “The Condensed Nearest Neighbor Rule.” IEEE Transactions on
Information Theory 14: 515–516.
Hastie, T. J., and R. J. Tibshirani. 1990. Generalized Additive Models. London:
Chapman and Hall.
Hastie, T. J., and R. J. Tibshirani. 1996. “Discriminant Adaptive Nearest Neighbor Classiﬁcation.” IEEE Transactions on Pattern Analysis and Machine Intelligence 18: 607–616.
Scott, D. W. 1992. Multivariate Density Estimation. New York: Wiley.
Silverman, B. W. 1986. Density Estimation in Statistics and Data Analysis. London: Chapman and Hall.
Stanﬁll, C., and D. Waltz. 1986. “Toward Memory-Based Reasoning.” Communications of the ACM 29: 1213–1228.
Webb, A. 1999. Statistical Pattern Recognition. London: Arnold.
Wettschereck, D., D. W. Aha, and T. Mohri. 1997. “A Review and Empirical Evaluation of Feature Weighting Methods for a Class of Lazy Learning Algorithms.”
Artiﬁcial Intelligence Review 11: 273–314.
Wilfong, G. 1992. “Nearest Neighbor Problems.” International Journal on Computational Geometry and Applications 2: 383–416.

9

Decision Trees

A decision tree is a hierarchical data structure implementing the
divide-and-conquer strategy. It is an eﬃcient nonparametric method,
which can be used for both classiﬁcation and regression. We discuss
learning algorithms that build the tree from a given labeled training
sample, as well as how the tree can be converted to a set of simple
rules that are easy to understand. Another possibility is to learn a
rule base directly.

9.1

decision tree

decision node

Introduction
I n p a r a m e tr i c estimation, we deﬁne a model over the whole input
space and learn its parameters from all of the training data. Then we
use the same model and the same parameter set for any test input. In
nonparametric estimation, we divide the input space into local regions,
deﬁned by a distance measure like the Euclidean norm, and for each input, the corresponding local model computed from the training data in
that region is used. In the instance-based models we discussed in chapter 8, given an input, identifying the local data deﬁning the local model
is costly; it requires calculating the distances from the given input to all
of the training instances, which is O(N).
A decision tree is a hierarchical model for supervised learning whereby
the local region is identiﬁed in a sequence of recursive splits in a smaller
number of steps. A decision tree is composed of internal decision nodes
and terminal leaves (see ﬁgure 9.1). Each decision node m implements a
test function fm (x) with discrete outcomes labeling the branches. Given
an input, at each node, a test is applied and one of the branches is taken
depending on the outcome. This process starts at the root and is repeated

186

x2

9 Decision Trees

x1>w10

C2
Yes

No

x2>w20

w20
Yes

No

C1

C1
C2
w10

C1

x1

Figure 9.1 Example of a dataset and the corresponding decision tree. Oval
nodes are the decision nodes and rectangles are leaf nodes. The univariate decision node splits along one axis, and successive splits are orthogonal to each
other. After the ﬁrst split, {x|x1 < w10 } is pure and is not split further.

leaf node

recursively until a leaf node is hit, at which point the value written in the
leaf constitutes the output.
A decision tree is also a nonparametric model in the sense that we
do not assume any parametric form for the class densities and the tree
structure is not ﬁxed a priori but the tree grows, branches and leaves
are added, during learning depending on the complexity of the problem
inherent in the data.
Each fm (x) deﬁnes a discriminant in the d-dimensional input space
dividing it into smaller regions that are further subdivided as we take a
path from the root down. fm (·) is a simple function and when written
down as a tree, a complex function is broken down into a series of simple
decisions. Diﬀerent decision tree methods assume diﬀerent models for
fm (·), and the model class deﬁnes the shape of the discriminant and
the shape of regions. Each leaf node has an output label, which in the
case of classiﬁcation is the class code and in regression is a numeric
value. A leaf node deﬁnes a localized region in the input space where
instances falling in this region have the same labels (in classiﬁcation),

9.2 Univariate Trees

187

or very similar numeric outputs (in regression). The boundaries of the
regions are deﬁned by the discriminants that are coded in the internal
nodes on the path from the root to the leaf node.
The hierarchical placement of decisions allows a fast localization of the
region covering an input. For example, if the decisions are binary, then
in the best case, each decision eliminates half of the cases. If there are b
regions, then in the best case, the correct region can be found in log2 b
decisions. Another advantage of the decision tree is interpretability. As
we will see shortly, the tree can be converted to a set of IF-THEN rules that
are easily understandable. For this reason, decision trees are very popular and sometimes preferred over more accurate but less interpretable
methods.
We start with univariate trees where the test in a decision node uses
only one input variable and we see how such trees can be constructed
for classiﬁcation and regression. We later generalize this to multivariate
trees where all inputs can be used in an internal node.

9.2
univariate tree

(9.1)

binary split

Univariate Trees
In a univariate tree, in each internal node, the test uses only one of the
input dimensions. If the used input dimension, xj , is discrete, taking one
of n possible values, the decision node checks the value of xj and takes
the corresponding branch, implementing an n-way split. For example, if
an attribute is color ∈ {red, blue, green}, then a node on that attribute
has three branches, each one corresponding to one of the three possible
values of the attribute.
A decision node has discrete branches and a numeric input should be
discretized. If xj is numeric (ordered), the test is a comparison
fm (x) : xj > wm0
where wm0 is a suitably chosen threshold value. The decision node divides the input space into two: Lm = {x|xj > wm0 } and Rm = {x|xj ≤
wm0 }; this is called a binary split. Successive decision nodes on a path
from the root to a leaf further divide these into two using other attributes
and generating splits orthogonal to each other. The leaf nodes deﬁne hyperrectangles in the input space (see ﬁgure 9.1).
Tree induction is the construction of the tree given a training sample.
For a given training set, there exists many trees that code it with no error, and, for simplicity, we are interested in ﬁnding the smallest among

188

9 Decision Trees

them, where tree size is measured as the number of nodes in the tree
and the complexity of the decision nodes. Finding the smallest tree is
NP-complete (Quinlan 1986), and we are forced to use local search procedures based on heuristics that give reasonable trees in reasonable time.
Tree learning algorithms are greedy and, at each step, starting at the
root with the complete training data, we look for the best split. This
splits the training data into two or n, depending on whether the chosen
attribute is numeric or discrete. We then continue splitting recursively
with the corresponding subset until we do not need to split anymore, at
which point a leaf node is created and labeled.

9.2.1
classification tree
impurity measure

(9.2)

entropy

Classiﬁcation Trees
In the case of a decision tree for classiﬁcation, namely, a classiﬁcation
tree, the goodness of a split is quantiﬁed by an impurity measure. A
split is pure if after the split, for all branches, all the instances choosing
a branch belong to the same class. Let us say for node m, Nm is the
number of training instances reaching node m. For the root node, it is N.
i
i
Nm of Nm belong to class Ci , with i Nm = Nm . Given that an instance
reaches node m, the estimate for the probability of class Ci is
i
ˆ
P (Ci |x, m) ≡ pm =

i
Nm
Nm

i
Node m is pure if pm for all i are either 0 or 1. It is 0 when none of the
instances reaching node m are of class Ci , and it is 1 if all such instances
are of Ci . If the split is pure, we do not need to split any further and can
i
add a leaf node labeled with the class for which pm is 1. One possible
function to measure impurity is entropy (Quinlan 1986) (see ﬁgure 9.2)
K

(9.3)

i
i
pm log2 pm

Im = −
i=1

where 0 log 0 ≡ 0. Entropy in information theory speciﬁes the minimum
number of bits needed to encode the class code of an instance. In a twoclass problem, if p1 = 1 and p2 = 0, all examples are of C 1 , and we do
not need to send anything, and the entropy is 0. If p1 = p2 = 0.5, we
need to send a bit to signal one of the two cases, and the entropy is 1.
In between these two extremes, we can devise codes and use less than
a bit per message by having shorter codes for the more likely class and

189

9.2 Univariate Trees

1

entropy=−p*log2(p)−(1−p)*log2(1−p)

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

p

Figure 9.2 Entropy function for a two-class problem.

longer codes for the less likely. When there are K > 2 classes, the same
discussion holds and the largest entropy is log2 K when pi = 1/K.
But entropy is not the only possible measure. For a two-class problem
where p1 ≡ p and p2 = 1 − p, φ(p, 1 − p) is a nonnegative function
measuring the impurity of a split if it satisﬁes the following properties
(Devroye, Györﬁ, and Lugosi 1996):
φ(1/2, 1/2) ≥ φ(p, 1 − p), for any p ∈ [0, 1].
φ(0, 1) = φ(1, 0) = 0.
φ(p, 1−p) is increasing in p on [0, 1/2] and decreasing in p on [1/2, 1].
Examples are
1. Entropy
(9.4)

φ(p, 1 − p) = −p log2 p − (1 − p) log2 (1 − p)
Equation 9.3 is the generalization to K > 2 classes.

Gini index

(9.5)

2. Gini index (Breiman et al. 1984)
φ(p, 1 − p) = 2p(1 − p)

190

9 Decision Trees

3. Misclassiﬁcation error
(9.6)

φ(p, 1 − p) = 1 − max(p, 1 − p)
These can be generalized to K > 2 classes, and the misclassiﬁcation error can be generalized to minimum risk given a loss function (exercise 1).
Research has shown that there is not a signiﬁcant diﬀerence between
these three measures.
If node m is not pure, then the instances should be split to decrease
impurity, and there are multiple possible attributes on which we can split.
For a numeric attribute, multiple split positions are possible. Among all,
we look for the split that minimizes impurity after the split because we
want to generate the smallest tree. If the subsets after the split are closer
to pure, fewer splits (if any) will be needed afterward. Of course this is
locally optimal, and we have no guarantee of ﬁnding the smallest decision
tree.
Let us say at node m, Nmj of Nm take branch j; these are x t for which
the test fm (x t ) returns outcome j. For a discrete attribute with n values,
there are n outcomes, and for a numeric attribute, there are two outcomes
n
i
(n = 2), in either case satisfying j=1 Nmj = Nm . Nmj of Nmj belong to
K

n

i
i
i
class Ci : i=1 Nmj = Nmj . Similarly, j=1 Nmj = Nm .
Then given that at node m, the test returns outcome j, the estimate for
the probability of class Ci is

(9.7)

i
ˆ
P (Ci |x, m, j) ≡ pmj =

i
Nmj

Nmj

and the total impurity after the split is given as
n

(9.8)

Im = −

Nmj
Nm
j=1

K
i
i
pmj log2 pmj
i=1

i
In the case of a numeric attribute, to be able to calculate pmj using
equation 9.1, we also need to know wm0 for that node. There are Nm − 1
possible wm0 between Nm data points: we do not need to test for all
(possibly inﬁnite) points; it is enough to test, for example, at halfway
between points. Note also that the best split is always between adjacent
points belonging to diﬀerent classes. So we try them, and the best in
terms of purity is taken for the purity of the attribute. In the case of a
discrete attribute, no such iteration is necessary.

191

9.2 Univariate Trees

GenerateTree(X)
If NodeEntropy(X)< θI /* equation 9.3 */
Create leaf labelled by majority class in X
Return
i ← SplitAttribute(X)
For each branch of x i
Find Xi falling in branch
GenerateTree(Xi )
SplitAttribute(X)
MinEnt← MAX
For all attributes i = 1, . . . , d
If x i is discrete with n values
Split X into X1 , . . . , Xn by x i
e ← SplitEntropy(X1 , . . . , Xn ) /* equation 9.8 */
If e<MinEnt MinEnt ← e; bestf ← i
Else /* x i is numeric */
For all possible splits
Split X into X1 , X2 on x i
e←SplitEntropy(X1 , X2 )
If e<MinEnt MinEnt ← e; bestf ← i
Return bestf
Figure 9.3

Classification and
Regression Trees
ID3
C4.5

Classiﬁcation tree construction.

So for all attributes, discrete and numeric, and for a numeric attribute
for all split positions, we calculate the impurity and choose the one that
has the minimum entropy, for example, as measured by equation 9.8.
Then tree construction continues recursively and in parallel for all the
branches that are not pure, until all are pure. This is the basis of the Classiﬁcation and Regression Trees (CART) algorithm (Breiman et al. 1984),
ID3 algorithm (Quinlan 1986), and its extension C4.5 (Quinlan 1993). The
pseudocode of the algorithm is given in ﬁgure 9.3.
It can also be said that at each step during tree construction, we choose
the split that causes the largest decrease in impurity, which is the diﬀerence between the impurity of data reaching node m (equation 9.3) and the
total entropy of data reaching its branches after the split (equation 9.8).

192

9 Decision Trees

One problem is that such splitting favors attributes with many values.
When there are many values, there are many branches, and the impurity
can be much less. For example, if we take training index t as an attribute,
the impurity measure will choose that because then the impurity of each
branch is 0, although it is not a reasonable feature. Nodes with many
branches are complex and go against our idea of splitting class discriminants into simple decisions. Methods have been proposed to penalize
such attributes and to balance the impurity drop and the branching factor.
When there is noise, growing the tree until it is purest, we may grow
a very large tree and it overﬁts; for example, consider the case of a mislabeled instance amid a group of correctly labeled instances. To alleviate such overﬁtting, tree construction ends when nodes become pure
enough, namely, a subset of data is not split further if I < θI . This imi
plies that we do not require that pmj be exactly 0 or 1 but close enough,
with a threshold θp . In such a case, a leaf node is created and is labeled
i
with the class having the highest pmj .
θI (or θp ) is the complexity parameter, like h or k of nonparametric
estimation. When they are small, the variance is high and the tree grows
large to reﬂect the training set accurately, and when they are large, variance is lower and a smaller tree roughly represents the training set and
may have large bias. The ideal value depends on the cost of misclassiﬁcation, as well as the costs of memory and computation.
It is generally advised that in a leaf, one stores the posterior probabilities of classes, instead of labeling the leaf with the class having the
highest posterior. These probabilities may be required in later steps,
for example, in calculating risks. Note that we do not need to store the
instances reaching the node or the exact counts; just ratios suﬃce.

9.2.2
regression tree

(9.9)

Regression Trees
A regression tree is constructed in almost the same manner as a classiﬁcation tree, except that the impurity measure that is appropriate for
classiﬁcation is replaced by a measure appropriate for regression. Let us
say for node m, Xm is the subset of X reaching node m; namely, it is the
set of all x ∈ X satisfying all the conditions in the decision nodes on the
path from the root until node m. We deﬁne
bm (x) =

1
0

if x ∈ Xm : x reaches node m
otherwise

9.2 Univariate Trees

193

In regression, the goodness of a split is measured by the mean square
error from the estimated value. Let us say gm is the estimated value in
node m.
(9.10)

Em =

1
Nm

(r t − gm )2 bm (x t )
t

where Nm = |Xm | = t bm (x t ).
In a node, we use the mean (median if there is too much noise) of the
required outputs of instances reaching the node
(9.11)

gm =

t

bm (x t )r t
t
t bm (x )

Then equation 9.10 corresponds to the variance at m. If at a node, the
error is acceptable, that is, Em < θr , then a leaf node is created and it
stores the gm value. Just like the regressogram of chapter 8, this creates
a piecewise constant approximation with discontinuities at leaf boundaries.
If the error is not acceptable, data reaching node m is split further
such that the sum of the errors in the branches is minimum. As in classiﬁcation, at each node, we look for the attribute (and split threshold
for a numeric attribute) that minimizes the error, and then we continue
recursively.
Let us deﬁne Xmj as the subset of Xm taking branch j: ∪n Xmj = Xm .
j=1
We deﬁne
(9.12)

1
0

bmj (x) =

if x ∈ Xmj : x reaches node m and takes branch j
otherwise

gmj is the estimated value in branch j of node m.
(9.13)

gmj =

t

bmj (x t )r t
t
t bmj (x )

and the error after the split is
(9.14)

Em =

1
Nm

(r t − gmj )2 bmj (x t )
j

t

The drop in error for any split is given as the diﬀerence between equation 9.10 and equation 9.14. We look for the split such that this drop is
maximum or, equivalently, where equation 9.14 takes its minimum. The
code given in ﬁgure 9.3 can be adapted to training a regression tree by

194

9 Decision Trees

replacing entropy calculations with mean square error and class labels
with averages.
Mean square error is one possible error function; another is worst possible error
(9.15)

Em = max max |r t − gmj |bm (x t )
j

t

and using this, we can guarantee that the error for any instance is never
larger than a given threshold.
The acceptable error threshold is the complexity parameter; when it is
small, we generate large trees and risk overﬁtting; when it is large, we
underﬁt and smooth too much (see ﬁgures 9.4 and 9.5).
Similar to going from running mean to running line in nonparametric
regression, instead of taking an average at a leaf that implements a constant ﬁt, we can also do a linear regression ﬁt over the instances choosing
the leaf:
(9.16)

gm (x) = w T x + wm0
m
This makes the estimate in a leaf dependent on x and generates smaller
trees, but there is the expense of extra computation at a leaf node.

9.3

prepruning
postpruning

pruning set

Pruning
Frequently, a node is not split further if the number of training instances
reaching a node is smaller than a certain percentage of the training set—
for example, 5 percent—regardless of the impurity or error. The idea is
that any decision based on too few instances causes variance and thus
generalization error. Stopping tree construction early on before it is full
is called prepruning the tree.
Another possibility to get simpler trees is postpruning, which in practice works better than prepruning. We saw before that tree growing is
greedy and at each step, we make a decision, namely, generate a decision
node, and continue further on, never backtracking and trying out an alternative. The only exception is postpruning where we try to ﬁnd and
prune unnecessary subtrees.
In postpruning, we grow the tree full until all leaves are pure and we
have no training error. We then ﬁnd subtrees that cause overﬁtting and
we prune them. From the initial labeled set, we set aside a pruning set,
unused during training. For each subtree, we replace it with a leaf node

195

9.3 Pruning

4

θ = 0.5
r

2
0
−2

0

1

2

3

4

4

5

6

7

8

5

6

7

8

5

6

7

8

θ = 0.2
r

2
0
−2

0

1

2

3

4

4
θr = 0.05

2
0
−2

0

1

2

3

4

Figure 9.4 Regression tree smooths for various values of θr . The corresponding
trees are given in ﬁgure 9.5.

labeled with the training instances covered by the subtree (appropriately
for classiﬁcation or regression). If the leaf node does not perform worse
than the subtree on the pruning set, we prune the subtree and keep the
leaf node because the additional complexity of the subtree is not justiﬁed;
otherwise, we keep the subtree.
For example, in the third tree of ﬁgure 9.5, there is a subtree starting
with condition x < 6.31. This subtree can be replaced by a leaf node of
y = 0.9 (as in the second tree) if the error on the pruning set does not
increase during the substitution. Note that the pruning set should not be
confused with (and is distinct from) the validation set.
Comparing prepruning and postpruning, we can say that prepruning is
faster but postpruning generally leads to more accurate trees.

196

9 Decision Trees

x < 3.16
Yes

No

x < 1.36

1.86

Yes

No

1.37

-1.35

x < 3.16
Yes

No

x < 1.36

x < 5 .96

Yes

No

Yes

1.37

-1.35

No

2.20

x < 6.91
Yes

No

0.9

2.40

x < 3.16
Yes

No

x < 1.36
Yes

x < 5 .96
No
-1.35

x < 0.7 6
Yes

No

2.20

x < 6.91
Yes

No

1.15

Yes

1.80

No

x < 6.31

2.40

Yes

No

1.20

0.60

Figure 9.5 Regression trees implementing the smooths of ﬁgure 9.4 for various
values of θr .

197

9.4 Rule Extraction from Trees

x1 : Age
x2 : Years in job
x3 : Gender
x4 : Job type

x1 > 38.5
Yes

No
x4

x2 > 2.5
Yes
0.8

'A'

No
0.6

0.4

'B'
0.3

'C'
0.2

Figure 9.6 Example of a (hypothetical) decision tree. Each path from the root to
a leaf can be written down as a conjunctive rule, composed of conditions deﬁned
by the decision nodes on the path.

9.4

interpretability

IF-THEN rules

Rule Extraction from Trees
A decision tree does its own feature extraction. The univariate tree only
uses the necessary variables, and after the tree is built, certain features
may not be used at all. We can also say that features closer to the root
are more important globally. For example, the decision tree given in ﬁgure 9.6 uses x1 , x2 , and x4 , but not x3 . It is possible to use a decision tree
for feature extraction: we build a tree and then take only those features
used by the tree as inputs to another learning method.
Another main advantage of decision trees is interpretability: The decision nodes carry conditions that are simple to understand. Each path
from the root to a leaf corresponds to one conjunction of tests, as all
those conditions should be satisﬁed to reach to the leaf. These paths together can be written down as a set of IF-THEN rules, called a rule base.
One such method is C4.5Rules (Quinlan 1993).
For example, the decision tree of ﬁgure 9.6 can be written down as the
following set of rules:
R1:
R2:
R3:
R4:
R5:

IF (age>38.5)
IF (age>38.5)
IF (age≤38.5)
IF (age≤38.5)
IF (age≤38.5)

AND
AND
AND
AND
AND

(years-in-job>2.5) THEN y =0.8
(years-in-job≤2.5) THEN y =0.6
(job-type=‘A’) THEN y =0.4
(job-type=‘B’) THEN y =0.3
(job-type=‘C’) THEN y =0.2

198

9 Decision Trees

knowledge
extraction

rule support

Such a rule base allows knowledge extraction; it can be easily understood and allows experts to verify the model learned from data. For each
rule, one can also calculate the percentage of training data covered by the
rule, namely, rule support. The rules reﬂect the main characteristics of
the dataset: they show the important features and split positions. For instance, in this (hypothetical) example, we see that in terms of our purpose
(y), people who are thirty-eight years old or less are diﬀerent from people
who are thirty-nine or more years old. And among this latter group, it is
the job type that makes them diﬀerent, whereas in the former group, it is
the number of years in a job that is the best discriminating characteristic.
In the case of a classiﬁcation tree, there may be more than one leaf
labeled with the same class. In such a case, these multiple conjunctive
expressions corresponding to diﬀerent paths can be combined as a disjunction (OR). The class region then corresponds to a union of these multiple patches, each patch corresponding to the region deﬁned by one leaf.
For example, class C1 of ﬁgure 9.1 is written as
IF (x ≤ w10 ) OR ((x1 > w10 ) AND (x2 ≤ w20 )) THEN C1

pruning rules

Pruning rules is possible for simpliﬁcation. Pruning a subtree corresponds to pruning terms from a number of rules at the same time. It
may be possible to prune a term from one rule without touching other
rules. For example, in the previous rule set, for R3, if we see that all
whose job-type=‘A’ have outcomes close to 0.4, regardless of age, R3
can be pruned as
R3 : IF (job-type=‘A’) THEN y =0.4
Note that after the rules are pruned, it may not be possible to write
them back as a tree anymore.

9.5

rule induction

Learning Rules from Data
As we have just seen, one way to get IF-THEN rules is to train a decision
tree and convert it to rules. Another is to learn the rules directly. Rule
induction works similar to tree induction except that rule induction does
a depth-ﬁrst search and generates one path (rule) at a time, whereas tree
induction goes breadth-ﬁrst and generates all paths simultaneously.
Rules are learned one at a time. Each rule is a conjunction of conditions on discrete or numeric attributes (as in decision trees) and these

9.5 Learning Rules from Data

sequential
covering

Ripper
Irep

Foil

(9.17)

rule value metric

(9.18)

199

conditions are added one at a time, to optimize some criterion, for example, minimize entropy. A rule is said to cover an example if the example
satisﬁes all the conditions of the rule. Once a rule is grown and pruned,
it is added to the rule base and all the training examples covered by the
rule are removed from the training set, and the process continues until
enough rules are added. This is called sequential covering. There is an
outer loop of adding one rule at a time to the rule base and an inner loop
of adding one condition at a time to the current rule. These steps are
both greedy and do not guarantee optimality. Both loops have a pruning
step for better generalization.
One example of a rule induction algorithm is Ripper (Cohen 1995),
based on an earlier algorithm Irep (Fürnkranz and Widmer 1994). We
start with the case of two classes where we talk of positive and negative
examples, then later generalize to K > 2 classes. Rules are added to explain positive examples such that if an instance is not covered by any
rule, then it is classiﬁed as negative. So a rule when it matches is either
correct (true positive), or it causes a false positive. The pseudocode of
the outer loop of Ripper is given in ﬁgure 9.7.
In Ripper, conditions are added to the rule to maximize an information
gain measure used in Quinlan’s (1990) Foil algorithm. Let us say we have
rule R and R is the candidate rule after adding a condition. Change in
gain is deﬁned as
Gain(R , R) = s · log2

N+
N+
− log2
N
N

where N is the number of instances that are covered by R and N+ is the
number of true positives in them. N and N+ are similarly deﬁned for R .
s is the number of true positives in R, which are still true positives in R ,
after adding the condition. In terms of information theory, the change in
gain measures the reduction in bits to encode a positive instance.
Conditions are added to a rule until it covers no negative example.
Once a rule is grown, it is pruned back by deleting conditions in reverse
order, to ﬁnd the rule that maximizes the rule value metric
r vm(R) =

p−n
p+n

where p and n are the number of true and false positives, respectively,
on the pruning set, which is one-third of the data, having used two-thirds
as the growing set.

200

9 Decision Trees

Ripper(Pos,Neg,k)
RuleSet ← LearnRuleSet(Pos,Neg)
For k times
RuleSet ← OptimizeRuleSet(RuleSet,Pos,Neg)
LearnRuleSet(Pos,Neg)
RuleSet ← ∅
DL ← DescLen(RuleSet,Pos,Neg)
Repeat
Rule ← LearnRule(Pos,Neg)
Add Rule to RuleSet
DL’ ← DescLen(RuleSet,Pos,Neg)
If DL’>DL+64
PruneRuleSet(RuleSet,Pos,Neg)
Return RuleSet
If DL’<DL DL ← DL’
Delete instances covered by Rule from Pos and Neg
Until Pos = ∅
Return RuleSet
PruneRuleSet(RuleSet,Pos,Neg)
For each Rule ∈ RuleSet in reverse order
DL ← DescLen(RuleSet,Pos,Neg)
DL’ ← DescLen(RuleSet-Rule,Pos,Neg)
IF DL’<DL Delete Rule from RuleSet
Return RuleSet
OptimizeRuleSet(RuleSet,Pos,Neg)
For each Rule ∈ RuleSet
DL0 ← DescLen(RuleSet,Pos,Neg)
DL1 ← DescLen(RuleSet-Rule+
ReplaceRule(RuleSet,Pos,Neg),Pos,Neg)
DL2 ← DescLen(RuleSet-Rule+
ReviseRule(RuleSet,Rule,Pos,Neg),Pos,Neg)
If DL1=min(DL0,DL1,DL2)
Delete Rule from RuleSet and
add ReplaceRule(RuleSet,Pos,Neg)
Else If DL2=min(DL0,DL1,DL2)
Delete Rule from RuleSet and
add ReviseRule(RuleSet,Rule,Pos,Neg)
Return RuleSet
Figure 9.7 Ripper algorithm for learning rules. Only the outer loop is given; the
inner loop is similar to adding nodes in a decision tree.

9.5 Learning Rules from Data

propositional rules
first-order rules

201

Once a rule is grown and pruned, all positive and negative training examples covered by the rule are removed from the training set. If there
are remaining positive examples, rule induction continues. In the case of
noise, we may stop early, namely, when a rule does not explain enough
number of examples. To measure the worth of a rule, minimum description length (section 4.8) is used (Quinlan 1995). Typically, we stop if the
description of the rule is not shorter than the description of instances
it explains. The description length of a rule base is the sum of the description lengths of all the rules in the rule base, plus the description of
instances not covered by the rule base. Ripper stops adding rules when
the description length of the rule base is more than 64 bits larger than
the best description length so far. Once the rule base is learned, we pass
over the rules in reverse order to see if they can be removed without
increasing the description length.
Rules in the rule base are also optimized after they are learned. Ripper
considers two alternatives to a rule: One, called the replacement rule,
starts from an empty rule, is grown, and is then pruned. The second,
called the revision rule, starts with the rule as it is, is grown, and is then
pruned. These two are compared with the original rule, and the shortest
of three is added to the rule base. This optimization of the rule base can
be done k times, typically twice.
When there are K > 2 classes, they are ordered in terms of their prior
probabilities such that C1 has the lowest prior probability and CK has the
highest. Then a sequence of two-class problems are deﬁned such that,
ﬁrst, instances belonging to C1 are taken as positive examples and instances of all other classes are taken as negative examples. Then, having
learned C1 and all its instances removed, it learns to separate C2 from
C3 , . . . , CK . This process is repeated until only CK remains. The empty
default rule is then labeled CK , so that if an instance is not covered by
any rule, it will be assigned to CK .
For a training set of size N, Ripper’s complexity is O(N log2 N) and
is an algorithm that can be used on very large training sets (Dietterich
1997). The rules we learn are propositional rules. More expressive, ﬁrstorder rules have variables in conditions, called predicates. A predicate is
a function that returns true or false depending on the value of its argument. Predicates therefore allow deﬁning relations between the values of
attributes, which cannot be done by propositions (Mitchell 1997):
IF Father(y, x) AND Female(y) THEN Daughter(x, y)

202

9 Decision Trees

inductive logic
programming
binding

9.6

multivariate tree

(9.19)

Such rules can be seen as programs in a logic programming language,
such as Prolog, and learning them from data is called inductive logic programming. One such algorithm is Foil (Quinlan 1990).
Assigning a value to a variable is called binding. A rule matches if
there is a set of bindings to the variables existing in the training set.
Learning ﬁrst-order rules is similar to learning propositional rules with
an outer loop of adding rules, and an inner loop of adding conditions to
a rule, with prunings at the end of each loop. The diﬀerence is in the
inner loop, where at each step we consider one predicate to add (instead
of a proposition) and check the increase in the performance of the rule
(Mitchell 1997). To calculate the performance of a rule, we consider all
possible bindings of the variables, count the number of positive and negative bindings in the training set, and use, for example, equation 9.17. In
this ﬁrst-order case, we have predicates instead of propositions, so they
should be previously deﬁned, and the training set is a set of predicates
known to be true.

Multivariate Trees
In the case of a univariate tree, only one input dimension is used at a
split. In a multivariate tree, at a decision node, all input dimensions can
be used and thus it is more general. When all inputs are numeric, a binary
linear multivariate node is deﬁned as
fm (x) : w T x + wm0 > 0
m
Because the linear multivariate node takes a weighted sum, discrete
attributes should be represented by 0/1 dummy numeric variables. Equation 9.19 deﬁnes a hyperplane with arbitrary orientation (see ﬁgure 9.8).
Successive nodes on a path from the root to a leaf further divide these,
and leaf nodes deﬁne polyhedra in the input space. The univariate node
with a numeric feature is a special case when all but one of wmj are 0.
Thus the univariate numeric node of equation 9.1 also deﬁnes a linear
discriminant but one that is orthogonal to axis xj , intersecting it at wm0
and parallel to all other xi . We therefore see that in a univariate node
there are d possible orientations (w m ) and Nm − 1 possible thresholds
(−wm0 ), making an exhaustive search possible. In a multivariate node,
Nm
there are 2d
possible hyperplanes (Murthy, Kasif, and Salzberg
d
1994) and an exhaustive search is no longer practical.

203

9.6 Multivariate Trees

 
¡
¢
Figure 9.8 Example of a linear multivariate decision tree. The linear multivariate node can place an arbitrary hyperplane and thus is more general, whereas
the univariate node is restricted to axis-aligned splits.

When we go from a univariate node to a linear multivariate node, the
node becomes more ﬂexible. It is possible to make it even more ﬂexible
by using a nonlinear multivariate node. For example, with a quadratic, we
have
(9.20)

sphere node

(9.21)

CART

OC1

fm (x) : x T Wm x + w T x + wm0 > 0
m
Guo and Gelfand (1992) propose to use a multilayer perceptron (chapter 11) that is a linear sum of nonlinear basis functions, and this is another way of having nonlinear decision nodes. Another possibility is a
sphere node (Devroye, Györﬁ, and Lugosi 1996)
fm (x) : x − c m ≤ αm
where c m is the center and αm is the radius.
There are a number of algorithms proposed for learning multivariate
decision trees for classiﬁcation: The earliest is the multivariate version of
the CART algorithm (Breiman et al. 1984), which ﬁne-tunes the weights
wmj one by one to decrease impurity. CART also has a preprocessing
stage to decrease dimensionality through subset selection (chapter 6) and
reduce the complexity of the node. An algorithm with some extensions
to CART is the OC1 algorithm (Murthy, Kasif, and Salzberg 1994). One

204

9 Decision Trees

possibility (Loh and Vanichsetakul 1988) is to assume that all classes are
Gaussian with a common covariance matrix, thereby having linear discriminants separating each class from the others (chapter 5). In such a
case, with K classes, each node has K branches and each branch carries
the discriminant separating one class from the others. Brodley and Utgoﬀ (1995) propose a method where the linear discriminants are trained
to minimize classiﬁcation error (chapter 10). Guo and Gelfand (1992)
propose a heuristic to group K > 2 classes into two supergroups, and
then binary multivariate trees can be learned. Loh and Shih (1997) use 2means clustering (chapter 7) to group data into two. Yıldız and Alpaydın
(2000) use LDA (chapter 6) to ﬁnd the discriminant once the classes are
grouped into two.
Any classiﬁer approximates the real (unknown) discriminant choosing
one hypothesis from its hypothesis class. When we use univariate nodes,
our approximation uses piecewise, axis-aligned hyperplanes. With linear
multivariate nodes, we can use arbitrary hyperplanes and do a better approximation using fewer nodes. If the underlying discriminant is curved,
nonlinear nodes work better. The branching factor has a similar eﬀect
in that it speciﬁes the number of discriminants that a node deﬁnes. A
binary decision node with two branches deﬁnes one discriminant separating the input space into two. An n-way node separates into n. Thus,
there is a dependency among the complexity of a node, the branching
factor, and tree size. With simple nodes and low branching factors, one
may grow large trees, but such trees, for example, with univariate binary
nodes, are more interpretable. Linear multivariate nodes are more difﬁcult to interpret. More complex nodes also require more data and are
prone to overﬁtting as we get down the tree and have less and less data.
If the nodes are complex and the tree is small, we also lose the main idea
of the tree, which is that of dividing the problem into a set of simple
problems. After all, we can have a very complex classiﬁer in the root that
separates all classes from each other, but then this will not be a tree!

9.7

Notes
Divide-and-conquer is a frequently used heuristic that has been used
since the days of Caesar to break a complex problem, for example, Gaul,
into a group of simpler problems. Trees are frequently used in computer
science to decrease complexity from linear to log time. Decision trees

9.7 Notes

omnivariate
decision tree

205

were made popular in statistics in Breiman et al. 1984 and in machine
learning in Quinlan 1986 and Quinlan 1993. Multivariate tree induction
methods became popular more recently; a review and comparison on
many datasets are given in Yıldız and Alpaydın 2000. Many researchers
(e.g., Guo and Gelfand 1992), proposed to combine the simplicity of trees
with the accuracy of multilayer perceptrons (chapter 11). Many studies,
however, have concluded that the univariate trees are quite accurate and
interpretable, and the additional complexity brought by linear (or nonlinear) multivariate nodes is hardly justiﬁed. A recent survey is given by
Rokach and Maimon (2005).
The omnivariate decision tree (Yıldız and Alpaydın 2001) is a hybrid
tree architecture where the tree may have univariate, linear multivariate,
or nonlinear multivariate nodes. The idea is that during construction, at
each decision node, which corresponds to a diﬀerent subproblem deﬁned
by the subset of the training data reaching that node, a diﬀerent model
may be appropriate and the appropriate one should be found and used.
Using the same type of nodes everywhere corresponds to assuming that
the same inductive bias is good in all parts of the input space. In an omnivariate tree, at each node, candidate nodes of diﬀerent types are trained
and compared using a statistical test (chapter 19) on a validation set to
determine which one generalizes the best. The simpler one is chosen
unless a more complex one is shown to have signiﬁcantly higher accuracy. Results show that more complex nodes are used early in the tree,
closer to the root, and as we go down the tree, simple univariate nodes
suﬃce. As we get closer to the leaves, we have simpler problems and, at
the same time, we have less data. In such a case, complex nodes overﬁt
and are rejected by the statistical test. The number of nodes increases
exponentially as we go down the tree; therefore, a large majority of the
nodes are univariate and the overall complexity does not increase much.
Decision trees are used more frequently for classiﬁcation than for regression. They are very popular: They learn and respond quickly, and
are accurate in many domains (Murthy 1998). It is even the case that a
decision tree is preferred over more accurate methods, because it is interpretable. When written down as a set of IF-THEN rules, the tree can be
understood and the rules can be validated by human experts who have
knowledge of the application domain.
It is generally recommended that a decision tree be tested and its accuracy be taken as a benchmark before more complicated algorithms are
employed. Analysis of the tree also allows an understanding of the im-

206

9 Decision Trees

portant features, and the univariate tree does its own automatic feature
extraction. Another big advantage of the univariate tree is that it can use
numeric and discrete features together, without needing to convert one
type into the other.
The decision tree is a nonparametric method, similar to the instancebased methods discussed in chapter 8, but there are a number of diﬀerences:
Each leaf node corresponds to a “bin,” except that the bins need not
be the same size (as in Parzen windows) or contain an equal number
of training instances (as in k-nearest neighbor).
The bin divisions are not done based only on similarity in the input
space, but supervised output information through entropy or mean
square error is also used.
Another advantage of the decision tree is that, thanks to the tree structure, the leaf (“bin”) is found much faster with smaller number of comparisons.
The decision tree, once it is constructed, does not store all the training
set but only the structure of the tree, the parameters of the decision
nodes, and the output values in leaves; this implies that the space complexity is also much less, as opposed to instance-based nonparametric
methods that store all training examples.
With a decision tree, a class need not have a single description to which
all instances should match. It may have a number of possible descriptions that can even be disjoint in the input space.
The tree is diﬀerent from the statistical models discussed in previous
chapters. The tree codes directly the discriminants separating class instances without caring much for how those instances are distributed in
the regions. The decision tree is discriminant-based, whereas the statistical methods are likelihood-based in that they explicitly estimate p(x|Ci )
before using Bayes’ rule and calculating the discriminant. Discriminantbased methods directly estimate the discriminants, bypassing the estimation of class densities. We further discuss such discriminant-based
methods in the chapters ahead.

9.8 Exercises

9.8

207

Exercises
1. Generalize the Gini index (equation 9.5) and the misclassiﬁcation error (equation 9.6) for K > 2 classes. Generalize misclassiﬁcation error to risk, taking a
loss function into account.
2. For a numeric input, instead of a binary split, one can use a ternary split with
two thresholds and three branches as
xj < wma , wma ≤ xj < wmb , xj ≥ wmb
Propose a modiﬁcation of the tree induction method to learn the two thresholds, wma , wmb . What are the advantages and the disadvantages of such a
node over a binary node?
3. Propose a tree induction algorithm with backtracking.
4. In generating a univariate tree, a discrete attribute with n possible values
can be represented by n 0/1 dummy variables and then treated as n separate numeric attributes. What are the advantages and disadvantages of this
approach?
5. Derive a learning algorithm for sphere trees (equation 9.21). Generalize to
ellipsoid trees.
6. In a regression tree, we discussed that in a leaf node, instead of calculating
the mean, we can do a linear regression ﬁt and make the response at the leaf
dependent on the input. Propose a similar method for classiﬁcation trees.
7. Propose a rule induction algorithm for regression.
8. In regression trees, how can we get rid of discountinuities at the leaf boundaries?
9. Let us say that for a classiﬁcation problem, we already have a trained decision
tree. How can we use it in addition to the training set in constructing a knearest neighbor classiﬁer?
10. In a multivariate tree, very probably, at each internal node, we will not be
needing all the input variables. How can we decrease dimensionality at a
node?

9.9

References
Breiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. Classiﬁcation
and Regression Trees. Belmont, CA: Wadsworth International Group.
Brodley, C. E., and P. E. Utgoﬀ. 1995. “Multivariate Decision Trees.” Machine
Learning 19: 45–77.

208

9 Decision Trees

Cohen, W. 1995. “Fast Eﬀective Rule Induction.” In Twelfth International Conference on Machine Learning, ed. A. Prieditis and S. J. Russell, 115–123. San
Mateo, CA: Morgan Kaufmann.
Devroye, L., L. Györﬁ, and G. Lugosi. 1996. A Probabilistic Theory of Pattern
Recognition. New York: Springer.
Dietterich, T. G. 1997. “Machine Learning Research: Four Current Directions.”
AI Magazine 18: 97–136.
Fürnkranz, J., and G. Widmer. 1994. “Incremental Reduced Error Pruning.” In
Eleventh International Conference on Machine Learning, ed. W. Cohen and H.
Hirsh, 70–77. San Mateo, CA: Morgan Kaufmann.
Guo, H., and S. B. Gelfand. 1992. “Classiﬁcation Trees with Neural Network
Feature Extraction.” IEEE Transactions on Neural Networks 3: 923–933.
Loh, W.-Y., and Y. S. Shih. 1997. “Split Selection Methods for Classiﬁcation
Trees.” Statistica Sinica 7: 815–840.
Loh, W.-Y., and N. Vanichsetakul. 1988. “Tree-Structured Classiﬁcation via Generalized Discriminant Analysis.” Journal of the American Statistical Association 83: 715–725.
Mitchell, T. 1997. Machine Learning. New York: McGraw-Hill.
Murthy, S. K. 1998. “Automatic Construction of Decision Trees from Data: A
Multi-Disciplinary Survey.” Data Mining and Knowledge Discovery 4: 345–
389.
Murthy, S. K., S. Kasif, and S. Salzberg. 1994. “A System for Induction of Oblique
Decision Trees.” Journal of Artiﬁcial Intelligence Research 2: 1–32.
Quinlan, J. R. 1986. “Induction of Decision Trees.” Machine Learning 1: 81–106.
Quinlan, J. R. 1990. “Learning Logical Deﬁnitions from Relations.” Machine
Learning 5: 239–266.
Quinlan, J. R. 1993. C4.5: Programs for Machine Learning. San Mateo, CA:
Morgan Kaufmann.
Quinlan, J. R. 1995. “MDL and Categorical Theories (continued).” In Twelfth International Conference on Machine Learning, ed. A. Prieditis and S. J. Russell,
467–470. San Mateo, CA: Morgan Kaufmann.
Rokach, L., and O. Maimon. 2005. “Top-Down Induction of Decision Trees
Classiﬁers—A Survey.” IEEE Transactions on Systems, Man, and Cybernetics–
Part C 35: 476–487.
Yıldız, O. T., and E. Alpaydın. 2000. “Linear Discriminant Trees.” In Seventeenth
International Conference on Machine Learning, ed. P. Langley, 1175–1182.
San Francisco: Morgan Kaufmann.
Yıldız, O. T., and E. Alpaydın. 2001. “Omnivariate Decision Trees.” IEEE Transactions on Neural Networks 12: 1539–1546.

10

Linear Discrimination

In linear discrimination, we assume that instances of a class are linearly separable from instances of other classes. This is a discriminantbased approach that estimates the parameters of the linear discriminant directly from a given labeled sample.

10.1

Introduction
W e r e m e m be r from the previous chapters that in classiﬁcation we deﬁne a set of discriminant functions gj (x), j = 1, . . . , K, and then we
K

choose Ci if gi (x) = max gj (x)
j=1

Previously, when we discussed methods for classiﬁcation, we ﬁrst esˆ
ˆ
timated the prior probabilities, P (Ci ), and the class likelihoods, p(x|Ci ),
then used Bayes’ rule to calculate the posterior densities. We then deﬁned
the discriminant functions in terms of the posterior, for example,
ˆ
gi (x) = log P (Ci |x)
likelihood-based
classification

discriminant-based
classification

This is called likelihood-based classiﬁcation, and we have previously
discussed the parametric (chapter 5), semiparametric (chapter 7), and
nonparametric (chapter 8) approaches to estimating the class likelihoods,
p(x|Ci ).
We are now going to discuss discriminant-based classiﬁcation where
we assume a model directly for the discriminant, bypassing the estimation of likelihoods or posteriors. The discriminant-based approach, as we
also saw for the case of decision trees in chapter 9, makes an assumption
on the form of the discriminant between the classes and makes no assumption about, or requires no knowledge of the densities—for example,

210

10 Linear Discrimination

whether they are Gaussian, or whether the inputs are correlated, and so
forth.
We deﬁne a model for the discriminant
gi (x|Φi )
explicitly parameterized with the set of parameters Φi , as opposed to
a likelihood-based scheme that has implicit parameters in deﬁning the
likelihood densities. This is a diﬀerent inductive bias: instead of making
an assumption on the form of the class densities, we make an assumption
on the form of the boundaries separating classes.
Learning is the optimization of the model parameters Φi to maximize
the quality of the separation, that is, the classiﬁcation accuracy on a given
labeled training set. This diﬀers from the likelihood-based methods that
search for the parameters that maximize sample likelihoods, separately
for each class.
In the discriminant-based approach, we do not care about correctly
estimating the densities inside class regions; all we care about is the correct estimation of the boundaries between the class regions. Those who
advocate the discriminant-based approach (e.g., Vapnik 1995) state that
estimating the class densities is a harder problem than estimating the
class discriminants, and it does not make sense to solve a hard problem to solve an easier problem. This is of course true only when the
discriminant can be approximated by a simple function.
In this chapter, we concern ourselves with the simplest case where the
discriminant functions are linear in x:
d

(10.1)

gi (x|w i , wi0 ) = w T x + wi0 =
i

wij xj + wi0
j=1

linear discriminant

The linear discriminant is used frequently mainly due to its simplicity:
both the space and time complexities are O(d). The linear model is easy
to understand: the ﬁnal output is a weighted sum of the input attributes
xj . The magnitude of the weight wj shows the importance of xj and
its sign indicates if the eﬀect is positive or negative. Most functions are
additive in that the output is the sum of the eﬀects of several attributes
where the weights may be positive (enforcing) or negative (inhibiting).
For example, when a customer applies for credit, ﬁnancial institutions
calculate the applicant’s credit score that is generally written as a sum of
the eﬀects of various attributes; for example, yearly income has a positive
eﬀect (higher incomes increase the score).

10.2 Generalizing the Linear Model

211

In many applications, the linear discriminant is also quite accurate. We
know, for example, that when classes are Gaussian with a shared covariance matrix, the optimal discriminant is linear. The linear discriminant,
however, can be used even when this assumption does not hold, and the
model parameters can be calculated without making any assumptions
on the class densities. We should always use the linear discriminant before trying a more complicated model to make sure that the additional
complexity is justiﬁed.
As always, we formulate the problem of ﬁnding a linear discriminant
function as a search for the parameter values that minimize an error
function. In particular, we concentrate on gradient methods for optimizing a criterion function.

10.2
quadratic
discriminant

(10.2)

higher-order terms
product terms

Generalizing the Linear Model
When a linear model is not ﬂexible enough, we can use the quadratic
discriminant function and increase complexity
gi (x|Wi , w i , wi0 ) = x T Wi x + w i x + wi0
but this approach is O(d 2 ) and we again have the bias/variance dilemma:
the quadratic model, though is more general, requires much larger training sets and may overﬁt on small samples.
An equivalent way is to preprocess the input by adding higher-order
terms, also called product terms. For example, with two inputs x1 and x2 ,
we can deﬁne new variables
z1 = x1 , z2 = x2 , z3 = x2 , z4 = x2 , z5 = x1 x2
1
2
and take z = [z1 , z2 , z3 , z4 , z5 ]T as the input. The linear function deﬁned
in the ﬁve-dimensional z space corresponds to a nonlinear function in
the two-dimensional x space. Instead of deﬁning a nonlinear function
(discriminant or regression) in the original space, what we do is to deﬁne
a suitable nonlinear transformation to a new space where the function
can be written in a linear form.
We write the discriminant as
k

(10.3)

gi (x) =

wj φij (x)
j=1

basis function

where φij (x) are basis functions. Higher-order terms are only one set of
possible basis functions; other examples are

212

10 Linear Discrimination

sin(x1 )
exp(−(x1 − m)2 /c)
exp(− x − m

2

/c)

log(x2 )
1(x1 > c)
1(ax1 + bx2 > c)

potential function

10.3
10.3.1

where m, a, b, c are scalars, m is a d-dimensional vector, and 1(b) returns
1 if b is true and returns 0 otherwise. The idea of writing a nonlinear
function as a linear sum of nonlinear basis functions is an old idea and
was originally called potential functions (Aizerman, Braverman, and Rozonoer 1964). Multilayer perceptrons (chapter 11) and radial basis functions (chapter 12) have the advantage that the parameters of the basis
functions can be ﬁne-tuned to the data during learning. In chapter 13,
we discuss support vector machines that use kernel functions built from
such basis functions.

Geometry of the Linear Discriminant
Two Classes
Let us start with the simpler case of two classes. In such a case, one
discriminant function is suﬃcient:
=

g1 (x) − g2 (x)

=

(w T x + w10 ) − (w T x + w20 )
1
2

=

(w 1 − w 2 )T x + (w10 − w20 )

=

g(x)

w T x + w0

and we
choose
weight vector
threshold

C1
C2

if g(x) > 0
otherwise

This deﬁnes a hyperplane where w is the weight vector and w0 is the
threshold. This latter name comes from the fact that the decision rule
can be rewritten as follows: choose C1 if w T x > −w0 , and choose C2

213

x2

10.3 Geometry of the Linear Discriminant

g(x)=w1x1+w2 x2+w0=0
g(x)> 0

g(x)< 0

C1

C2

+

x1
Figure 10.1 In the two-dimensional case, the linear discriminant is a line that
separates the examples from two classes.

otherwise. The hyperplane divides the input space into two half-spaces:
the decision region R1 for C1 and R2 for C2 . Any x in R1 is on the positive
side of the hyperplane and any x in R2 is on its negative side. When x is
0, g(x) = w0 and we see that if w0 > 0, the origin is on the positive side
of the hyperplane, and if w0 < 0, the origin is on the negative side, and if
w0 = 0, the hyperplane passes through the origin (see ﬁgure 10.1).
Take two points x 1 and x 2 both on the decision surface; that is, g(x 1 ) =
g(x 2 ) = 0, then
w T x 1 + w0

=

w T x 2 + w0

w T (x 1 − x 2 )

=

0

and we see that w is normal to any vector lying on the hyperplane. Let us
rewrite x as (Duda, Hart, and Stork 2001)
x = xp + r

w
w

where x p is the normal projection of x onto the hyperplane and r gives
us the distance from x to the hyperplane, negative if x is on the negative

214

x2

10 Linear Discrimination

g(x)=0
g(x)<0

g(x)>0

|w0|/||w||

x

w

|g(x)|/||w||
x1

Figure 10.2 The geometric interpretation of the linear discriminant.

side, and positive if x is on the positive side (see ﬁgure 10.2). Calculating
g(x) and noting that g(x p ) = 0, we have
(10.4)

r=

g(x)
w

We see then that the distance to origin is
(10.5)

r0 =

w0
w

Thus w0 determines the location of the hyperplane with respect to the
origin, and w determines its orientation.

10.3.2

Multiple Classes
When there are K > 2 classes, there are K discriminant functions. When
they are linear, we have

(10.6)

gi (x|w i , wi0 ) = w T x + wi0
i

215

x2

10.3 Geometry of the Linear Discriminant

H1

H2

+
C1

C2

H3

+

C3

+

x1
Figure 10.3 In linear classiﬁcation, each hyperplane Hi separates the examples
of Ci from the examples of all other classes. Thus for it to work, the classes
should be linearly separable. Dotted lines are the induced boundaries of the
linear classiﬁer.

We are going to talk about learning later on but for now, we assume
that the parameters, w i , wi0 , are computed so as to have
(10.7)

linearly separable
classes

(10.8)

gi (x|w i , wi0 ) =

>0
≤0

if x ∈ Ci
otherwise

for all x in the training set. Using such discriminant functions corresponds to assuming that all classes are linearly separable; that is, for
each class Ci , there exists a hyperplane Hi such that all x ∈ Ci lie on its
positive side and all x ∈ Cj , j = i lie on its negative side (see ﬁgure 10.3).
During testing, given x, ideally, we should have only one gj (x), j =
1, . . . , K greater than 0 and all others should be less than 0, but this is
not always the case: The positive half-spaces of the hyperplanes may
overlap, or, we may have a case where all gj (x) < 0. These may be taken
as reject cases, but the usual approach is to assign x to the class having
the highest discriminant:
Choose Ci if gi (x) = maxK gj (x)
j=1
Remembering that |gi (x)|/ w i is the distance from the input point to
the hyperplane, assuming that all w i have similar length, this assigns the

216

x2

10 Linear Discrimination

H12
C2

+
C1

+

+

H31

H23

C3
x1

Figure 10.4 In pairwise linear separation, there is a separate hyperplane for
each pair of classes. For an input to be assigned to C1 , it should be on the
positive side of H12 and H13 (which is the negative side of H31 ); we do not care
for the value of H23 . In this case, C1 is not linearly separable from other classes
but is pairwise linearly separable.

linear classifier

10.4

pairwise separation

point to the class (among all gj (x) > 0) to whose hyperplane the point is
most distant. This is called a linear classiﬁer, and geometrically it divides
the feature space into K convex decision regions Ri (see ﬁgure 10.3).

Pairwise Separation
If the classes are not linearly separable, one approach is to divide it into
a set of linear problems. One possibility is pairwise separation of classes
(Duda, Hart, and Stork 2001). It uses K(K − 1)/2 linear discriminants,
gij (x), one for every pair of distinct classes (see ﬁgure 10.4):
gij (x|w ij , wij0 ) = w T x + wij0
ij

(10.9)

The parameters w ij , j = i are computed during training so as to have
⎧
⎪ >0
if x ∈ Ci
⎨
≤0
if x ∈ Cj
gij (x) =
i, j = 1, . . . , K and i = j
⎪
⎩
don’t care otherwise
that is, if x t ∈ Ck where k = i, k = j, then x t is not used during training
of gij (x).

10.5 Parametric Discrimination Revisited

217

During testing, we
choose Ci if ∀j = i, gij (x) > 0
In many cases, this may not be true for any i and if we do not want
to reject such cases, we can relax the conjunction by using a summation
and choosing the maximum of
(10.10)

gi (x) =

gij (x)
j=i

Even if the classes are not linearly separable, if the classes are pairwise
linearly separable—which is much more likely—pairwise separation can
be used, leading to nonlinear separation of classes (see ﬁgure 10.4). This
is another example of breaking down a complex (e.g., nonlinear) problem,
into a set of simpler (e.g., linear) problems. We have already seen decision
trees (chapter 9) that use this idea, and we will see more examples of
this in chapter 17 on combining multiple models, for example, errorcorrecting output codes, and mixture of experts, where the number of
linear models is less than O(K 2 ).

10.5

Parametric Discrimination Revisited
In chapter 5, we saw that if the class densities, p(x|Ci ), are Gaussian and
share a common covariance matrix, the discriminant function is linear

(10.11)

gi (x) = w T x + wi0
i
where the parameters can be analytically calculated as
wi

(10.12)

=

wi0

=

Σ−1 μi
1
− μT Σ−1 μi + log P (Ci )
2 i

Given a dataset, we ﬁrst calculate the estimates for μi and Σ and then
plug the estimates, m i , S, in equation 10.12 and calculate the parameters
of the linear discriminant.
Let us again see the special case where there are two classes. We deﬁne
y ≡ P (C1 |x) and P (C2 |x) = 1 − y. Then in classiﬁcation, we
⎧
⎪ y > 0.5
⎪
⎨ y
>1
and C2 otherwise
choose C1 if
⎪ 1−y y
⎪
⎩ log
>0
1−y

218

10 Linear Discrimination

logit
log odds

log y/(1 − y) is known as the logit transformation or log odds of y. In
the case of two normal classes sharing a common covariance matrix, the
log odds is linear:
P (C1 |x)
P (C1 |x)
= log
1 − P (C1 |x)
P (C2 |x)
p(x|C1 )
P (C1 )
log
+ log
p(x|C2 )
P (C2 )

logit(P (C1 |x)) = log
=

P (C1 )
(2π )−d/2 |Σ|−1/2 exp[−(1/2)(x − μ1 )T Σ−1 (x − μ1 )]
+ log
P (C2 )
(2π )−d/2 |Σ|−1/2 exp[−(1/2)(x − μ2 )T Σ−1 (x − μ2 )]

=
(10.13)

log

=

w T x + w0

where
w
(10.14)

=

w0

=

Σ−1 (μ1 − μ2 )
1
P (C1 )
− (μ1 + μ2 )T Σ−1 (μ1 − μ2 ) + log
2
P (C2 )

The inverse of logit
log

P (C1 |x)
= w T x + w0
1 − P (C1 |x)

logistic
sigmoid

is the logistic function, also called the sigmoid function (see ﬁgure 10.5):

(10.15)

P (C1 |x) = sigmoid(w T x + w0 ) =

1
1 + exp [−(w T x + w0 )]

During training, we estimate m 1 , m2 , S and plug these estimates in
equation 10.14 to calculate the discriminant parameters. During testing,
given x, we can either
1. calculate g(x) = w T x + w0 and choose C1 if g(x) > 0, or
2. calculate y = sigmoid(w T x + w0 ) and choose C1 if y > 0.5,
because sigmoid(0) = 0.5. In this latter case, sigmoid transforms the
discriminant value to a posterior probability. This is valid when there
are two classes and one discriminant; we see in section 10.7 how we can
estimate posterior probabilities for K > 2.

10.6

Gradient Descent
In likelihood-based classiﬁcation, the parameters were the suﬃcient statistics of p(x|Ci ) and P (Ci ), and the method we used to estimate the parameters is maximum likelihood. In the discriminant-based approach,

219

10.6 Gradient Descent

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
−10

−8

−6

−4

−2

0

2

4

6

8

10

Figure 10.5 The logistic, or sigmoid, function.

the parameters are those of the discriminants, and they are optimized
to minimize the classiﬁcation error on the training set. When w denotes
the set of parameters and E(w|X) is the error with parameters w on the
given training set X, we look for
w ∗ = arg min E(w|X)
w

gradient descent
gradient vector

In many cases, some of which we will see shortly, there is no analytical
solution and we need to resort to iterative optimization methods, the
most commonly employed being that of gradient descent. When E(w) is
a diﬀerentiable function of a vector of variables, we have the gradient
vector composed of the partial derivatives
∇w E =

∂E ∂E
∂E
,
,...,
∂w1 ∂w2
∂wd

T

and the gradient descent procedure to minimize E starts from a random
w, and at each step, updates w, in the opposite direction of the gradient
(10.16)

Δwi

=

(10.17)

wi

=

∂E
, ∀i
∂wi
wi + Δwi
−η

where η is called the stepsize, or learning factor, and determines how
much to move in that direction. Gradient ascent is used to maximize a

220

10 Linear Discrimination

function and goes in the direction of the gradient. When we get to a minimum (or maximum), the derivative is 0 and the procedure terminates.
This indicates that the procedure ﬁnds the nearest minimum that can
be a local minimum, and there is no guarantee of ﬁnding the global minimum unless the function has only one minimum. The use of a good value
for η is also critical; if it is too small, the convergence may be too slow,
and a large value may cause oscillations and even divergence.
Throughout this book, we use gradient methods that are simple and
quite eﬀective. We keep in mind, however, that once a suitable model and
an error function is deﬁned, the optimization of the model parameters to
minimize the error function can be done by using one of many possible
techniques. There are second-order methods and conjugate gradient that
converge faster, at the expense of more memory and computation. More
costly methods like simulated annealing and genetic algorithms allow a
more thorough search of the parameter space and do not depend as much
on the initial point.

10.7
10.7.1
logistic
discrimination

(10.18)

Logistic Discrimination
Two Classes
In logistic discrimination, we do not model the class-conditional densities,
p(x|Ci ), but rather their ratio. Let us again start with two classes and
assume that the log likelihood ratio is linear:
log

p(x|C1 )
o
= w T x + w0
p(x|C2 )

This indeed holds when the class-conditional densities are normal (equation 10.13). But logistic discrimination has a wider scope of applicability;
for example, x may be composed of discrete attributes or may be a mixture of continuous and discrete attributes.
Using Bayes’ rule, we have
logit(P (C1 |x))

=
=

(10.19)

=

P (C1 |x)
1 − P (C1 |x)
p(x|C1 )
P (C1 )
log
+ log
p(x|C2 )
P (C2 )
log

w T x + w0

221

10.7 Logistic Discrimination

where
(10.20)

o
w0 = w0 + log

P (C1 )
P (C2 )

Rearranging terms, we get the sigmoid function again:
(10.21)

1
1 + exp[−(w T x + w0 )]

ˆ
y = P (C1 |x) =

as our estimator of P (C1 |x).
Let us see how we can learn w and w0 . We are given a sample of two
classes, X = {x t , r t }, where r t = 1 if x ∈ C1 and r t = 0 if x ∈ C2 .
We assume r t , given x t , is Bernoulli with probability y t ≡ P (C1 |x t ) as
calculated in equation 10.21:
r t |x t ∼ Bernoulli(y t )
Here, we see the diﬀerence from the likelihood-based methods where
we modeled p(x|Ci ); in the discriminant-based approach, we model directly r |x. The sample likelihood is
(10.22)

t

(y t )(r ) (1 − y t )(1−r

l(w, w0 |X) =

t)

t

cross-entropy

(10.23)

We know that when we have a likelihood function to maximize, we can
always turn it into an error function to be minimized as E = − log l, and
in our case, we have cross-entropy:
r t log y t + (1 − r t ) log(1 − y t )

E(w, w0 |X) = −
t

Because of the nonlinearity of the sigmoid function, we cannot solve directly and we use gradient descent to minimize cross-entropy, equivalent
to maximizing the likelihood or the log likelihood. If y = sigmoid(a) =
1/(1 + exp(−a)), its derivative is given as
dy
= y(1 − y)
da
and we get the following update equations:
Δwj

=
=

−η

∂E
=η
∂wj

t

rt
1 − rt
−
yt
1 − yt

(r t − y t )xt , j = 1, . . . , d
j

η
t

(10.24)

Δw0

=

−η

∂E
=η
∂w0

(r t − y t )
t

y t (1 − y t )xt
j

222

10 Linear Discrimination

For j = 0, . . . , d
wj ←rand(-0.01,0.01)
Repeat
For j = 0, . . . , d
Δwj ← 0
For t = 1, . . . , N
o←0
For j = 0, . . . , d
o ← o + wj xt
j
y ← sigmoid(o)
For j = 0, . . . , d
Δwj ← Δwj + (r t − y)xt
j
For j = 0, . . . , d
wj ← wj + ηΔwj
Until convergence
Figure 10.6 Logistic discrimination algorithm implementing gradient descent
for the single output case with two classes. For w0 , we assume that there is an
extra input x0 , which is always +1: xt ≡ +1, ∀t.
0

It is best to initialize wj with random values close to 0; generally they
are drawn uniformly from the interval [−0.01, 0.01]. The reason for this
is that if the initial wj are large in magnitude, the weighted sum may
also be large and may saturate the sigmoid. We see from ﬁgure 10.5
that if the initial weights are close to 0, the sum will stay in the middle
region where the derivative is nonzero and an update can take place. If
the weighted sum is large in magnitude (smaller than −5 or larger than
+5), the derivative of the sigmoid will be almost 0 and weights will not
be updated.
Pseudocode is given in ﬁgure 10.6. We see an example in ﬁgure 10.7
where the input is one-dimensional. Both the line w x + w0 and its value
after the sigmoid are shown as a function of learning iterations. We see
that to get outputs of 0 and 1, the sigmoid hardens, which is achieved by
increasing the magnitude of w , or w in the multivariate case.
Once training is complete and we have the ﬁnal w and w0 , during testing, given x t , we calculate y t = sigmoid(w T x t + w0 ) and we choose C1
if y t > 0.5 and choose C2 otherwise. This implies that to minimize the
number of misclassiﬁcations, we do not need to continue learning un-

223

10.7 Logistic Discrimination

3

2.5

1000
100

2
10
1.5

P(Co|x)

1

0.5

0

−0.5

−1

−1.5

−2

0

0.5

1

1.5

2

2.5
x

3

3.5

4

4.5

5

Figure 10.7 For a univariate two-class problem (shown with ‘◦’ and ‘×’ ), the
evolution of the line w x + w0 and the sigmoid output after 10, 100, and 1,000
iterations over the sample.

early stopping

til all y t are 0 or 1, but only until y t are less than or greater than 0.5,
that is, on the correct side of the decision boundary. If we do continue
training beyond this point, cross-entropy will continue decreasing (|wj |
will continue increasing to harden the sigmoid), but the number of misclassiﬁcations will not decrease. Generally, we continue training until the
number of misclassiﬁcations does not decrease (which will be 0 if the
classes are linearly separable). Actually stopping early before we have 0
training error is a form of regularization. Because we start with weights
almost 0 and they move away as training continues, stopping early corresponds to a model with more weights close to 0 and eﬀectively fewer
parameters.
Note that though we assumed the log ratio of the class densities are
linear to derive the discriminant, we estimate directly the posterior and
never explicitly estimate p(x|Ci ) or P (Ci ).

224

10 Linear Discrimination

10.7.2

Multiple Classes
Let us now generalize to K > 2 classes. We take one of the classes, for
example, CK , as the reference class and assume that

(10.25)

log

p(x|Ci )
o
= w T x + wi0
i
p(x|CK )

Then we have
(10.26)

P (Ci |x)
= exp[w T x + wi0 ]
i
P (CK |x)
o
with wi0 = wi0 + log P (Ci )/P (CK ).
We see that
K−1
i=1

P (Ci |x)
P (CK |x)

=
⇒

(10.27)

1 − P (CK |x)
=
P (CK |x)
P (CK |x) =

K−1

exp[w T x + wi0 ]
i
i=1

1
1+

K−1
i=1

exp[w T x + wi0 ]
i

and also that
P (Ci |x)
P (CK |x)
(10.28)

=

exp[w T x + wi0 ]
i

⇒

P (Ci |x) =

exp[w T x + wi0 ]
i
1+

K−1
T
j=1 exp[w j x

+ wj0 ]

, i = 1, . . . , K − 1

To treat all classes uniformly, we can write
(10.29)
softmax

(10.30)

ˆ
yi = P (Ci |x) =

exp[w T x + wi0 ]
i
K
T
j=1 exp[w j x

+ wj0 ]

, i = 1, . . . , K

which is called the softmax function (Bridle 1990). If the weighted sum
for one class is suﬃciently larger than for the others, after it is boosted
through exponentiation and normalization, its corresponding yi will be
close to 1 and the others will be close to 0. Thus it works like taking a
maximum, except that it is diﬀerentiable; hence the name softmax. Softmax also guarantees that i yi = 1.
Let us see how we can learn the parameters. In this case of K > 2
classes, each sample point is a multinomial trial with one draw; that is,
r t |x t ∼ Multk (1, y t ), where yit ≡ P (Ci |x t ). The sample likelihood is
t

(yit )ri

l({w i , wi0 }i |X) =
t

i

225

10.7 Logistic Discrimination

and the error function is again cross-entropy:
(10.31)

rit log yit

E({w i , wi0 }i |X) = −
t

i

We again use gradient descent. If yi = exp(ai )/
(10.32)

j

exp(aj ), we have

∂yi
= yi (δij − yj )
∂aj
where δij is the Kronecker delta, which is 1 if i = j and 0 if i = j (exercise 3). Given that i rit = 1, we have the following update equations, for
j = 1, . . . , K
Δw j

=

t

=

t
rit (δij − yj )x t
i

⎡

⎤

⎣

η
t

=

i

η
t

=

rit t
t
y (δij − yj )x t
yit i

η

rit ⎦ x t

t
rit δij − yj
i

i

t
t
(rj − yj )x t

η
t

(10.33)

Δwj0

=

t
t
(rj − yj )

η
t

Note that because of the normalization in softmax, w j and wj0 are affected not only by x t ∈ Cj but also by x t ∈ Ci , i = j. The discriminants
are updated so that the correct class has the highest weighted sum after softmax, and the other classes have their weighted sums as low as
possible. Pseudocode is given in ﬁgure 10.8. For a two-dimensional example with three classes, the contour plot is given in ﬁgure 10.9, and the
discriminants and the posterior probabilities in ﬁgure 10.10.
During testing, we calculate all yk , k = 1, . . . , K and choose Ci if yi =
maxk yk . Again we do not need to continue training to minimize crossentropy as much as possible; we train only until the correct class has
the highest weighted sum, and therefore we can stop training earlier by
checking the number of misclassiﬁcations.
When data are normally distributed, the logistic discriminant has a
comparable error rate to the parametric, normal-based linear discriminant (McLachlan 1992). Logistic discrimination can still be used when the
class-conditional densities are nonnormal or when they are not unimodal,
as long as classes are linearly separable.

226

10 Linear Discrimination

For i = 1, . . . , K, For j = 0, . . . , d, wij ← rand(−0.01, 0.01)
Repeat
For i = 1, . . . , K, For j = 0, . . . , d, Δwij ← 0
For t = 1, . . . , N
For i = 1, . . . , K
oi ← 0
For j = 0, . . . , d
oi ← oi + wij xt
j
For i = 1, . . . , K
yi ← exp(oi )/ k exp(ok )
For i = 1, . . . , K
For j = 0, . . . , d
Δwij ← Δwij + (rit − yi )xt
j
For i = 1, . . . , K
For j = 0, . . . , d
wij ← wij + ηΔwij
Until convergence
Figure 10.8 Logistic discrimination algorithm implementing gradient descent
for the case with K > 2 classes. For generality, we take xt ≡ 1, ∀t.
0

4

3.5

3

x2

2.5

2

1.5

1

0.5

0

0

0.5

1

1.5

2
x

2.5

3

3.5

4

1

Figure 10.9 For a two-dimensional problem with three classes, the solution
found by logistic discrimination. Thin lines are where gi (x) = 0, and the thick
line is the boundary induced by the linear classiﬁer choosing the maximum.

227

10.7 Logistic Discrimination

20

0

i

1 2

g (x ,x )

10

−10
−20

x

1

x2

1

i 1 2

P(C |x ,x )

0.8
0.6
0.4
0.2
0

x

1

x2

Figure 10.10 For the same example in ﬁgure 10.9, the linear discriminants (top),
and the posterior probabilities after the softmax (bottom).

228

10 Linear Discrimination

The ratio of class-conditional densities is of course not restricted to be
linear (Anderson 1982; McLachlan 1992). Assuming a quadratic discriminant, we have
(10.34)

log

p(x|Ci )
= x T Wi x + w T x + wi0
i
p(x|CK )

corresponding to and generalizing parametric discrimination with multivariate normal class-conditionals having diﬀerent covariance matrices.
When d is large, just as we can simplify (regularize) Σi , we can equally do
it on Wi by taking only its leading eigenvectors into account.
As discussed in section 10.2, any speciﬁed function of the basic variables can be included as x-variates. One can, for example, write the discriminant as a linear sum of nonlinear basis functions
(10.35)

log

p(x|Ci )
= w T φ(x) + wi0
i
p(x|CK )

where φ(·) are the basis functions, which can be viewed as transformed
variables. In neural network terminology, this is called a multilayer perceptron (chapter 11), and sigmoid is the most popular basis function.
When a Gaussian basis function is used, the model is called radial basis functions (chapter 12). We can even use a completely nonparametric
approach, for example, Parzen windows (chapter 8).

10.8

Discrimination by Regression
In regression, the probabilistic model is

(10.36)

rt = yt +
where ∼ N (0, σ 2 ). If r t ∈ {0, 1}, y t can be constrained to lie in this
range using the sigmoid function. Assuming a linear model and two
classes, we have

(10.37)

y t = sigmoid(w T x t + w0 ) =

1
1 + exp[−(w T x t + w0 )]

Then the sample likelihood in regression, assuming r |x ∼ N (y, σ 2 ), is
(10.38)

√

l(w, w0 |X) =
t

1
(r t − y t )2
exp −
2σ 2
2π σ

229

10.8 Discrimination by Regression

Maximizing the log likelihood is minimizing the sum of square errors:
(10.39)

E(w, w0 |X) =

1
2

(r t − y t )2
t

Using gradient descent, we get
Δw

=

(r t − y t )y t (1 − y t )x t

η
t

(10.40)

Δw0

=

(r t − y t )y t (1 − y t )

η
t

This method can also be used when there are K > 2 classes. The probabilistic model is
(10.41)

r t = yt +
where

(10.42)

∼ NK (0, σ 2 IK ). Assuming a linear model for each class, we have

yit = sigmoid(w T x t + wi0 ) =
i

1
1 + exp[−(w T x t + wi0 )]
i

Then the sample likelihood is
(10.43)

1

l({w i , wi0 }i |X) =
t

(2π )K/2 |Σ|1/2

exp −

r t − yt
2σ 2

2

and the error function is
(10.44)

E({w i , wi0 }i |X) =

1
2

r t − yt

2

=

t

1
2

(rit − yit )2
t

i

The update equations for i = 1, . . . , K, are
Δw i

=

(rit − yit )yit (1 − yit )x t

η
t

(10.45)

Δwi0

=

(rit − yit )yit (1 − yit )

η
t

But note that in doing so, we do not make use of the information that
only one of yi needs to be 1 and all others are 0, or that i yi = 1. The
softmax function of equation 10.29 allows us to incorporate this extra
information we have due to the outputs’ estimating class posterior probabilities. Using sigmoid outputs in K > 2 case, we treat yi as if they are
independent functions.
Note also that for a given class, if we use the regression approach, there
will be updates until the right output is 1 and all others are 0. This is not

230

10 Linear Discrimination

in fact necessary because during testing, we are just going to choose the
maximum anyway; it is enough to train only until the right output is
larger than others, which is exactly what the softmax function does.
So this approach with multiple sigmoid outputs is more appropriate
when the classes are not mutually exclusive and exhaustive. That is, for
an x t , all rit may be 0; namely, x t does not belong to any of the classes, or
more than one rit may be 1, when classes overlap.

10.9

generalized linear
models

10.10

Notes
The linear discriminant, due to its simplicity, is the classiﬁer most used
in pattern recognition (Duda, Hart, and Stork 2001; McLachlan 1992).
We discussed the case of Gaussian distributions with a common covariance matrix in chapter 4 and Fisher’s linear discriminant in chapter 6,
and in this chapter we discuss the logistic discriminant. In chapter 11,
we discuss the perceptron that is the neural network implementation of
the linear discriminant. In chapter 13, we will discuss support vector
machines, another type of linear discriminant.
Logistic discrimination is covered in more detail in Anderson 1982 and
in McLachlan 1992. Logistic (sigmoid) is the inverse of logit, which is the
canonical link in case of Bernoulli samples. Softmax is its generalization
to multinomial samples. More information on such generalized linear
models is given in McCullogh and Nelder 1989.
Generalizing linear models by using nonlinear basis functions is a very
old idea. We will discuss multilayer perceptrons (chapter 11) and radial
basis functions (chapter 12) where the parameters of the basis functions
can also be learned from data while learning the discriminant. Support
vector machines (chapter 13) use kernel functions built from such basis
functions.

Exercises
1. For each of the following basis functions, describe where it is nonzero:
a. sin(x1 )
b. exp(−(x1 − a)2 /c)
c. exp(− x − a
d. log(x2 )

2

/c)

231

10.11 References

e. 1(x1 > c)
f. 1(ax1 + bx2 > c)
2. For the two-dimensional case of ﬁgure 10.2, show equations 10.4 and 10.5.
3. Show that the derivative of the softmax, yi = exp(ai )/
yi (δij − yj ), where δij is 1 if i = j and 0 otherwise.

j

exp(aj ), is ∂yi /∂aj =

4. With K = 2, show that using two softmax outputs is equal to using one sigmoid output.
5. How can we learn Wi in equation 10.34?
6. In using quadratic (or higher-order) discriminants as in equation 10.34, how
can we keep variance under control?
7. What is the implication of the use of a single η for all xj in gradient descent?
8. In the univariate case for classiﬁcation as in ﬁgure 10.7, what do w and w0
correspond to?
9. Let us say for univariate x, x ∈ (2, 4) belong to C1 and x < 2 or x > 4 belong
to C2 . How can we separate the two classes using a linear discriminant?

10.11

References
Aizerman, M. A., E. M. Braverman, and L. I. Rozonoer. 1964. “Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning.”
Automation and Remote Control 25: 821–837.
Anderson, J. A. 1982. “Logistic Discrimination.” In Handbook of Statistics,
Vol. 2, Classiﬁcation, Pattern Recognition and Reduction of Dimensionality,
ed. P. R. Krishnaiah and L. N. Kanal, 169–191. Amsterdam: North Holland.
Bridle, J. S. 1990. “Probabilistic Interpretation of Feedforward Classiﬁcation
Network Outputs with Relationships to Statistical Pattern Recognition.” In
Neurocomputing: Algorithms, Architectures and Applications, ed. F. FogelmanSoulie and J. Herault, 227–236. Berlin: Springer.
Duda, R. O., P. E. Hart, and D. G. Stork. 2001. Pattern Classiﬁcation, 2nd ed.
New York: Wiley.
McCullagh, P., and J. A. Nelder. 1989. Generalized Linear Models. London:
Chapman and Hall.
McLachlan, G. J. 1992. Discriminant Analysis and Statistical Pattern Recognition.
New York: Wiley.
Vapnik, V. 1995. The Nature of Statistical Learning Theory. New York: Springer.

11

Multilayer Perceptrons

The multilayer perceptron is an artiﬁcial neural network structure
and is a nonparametric estimator that can be used for classiﬁcation
and regression. We discuss the backpropagation algorithm to train
a multilayer perceptron for a variety of applications.

11.1

artificial neural
networks

neurons

Introduction
A r t i f i c i a l n e u r a l network models, one of which is the perceptron
we discuss in this chapter, take their inspiration from the brain. There
are cognitive scientists and neuroscientists whose aim is to understand
the functioning of the brain (Posner 1989; Thagard 2005), and toward
this aim, build models of the natural neural networks in the brain and
make simulation studies.
However, in engineering, our aim is not to understand the brain per
se, but to build useful machines. We are interested in artiﬁcial neural
networks because we believe that they may help us build better computer
systems. The brain is an information processing device that has some
incredible abilities and surpasses current engineering products in many
domains—for example, vision, speech recognition, and learning, to name
three. These applications have evident economic utility if implemented
on machines. If we can understand how the brain performs these functions, we can deﬁne solutions to these tasks as formal algorithms and
implement them on computers.
The human brain is quite diﬀerent from a computer. Whereas a computer generally has one processor, the brain is composed of a very large
(1011 ) number of processing units, namely, neurons, operating in parallel.
Though the details are not known, the processing units are believed to be

234

11 Multilayer Perceptrons

synapses

11.1.1

levels of analysis

much simpler and slower than a processor in a computer. What also
makes the brain diﬀerent, and is believed to provide its computational
power, is the large connectivity. Neurons in the brain have connections,
called synapses, to around 104 other neurons, all operating in parallel.
In a computer, the processor is active and the memory is separate and
passive, but it is believed that in the brain, both the processing and memory are distributed together over the network; processing is done by the
neurons, and the memory is in the synapses between the neurons.

Understanding the Brain
According to Marr (1982), understanding an information processing system has three levels, called the levels of analysis:
1. Computational theory corresponds to the goal of computation and an
abstract deﬁnition of the task.
2. Representation and algorithm is about how the input and the output
are represented and about the speciﬁcation of the algorithm for the
transformation from the input to the output.
3. Hardware implementation is the actual physical realization of the system.
One example is sorting: The computational theory is to order a given
set of elements. The representation may use integers, and the algorithm
may be Quicksort. After compilation, the executable code for a particular
processor sorting integers represented in binary is one hardware implementation.
The idea is that for the same computational theory, there may be multiple representations and algorithms manipulating symbols in that representation. Similarly, for any given representation and algorithm, there
may be multiple hardware implementations. We can use one of various sorting algorithms, and even the same algorithm can be compiled
on computers with diﬀerent processors and lead to diﬀerent hardware
implementations.
To take another example, ‘6’, ‘VI’, and ‘110’ are three diﬀerent representations of the number six. There is a diﬀerent algorithm for addition
depending on the representation used. Digital computers use binary representation and have circuitry to add in this representation, which is one

11.1 Introduction

235

particular hardware implementation. Numbers are represented diﬀerently, and addition corresponds to a diﬀerent set of instructions on an
abacus, which is another hardware implementation. When we add two
numbers in our head, we use another representation and an algorithm
suitable to that representation, which is implemented by the neurons. But
all these diﬀerent hardware implementations—for example, us, abacus,
digital computer—implement the same computational theory, addition.
The classic example is the diﬀerence between natural and artiﬁcial ﬂying machines. A sparrow ﬂaps its wings; a commercial airplane does not
ﬂap its wings but uses jet engines. The sparrow and the airplane are
two hardware implementations built for diﬀerent purposes, satisfying
diﬀerent constraints. But they both implement the same theory, which is
aerodynamics.
The brain is one hardware implementation for learning or pattern recognition. If from this particular implementation, we can do reverse engineering and extract the representation and the algorithm used, and if
from that in turn, we can get the computational theory, we can then use
another representation and algorithm, and in turn a hardware implementation more suited to the means and constraints we have. One hopes our
implementation will be cheaper, faster, and more accurate.
Just as the initial attempts to build ﬂying machines looked very much
like birds until we discovered aerodynamics, it is also expected that the
ﬁrst attempts to build structures possessing brain’s abilities will look
like the brain with networks of large numbers of processing units, until
we discover the computational theory of intelligence. So it can be said
that in understanding the brain, when we are working on artiﬁcial neural
networks, we are at the representation and algorithm level.
Just as the feathers are irrelevant to ﬂying, in time we may discover
that neurons and synapses are irrelevant to intelligence. But until that
time there is one other reason why we are interested in understanding
the functioning of the brain, and that is related to parallel processing.

11.1.2

Neural Networks as a Paradigm for Parallel Processing
Since the 1980s, computer systems with thousands of processors have
been commercially available. The software for such parallel architectures,
however, has not advanced as quickly as hardware. The reason for this
is that almost all our theory of computation up to that point was based

236

parallel processing

11 Multilayer Perceptrons

on serial, one-processor machines. We are not able to use the parallel
machines we have eﬃciently because we cannot program them eﬃciently.
There are mainly two paradigms for parallel processing: In Single Instruction Multiple Data (SIMD) machines, all processors execute the same
instruction but on diﬀerent pieces of data. In Multiple Instruction Multiple Data (MIMD) machines, diﬀerent processors may execute diﬀerent
instructions on diﬀerent data. SIMD machines are easier to program because there is only one program to write. However, problems rarely have
such a regular structure that they can be parallelized over a SIMD machine. MIMD machines are more general, but it is not an easy task to write
separate programs for all the individual processors; additional problems
are related to synchronization, data transfer between processors, and so
forth. SIMD machines are also easier to build, and machines with more
processors can be constructed if they are SIMD. In MIMD machines, processors are more complex, and a more complex communication network
should be constructed for the processors to exchange data arbitrarily.
Assume now that we can have machines where processors are a little bit more complex than SIMD processors but not as complex as MIMD
processors. Assume we have simple processors with a small amount of
local memory where some parameters can be stored. Each processor implements a ﬁxed function and executes the same instructions as SIMD
processors; but by loading diﬀerent values into the local memory, they
can be doing diﬀerent things and the whole operation can be distributed
over such processors. We will then have what we can call Neural Instruction Multiple Data (NIMD) machines, where each processor corresponds
to a neuron, local parameters correspond to its synaptic weights, and the
whole structure is a neural network. If the function implemented in each
processor is simple and if the local memory is small, then many such
processors can be ﬁt on a single chip.
The problem now is to distribute a task over a network of such processors and to determine the local parameter values. This is where learning
comes into play: We do not need to program such machines and determine the parameter values ourselves if such machines can learn from
examples.
Thus, artiﬁcial neural networks are a way to make use of the parallel
hardware we can build with current technology and—thanks to learning—
they need not be programmed. Therefore, we also save ourselves the
eﬀort of programming them.
In this chapter, we discuss such structures and how they are trained.

11.2 The Perceptron

237

Figure 11.1 Simple perceptron. xj , j = 1, . . . , d are the input units. x0 is the
bias unit that always has the value 1. y is the output unit. wj is the weight of
the directed connection from input xj to the output.

Keep in mind that the operation of an artiﬁcial neural network is a mathematical function that can be implemented on a serial computer—as it
generally is—and training the network is not much diﬀerent from statistical techniques that we have discussed in the previous chapters. Thinking
of this operation as being carried out on a network of simple processing
units is meaningful only if we have the parallel hardware, and only if the
network is so large that it cannot be simulated fast enough on a serial
computer.

11.2
perceptron

connection weight
synaptic weight

The Perceptron
The perceptron is the basic processing element. It has inputs that may
come from the environment or may be the outputs of other perceptrons.
Associated with each input, xj ∈ , j = 1, . . . , d, is a connection weight,
or synaptic weight wj ∈ , and the output, y, in the simplest case is a
weighted sum of the inputs (see ﬁgure 11.1):
d

(11.1)

y=

wj xj + w0
j=1

bias unit

w0 is the intercept value to make the model more general; it is generally
modeled as the weight coming from an extra bias unit, x0 , which is always

238

11 Multilayer Perceptrons

+1. We can write the output of the perceptron as a dot product
(11.2)

y = wT x
where w = [w0 , w1 , . . . , wd ]T and x = [1, x1 , . . . , xd ]T are augmented vectors to include also the bias weight and input.
During testing, with given weights, w, for input x, we compute the
output y. To implement a given task, we need to learn the weights w, the
parameters of the system, such that correct outputs are generated given
the inputs.
When d = 1 and x is fed from the environment through an input unit,
we have
y = w x + w0

threshold function

(11.3)

which is the equation of a line with w as the slope and w0 as the intercept. Thus this perceptron with one input and one output can be used
to implement a linear ﬁt. With more than one input, the line becomes a
(hyper)plane, and the perceptron with more than one input can be used
to implement multivariate linear ﬁt. Given a sample, the parameters wj
can be found by regression (see section 5.8).
The perceptron as deﬁned in equation 11.1 deﬁnes a hyperplane and as
such can be used to divide the input space into two: the half-space where
it is positive and the half-space where it is negative (see chapter 10). By
using it to implement a linear discriminant function, the perceptron can
separate two classes by checking the sign of the output. If we deﬁne s(·)
as the threshold function
s(a) =

1
0

if a > 0
otherwise

then we can
choose

C1
C2

if s(w T x) > 0
otherwise

Remember that using a linear discriminant assumes that classes are
linearly separable. That is to say, it is assumed that a hyperplane w T x = 0
can be found that separates x t ∈ C1 and x t ∈ C2 . If at a later stage we
need the posterior probability—for example, to calculate risk—we need
to use the sigmoid function at the output as
o
(11.4)

=

wT x

y

=

sigmoid(o) =

1
1 + exp[−w T x]

11.2 The Perceptron

239

Figure 11.2 K parallel perceptrons. xj , j = 0, . . . , d are the inputs and yi , i =
1, . . . , K are the outputs. wij is the weight of the connection from input xj to
output yi . Each output is a weighted sum of the inputs. When used for K-class
classiﬁcation problem, there is a postprocessing to choose the maximum, or
softmax if we need the posterior probabilities.

When there are K > 2 outputs, there are K perceptrons, each of which
has a weight vector w i (see ﬁgure 11.2)
d

yi

wij xj + wi0 = w T x
i

=
j=1

(11.5)

y

=

Wx

where wij is the weight from input xj to output yi . W is the K × (d + 1)
weight matrix of wij whose rows are the weight vectors of the K perceptrons. When used for classiﬁcation, during testing, we
choose Ci if yi = max yk
k

In the case of a neural network, the value of each perceptron is a local
function of its inputs and its synaptic weights. However, in classiﬁcation,
if we need the posterior probabilities (instead of just the code of the
winner class) and use the softmax, we also need the values of the other
outputs. So, to implement this as a neural network, we can see this as
a two-stage process, where the ﬁrst stage calculates the weighted sums,
and the second stage calculates the softmax values; but we still denote

240

11 Multilayer Perceptrons

this as a single layer of output units:
oi
(11.6)

=

yi

=

wT x
i
exp oi
k exp ok

Remember that by deﬁning auxiliary inputs, the linear model can also
be used for polynomial approximation; for example, deﬁne x3 = x2 , x4 =
1
x2 , x5 = x1 x2 (section 10.2). The same can also be used with perceptrons
2
(Durbin and Rumelhart 1989). In section 11.5, we see multilayer perceptrons where such nonlinear functions are learned from data in a “hidden”
layer instead of being assumed a priori.
Any of the methods discussed in chapter 10 on linear discrimination
can be used to calculate w i , i = 1, . . . , K oﬄine and then plugged into the
network. These include parametric approach with a common covariance
matrix, logistic discrimination, discrimination by regression, and support
vector machines. In some cases, we do not have the whole sample at hand
when training starts, and we need to iteratively update parameters as new
examples arrive; we discuss this case of online learning in section 11.3.
Equation 11.5 deﬁnes a linear transformation from a d-dimensional
space to a K-dimensional space and can also be used for dimensionality reduction if K < d. One can use any of the methods of chapter 6 to
calculate W oﬄine and then use the perceptrons to implement the transformation, for example, PCA. In such a case, we have a two-layer network
where the ﬁrst layer of perceptrons implements the linear transformation
and the second layer implements the linear regression or classiﬁcation in
the new space. We note that because both are linear transformations,
they can be combined and written down as a single layer. We will see the
more interesting case where the ﬁrst layer implements nonlinear dimensionality reduction in section 11.5.

11.3

Training a Perceptron
The perceptron deﬁnes a hyperplane, and the neural network perceptron
is just a way of implementing the hyperplane. Given a data sample, the
weight values can be calculated oﬄine and then when they are plugged
in, the perceptron can be used to calculate the output values.
In training neural networks, we generally use online learning where we
are not given the whole sample, but we are given instances one by one
and would like the network to update its parameters after each instance,

11.3 Training a Perceptron

241

adapting itself slowly in time. Such an approach is interesting for a number of reasons:
1. It saves us the cost of storing the training sample in an external memory and storing the intermediate results during optimization. An approach like support vector machines (chapter 13) may be quite costly
with large samples, and in some applications, we may prefer a simpler
approach where we do not need to store the whole sample and solve a
complex optimization problem on it.
2. The problem may be changing in time, which means that the sample
distribution is not ﬁxed, and a training set cannot be chosen a priori.
For example, we may be implementing a speech recognition system
that adapts itself to its user.
3. There may be physical changes in the system. For example, in a robotic
system, the components of the system may wear out, or sensors may
degrade.
online learning

In online learning, we do not write the error function over the whole
sample but on individual instances. Starting from random initial weights,
at each iteration we adjust the parameters a little bit to minimize the
error, without forgetting what we have previously learned. If this error
function is diﬀerentiable, we can use gradient descent.
For example, in regression the error on the single instance pair with
index t, (xt , r t ), is
E t (w|x t , r t ) =

1 t
1
(r − y t )2 = [r t − (w T x t )]2
2
2

and for j = 0, . . . , d, the online update is
(11.7)

stochastic
gradient descent

t
Δwj = η(r t − y t )xt
j

where η is the learning factor, which is gradually decreased in time for
convergence. This is known as stochastic gradient descent.
Similarly, update rules can be derived for classiﬁcation problems using
logistic discrimination where updates are done after each pattern, instead
of summing them and doing the update after a complete pass over the
training set. With two classes, for the single instance (x t , r t ) where rit = 1
if x t ∈ C1 and rit = 0 if x t ∈ C2 , the single output is
y t = sigmoid(w T x t )

242

11 Multilayer Perceptrons

and the cross-entropy is
E t (w|x t , r t ) = −r t log y t − (1 − r t ) log(1 − y t )
Using gradient descent, we get the following online update rule for
j = 0, . . . , d:
(11.8)

t
Δwj = η(r t − y t )xt
j

rit

When there are K > 2 classes, for the single instance (x t , r t ) where
= 1 if x t ∈ Ci and 0 otherwise, the outputs are

yit =

exp w T x t
i
T t
k exp w k x

and the cross-entropy is
rit log yit

E t ({w i }i |x t , r t ) = −
i

Using gradient descent, we get the following online update rule, for
i = 1, . . . , K, j = 0, . . . , d:
(11.9)

t
Δwij = η(rit − yit )xt
j

which is the same as the equations we saw in section 10.7 except that we
do not sum over all of the instances but update after a single instance.
The pseudocode of the algorithm is given in ﬁgure 11.3, which is the
online version of ﬁgure 10.8.
Both equations 11.7 and 11.9 have the form
(11.10)

Update = LearningFactor· (DesiredOutput − ActualOutput) · Input
Let us try to get some insight into what this does. First, if the actual
output is equal to the desired output, no update is done. When it is
done, the magnitude of the update increases as the diﬀerence between
the desired output and the actual output increases. We also see that if
the actual output is less than the desired output, update is positive if
the input is positive and negative if the input is negative. This has the
eﬀect of increasing the actual output and decreasing the diﬀerence. If
the actual output is greater than the desired output, update is negative if
the input is positive and positive if the input is negative; this decreases
the actual output and makes it closer to the desired output.

11.4 Learning Boolean Functions

243

For i = 1, . . . , K
For j = 0, . . . , d
wij ← rand(−0.01, 0.01)
Repeat
For all (x t , r t ) ∈ X in random order
For i = 1, . . . , K
oi ← 0
For j = 0, . . . , d
oi ← oi + wij xt
j
For i = 1, . . . , K
yi ← exp(oi )/ k exp(ok )
For i = 1, . . . , K
For j = 0, . . . , d
wij ← wij + η(rit − yi )xt
j
Until convergence
Figure 11.3 Perceptron training algorithm implementing stochastic online gradient descent for the case with K > 2 classes. This is the online version of the
algorithm given in ﬁgure 10.8.

When an update is done, its magnitude depends also on the input. If
the input is close to 0, its eﬀect on the actual output is small and therefore its weight is also updated by a small amount. The greater an input,
the greater the update of its weight.
Finally, the magnitude of the update depends on the learning factor, η.
If it is too large, updates depend too much on recent instances; it is as if
the system has a very short memory. If this factor is small, many updates
may be needed for convergence. In section 11.8.1, we discuss methods to
speed up convergence.

11.4

Learning Boolean Functions
In a Boolean function, the inputs are binary and the output is 1 if the
corresponding function value is true and 0 otherwise. Therefore, it can
be seen as a two-class classiﬁcation problem. As an example, for learning
to AND two inputs, the table of inputs and required outputs is given in
table 11.1. An example of a perceptron that implements AND and its

244

11 Multilayer Perceptrons

Table 11.1 Input and output for the AND function.

x1
0
0
1
1

x2
0
1
0
1

r
0
0
0
1

x2
y

1.5

-1.5

(0,1)
+1

(1,1)

+1

+
x0=+1

x1

x2

(0,0)

(1,0)

1.5

x1

Figure 11.4 The perceptron that implements AND and its geometric interpretation.

geometric interpretation in two dimensions is given in ﬁgure 11.4. The
discriminant is
y = s(x1 + x2 − 1.5)
that is, x = [1, x1 , x2 ]T and w = [−1.5, 1, 1]T . Note that y = s(x1 +x2 −1.5)
satisﬁes the four constraints given by the deﬁnition of AND function in
table 11.1, for example, for x1 = 1, x2 = 0, y = s(−0.5) = 0. Similarly it
can be shown that y = s(x1 + x2 − 0.5) implements OR.
Though Boolean functions like AND and OR are linearly separable and
are solvable using the perceptron, certain functions like XOR are not. The
table of inputs and required outputs for XOR is given in table 11.2. As
can be seen in ﬁgure 11.5, the problem is not linearly separable. This
can also be proved by noting that there are no w0 , w1 , and w2 values that

245

11.5 Multilayer Perceptrons

Table 11.2 Input and output for the XOR function.

x1
0
0
1
1

x2
0
1
0
1

r
0
1
1
0

x2

x1
Figure 11.5 XOR problem is not linearly separable. We cannot draw a line where
the empty circles are on one side and the ﬁlled circles on the other side.

satisfy the following set of inequalities:
w2 +
w1 +
w1 +

w2 +

w0
w0
w0
w0

≤0
>0
>0
≤0

This result should not be very surprising to us since the VC dimension
of a line (in two dimensions) is three. With two binary inputs there are
four cases, and thus we know that there exist problems with two inputs
that are not solvable using a line; XOR is one of them.

11.5

Multilayer Perceptrons
A perceptron that has a single layer of weights can only approximate linear functions of the input and cannot solve problems like the XOR, where
the discrimininant to be estimated is nonlinear. Similarly, a perceptron

246

11 Multilayer Perceptrons

hidden layers
multilayer
perceptrons

(11.11)

cannot be used for nonlinear regression. This limitation does not apply
to feedforward networks with intermediate or hidden layers between the
input and the output layers. If used for classiﬁcation, such multilayer
perceptrons (MLP) can implement nonlinear discriminants and, if used
for regression, can approximate nonlinear functions of the input.
Input x is fed to the input layer (including the bias), the “activation”
propagates in the forward direction, and the values of the hidden units
zh are calculated (see ﬁgure 11.6). Each hidden unit is a perceptron by
itself and applies the nonlinear sigmoid function to its weighted sum:
zh = sigmoid(w T x) =
h

1
1 + exp −

d
j=1

whj xj + wh0

, h = 1, . . . , H

The output yi are perceptrons in the second layer taking the hidden
units as their inputs
H

(11.12)

yi = v T z =
i

vih zh + vi0
h=1

where there is also a bias unit in the hidden layer, which we denote by z0 ,
and vi0 are the bias weights. The input layer of xj is not counted since
no computation is done there and when there is a hidden layer, this is a
two-layer network.
As usual, in a regression problem, there is no nonlinearity in the output
layer in calculating y. In a two-class discrimination task, there is one sigmoid output unit and when there are K > 2 classes, there are K outputs
with softmax as the output nonlinearity.
If the hidden units’ outputs were linear, the hidden layer would be of no
use: linear combination of linear combinations is another linear combination. Sigmoid is the continuous, diﬀerentiable version of thresholding.
We need diﬀerentiability because the learning equations we will see are
gradient-based. Another sigmoid (S-shaped) nonlinear basis function that
can be used is the hyperbolic tangent function, tanh, which ranges from
−1 to +1, instead of 0 to +1. In practice, there is no diﬀerence between
using the sigmoid and the tanh. Still another possibility is the Gaussian,
which uses Euclidean distance instead of the dot product for similarity;
we discuss such radial basis function networks in chapter 12.
The output is a linear combination of the nonlinear basis function values computed by the hidden units. It can be said that the hidden units
make a nonlinear transformation from the d-dimensional input space to

11.5 Multilayer Perceptrons

247

Figure 11.6 The structure of a multilayer perceptron. xj , j = 0, . . . , d are the
inputs and zh , h = 1, . . . , H are the hidden units where H is the dimensionality
of this hidden space. z0 is the bias of the hidden layer. yi , i = 1, . . . , K are the
output units. whj are weights in the ﬁrst layer, and vih are the weights in the
second layer.

the H-dimensional space spanned by the hidden units, and, in this space,
the second output layer implements a linear function.
One is not limited to having one hidden layer, and more hidden layers
with their own incoming weights can be placed after the ﬁrst hidden layer
with sigmoid hidden units, thus calculating nonlinear functions of the
ﬁrst layer of hidden units and implementing more complex functions of
the inputs. In practice, people rarely go beyond one hidden layer since
analyzing a network with many hidden layers is quite complicated; but
sometimes when the hidden layer contains too many hidden units, it may
be sensible to go to multiple hidden layers, preferring “long and narrow”
networks to “short and fat” networks.

248

11 Multilayer Perceptrons

11.6

MLP as a Universal Approximator
We can represent any Boolean function as a disjunction of conjunctions,
and such a Boolean expression can be implemented by a multilayer perceptron with one hidden layer. Each conjunction is implemented by one
hidden unit and the disjunction by the output unit. For example,
x1 XOR x2 = (x1 AND ∼ x2 ) OR (∼ x1 AND x2 )

universal
approximation

piecewise constant
approximation

We have seen previously how to implement AND and OR using perceptrons. So two perceptrons can in parallel implement the two AND, and
another perceptron on top can OR them together (see ﬁgure 11.7). We see
that the ﬁrst layer maps inputs from the (x1 , x2 ) to the (z1 , z2 ) space deﬁned by the ﬁrst-layer perceptrons. Note that both inputs, (0,0) and (1,1),
are mapped to (0,0) in the (z1 , z2 ) space, allowing linear separability in
this second space.
Thus in the binary case, for every input combination where the output
is 1, we deﬁne a hidden unit that checks for that particular conjunction of
the input. The output layer then implements the disjunction. Note that
this is just an existence proof, and such networks may not be practical
as up to 2d hidden units may be necessary when there are d inputs. Such
an architecture implements table lookup and does not generalize.
We can extend this to the case where inputs are continuous to show
that similarly, any arbitrary function with continuous input and outputs
can be approximated with a multilayer perceptron. The proof of universal
approximation is easy with two hidden layers. For every input case or
region, that region can be delimited by hyperplanes on all sides using
hidden units on the ﬁrst hidden layer. A hidden unit in the second layer
then ANDs them together to bound the region. We then set the weight
of the connection from that hidden unit to the output unit equal to the
desired function value. This gives a piecewise constant approximation
of the function; it corresponds to ignoring all the terms in the Taylor
expansion except the constant term. Its accuracy may be increased to
the desired value by increasing the number of hidden units and placing
a ﬁner grid on the input. Note that no formal bounds are given on the
number of hidden units required. This property just reassures us that
there is a solution; it does not help us in any other way. It has been proven
that an MLP with one hidden layer (with an arbitrary number of hidden
units) can learn any nonlinear function of the input (Hornik, Stinchcombe,
and White 1989).

249

11.7 Backpropagation Algorithm

y

-0.5

z2

y

+1

+1

+

z0=+1 -0.5

-0.5

+1 -1

z1

z2
-1 +1

x2

+

z1

z2
z1

+

x0=+1

x1

x2

x1

Figure 11.7 The multilayer perceptron that solves the XOR problem. The hidden units and the output have the threshold activation function with threshold
at 0.

11.7

Backpropagation Algorithm
Training a multilayer perceptron is the same as training a perceptron;
the only diﬀerence is that now the output is a nonlinear function of the
input thanks to the nonlinear basis function in the hidden units. Considering the hidden units as inputs, the second layer is a perceptron and
we already know how to update the parameters, vij , in this case, given
the inputs zh . For the ﬁrst-layer weights, whj , we use the chain rule to
calculate the gradient:
∂E
∂E ∂yi ∂zh
=
∂whj
∂yi ∂zh ∂whj

250

11 Multilayer Perceptrons

backpropagation

11.7.1

It is as if the error propagates from the output y back to the inputs
and hence the name backpropagation was coined (Rumelhart, Hinton, and
Williams 1986a).

Nonlinear Regression
Let us ﬁrst take the case of nonlinear regression (with a single output)
calculated as
H

(11.13)

t
vh zh + v0

yt =
h=1

with zh computed by equation 11.11. The error function over the whole
sample in regression is
(11.14)

E(W, v|X) =

1
2

(r t − y t )2
t

The second layer is a perceptron with hidden units as the inputs, and
we use the least-squares rule to update the second-layer weights:
(11.15)

t
(r t − y t )zh

Δvh = η
t

The ﬁrst layer are also perceptrons with the hidden units as the output
units but in updating the ﬁrst-layer weights, we cannot use the leastsquares rule directly as we do not have a desired output speciﬁed for the
hidden units. This is where the chain rule comes into play. We write
Δwhj

=

−η

=

∂E
∂whj

−η
t

=

−(r t − y t )

−η
t

(11.16)

=

t
∂E t ∂y t ∂zh
t
∂y t ∂zh ∂whj

vh

t
t
zh (1 − zh )xt
j

∂E t /∂y t

t
∂y t /∂zh

t
∂zh /∂whj

t
t
(r t − y t )vh zh (1 − zh )xt
j

η
t

The product of the ﬁrst two terms (r t −y t )vh acts like the error term for
hidden unit h. This error is backpropagated from the error to the hidden
unit. (r t − y t ) is the error in the output, weighted by the “responsibility”
of the hidden unit as given by its weight vh . In the third term, zh (1 − zh )

11.7 Backpropagation Algorithm

batch learning

epoch

is the derivative of the sigmoid and xt is the derivative of the weighted
j
sum with respect to the weight whj . Note that the change in the ﬁrstlayer weight, Δwhj , makes use of the second-layer weight, vh . Therefore,
we should calculate the changes in both layers and update the ﬁrst-layer
weights, making use of the old value of the second-layer weights, then
update the second-layer weights.
Weights, whj , vh are started from small random values initially, for example, in the range [−0.01, 0.01], so as not to saturate the sigmoids. It is
also a good idea to normalize the inputs so that they all have 0 mean and
unit variance and have the same scale, since we use a single η parameter.
With the learning equations given here, for each pattern, we compute
the direction in which each parameter needs be changed and the magnitude of this change. In batch learning, we accumulate these changes over
all patterns and make the change once after a complete pass over the
whole training set is made, as shown in the previous update equations.
It is also possible to have online learning, by updating the weights after each pattern, thereby implementing stochastic gradient descent. A
complete pass over all the patterns in the training set is called an epoch.
The learning factor, η, should be chosen smaller in this case and patterns
should be scanned in a random order. Online learning converges faster
because there may be similar patterns in the dataset, and the stochasticity has an eﬀect like adding noise and may help escape local minima.
An example of training a multilayer perceptron for regression is shown
in ﬁgure 11.8. As training continues, the MLP ﬁt gets closer to the underlying function and error decreases (see ﬁgure 11.9). Figure 11.10 shows
how the MLP ﬁt is formed as a sum of the outputs of the hidden units.
It is also possible to have multiple output units, in which case a number
of regression problems are learned at the same time. We have
H

(11.17)

yit =

t
vih zh + vi0
h=1

and the error is
(11.18)

E(W, V|X) =

1
2

(rit − yit )2
t

i

The batch update rules are then
(11.19)

251

Δvih

=

t
(rit − yit )zh

η
t

252

11 Multilayer Perceptrons

2

1.5

1

0.5
300

200

0
100
−0.5

−1

−1.5

−2
−0.5

−0.4

−0.3

−0.2

−0.1

0

0.1

0.2

0.3

0.4

0.5

Figure 11.8 Sample training data shown as ‘+’, where xt ∼ U (−0.5, 0.5), and
y t = f (xt ) + N (0, 0.1). f (x) = sin(6x) is shown by a dashed line. The evolution
of the ﬁt of an MLP with two hidden units after 100, 200, and 300 epochs is
drawn.

⎡
(11.20)

Δwhj

=

⎤

⎣

η
t

t
t
(rit − yit )vih ⎦ zh (1 − zh )xt
j
i

t
i (ri

− yit )vih is the accumulated backpropagated error of hidden unit
h from all output units. Pseudocode is given in ﬁgure 11.11. Note that in
this case, all output units share the same hidden units and thus use the
same hidden representation, hence, we are assuming that corresponding to these diﬀerent outputs, we have related prediction problems. An
alternative is to train separate multilayer perceptrons for the separate
regression problems, each with its own separate hidden units.

11.7.2

Two-Class Discrimination
When there are two classes, one output unit suﬃces:
⎞
⎛

(11.21)

y t = sigmoid ⎝

H

h=1

t
vh zh + v0 ⎠

253

11.7 Backpropagation Algorithm

1.4
Training
Validation
1.2

Mean Square Error

1

0.8

0.6

0.4

0.2

0

0

50

100

150
Training Epochs

200

250

300

Figure 11.9 The mean square error on training and validation sets as a function
of training epochs.

ˆ
which approximates P (C1 |x t ) and P (C2 |x t ) ≡ 1 − y t . We remember from
section 10.7 that the error function in this case is
(11.22)

r t log y t + (1 − r t ) log(1 − y t )

E(W, v|X) = −
t

The update equations implementing gradient descent are
(11.23)

Δvh

=

(11.24)

Δwhj

=

t
(r t − y t )zh

η
η

t
t
t
(r t − y t )vh zh (1 − zh )xt
j
t

As in the simple perceptron, the update equations for regression and
classiﬁcation are identical (which does not mean that the values are).

254

11 Multilayer Perceptrons

4

4

4

3

3

3

2

2

2

1

1

1

0

0

0

−1

−1

−1

−2

−2

−2

−3

−3

−3

−4
−0.5

0

0.5

−4
−0.5

0

0.5

−4
−0.5

0

0.5

Figure 11.10 (a) The hyperplanes of the hidden unit weights on the ﬁrst layer,
(b) hidden unit outputs, and (c) hidden unit outputs multiplied by the weights on
the second layer. Two sigmoid hidden units slightly displaced, one multiplied
by a negative weight, when added, implement a bump. With more hidden units,
a better approximation is attained (see ﬁgure 11.12).

11.7.3

Multiclass Discrimination
In a (K > 2)-class classiﬁcation problem, there are K outputs
H

(11.25)

ot =
i

t
vih zh + vi0
h=1

and we use softmax to indicate the dependency between classes; namely,
they are mutually exclusive and exhaustive:
(11.26)

yit =

exp ot
i
t
k exp ok

255

11.7 Backpropagation Algorithm

Initialize all vih and whj to rand(−0.01, 0.01)
Repeat
For all (x t , r t ) ∈ X in random order
For h = 1, . . . , H
zh ← sigmoid(w T x t )
h
For i = 1, . . . , K
yi = v T z
i
For i = 1, . . . , K
Δv i = η(rit − yit )z
For h = 1, . . . , H
Δw h = η( i (rit − yit )vih )zh (1 − zh )x t
For i = 1, . . . , K
v i ← v i + Δv i
For h = 1, . . . , H
w h ← w h + Δw h
Until convergence
Figure 11.11 Backpropagation algorithm for training a multilayer perceptron
for regression with K outputs. This code can easily be adapted for two-class
classiﬁcation (by setting a single sigmoid output) and to K > 2 classiﬁcation (by
using softmax outputs).

where yi approximates P (Ci |x t ). The error function is
(11.27)

rit log yit

E(W, V|X) = −
t

i

and we get the update equations using gradient descent:
(11.28)

Δvih

=

t
(rit − yit )zh

η
t

(11.29)

Δwhj

=

⎡

⎤

⎣

η
t

(rit

−

t
yit )vih ⎦ zh (1

t
− zh )xt
j

i

Richard and Lippmann (1991) have shown that given a network of
enough complexity and suﬃcient training data, a suitably trained multilayer perceptron estimates posterior probabilities.

256

11 Multilayer Perceptrons

11.7.4

Multiple Hidden Layers
As we saw before, it is possible to have multiple hidden layers each with
its own weights and applying the sigmoid function to its weighted sum.
For regression, let us say, if we have a multilayer perceptron with two
hidden layers, we write
⎛
z1h

=

sigmoid(w T x)
1h

d

= sigmoid ⎝

⎞
w1hj xj + w1h0 ⎠ , h = 1, . . . , H1

j=1

⎛
z2l

=

sigmoid(w T z 1 )
2l

= sigmoid ⎝

H1

⎞
w2lh z1h + w2l0 ⎠ , l = 1, . . . , H2

h=0
H2

y

=

v T z2 =

vl z2l + v0
l=1

where w 1h and w 2l are the ﬁrst- and second-layer weights, z1h and z2h
are the units on the ﬁrst and second hidden layers, and v are the thirdlayer weights. Training such a network is similar except that to train the
ﬁrst-layer weights, we need to backpropagate one more layer (exercise 5).

11.8
11.8.1

Training Procedures
Improving Convergence
Gradient descent has various advantages. It is simple. It is local; namely,
the change in a weight uses only the values of the presynaptic and postsynaptic units and the error (suitably backpropagated). When online training is used, it does not need to store the training set and can adapt as
the task to be learned changes. Because of these reasons, it can be (and
is) implemented in hardware. But by itself, gradient descent converges
slowly. When learning time is important, one can use more sophisticated
optimization methods (Battiti 1992). Bishop (1995) discusses in detail
the application of conjugate gradient and second-order methods to the
training of multilayer perceptrons. However, there are two frequently
used simple techniques that improve the performance of the gradient
descent considerably, making gradient-based methods feasible in real applications.

11.8 Training Procedures

257

Momentum

momentum

(11.30)

Let us say wi is any weight in a multilayer perceptron in any layer, including the biases. At each parameter update, successive Δwit values may be
so diﬀerent that large oscillations may occur and slow convergence. t is
the time index that is the epoch number in batch learning and the iteration number in online learning. The idea is to take a running average by
incorporating the previous update in the current change as if there is a
momentum due to previous updates:
Δwit = −η

∂E t
+ αΔwit−1
∂wi

α is generally taken between 0.5 and 1.0. This approach is especially
useful when online learning is used, where as a result we get an eﬀect of
averaging and smooth the trajectory during convergence. The disadvantage is that the past Δwit−1 values should be stored in extra memory.
Adaptive Learning Rate
In gradient descent, the learning factor η determines the magnitude of
change to be made in the parameter. It is generally taken between 0.0
and 1.0, mostly less than or equal to 0.2. It can be made adaptive for
faster convergence, where it is kept large when learning takes place and
is decreased when learning slows down:
(11.31)

Δη =

+a
−bη

if E t+τ < E t
otherwise

Thus we increase η by a constant amount if the error on the training set
decreases and decrease it geometrically if it increases. Because E may
oscillate from one epoch to another, it is a better idea to take the average
of the past few epochs as E t .

11.8.2

Overtraining
A multilayer perceptron with d inputs, H hidden units, and K outputs
has H(d + 1) weights in the ﬁrst layer and K(H + 1) weights in the second
layer. Both the space and time complexity of an MLP is O(H · (K + d)).
When e denotes the number of training epochs, training time complexity
is O(e · H · (K + d)).

258

11 Multilayer Perceptrons

early stopping
overtraining

11.8.3

In an application, d and K are predeﬁned and H is the parameter that
we play with to tune the complexity of the model. We know from previous chapters that an overcomplex model memorizes the noise in the
training set and does not generalize to the validation set. For example,
we have previously seen this phenomenon in the case of polynomial regression where we noticed that in the presence of noise or small samples,
increasing the polynomial order leads to worse generalization. Similarly
in an MLP, when the number of hidden units is large, the generalization
accuracy deteriorates (see ﬁgure 11.12), and the bias/variance dilemma
also holds for the MLP, as it does for any statistical estimator (Geman,
Bienenstock, and Doursat 1992).
A similar behavior happens when training is continued too long: as
more training epochs are made, the error on the training set decreases,
but the error on the validation set starts to increase beyond a certain
point (see ﬁgure 11.13). Remember that initially all the weights are close
to 0 and thus have little eﬀect. As training continues, the most important weights start moving away from 0 and are utilized. But if training is
continued further on to get less and less error on the training set, almost
all weights are updated away from 0 and eﬀectively become parameters.
Thus as training continues, it is as if new parameters are added to the system, increasing the complexity and leading to poor generalization. Learning should be stopped early to alleviate this problem of overtraining. The
optimal point to stop training, and the optimal number of hidden units,
is determined through cross-validation, which involves testing the network’s performance on validation data unseen during training.
Because of the nonlinearity, the error function has many minima and
gradient descent converges to the nearest minimum. To be able to assess
expected error, the same network is trained a number of times starting from diﬀerent initial weight values, and the average of the validation
error is computed.

Structuring the Network
In some applications, we may believe that the input has a local structure.
For example, in vision we know that nearby pixels are correlated and
there are local features like edges and corners; any object, for example,
a handwritten digit, may be deﬁned as a combination of such primitives.
Similarly, in speech, locality is in time and inputs close in time can be
grouped as speech primitives. By combining these primitives, longer ut-

259

11.8 Training Procedures

0.12
Training
Validation

0.1

Mean Square Error

0.08

0.06

0.04

0.02

0

0

5

10

15
Number of Hidden Units

20

25

30

Figure 11.12 As complexity increases, training error is ﬁxed but the validation
error starts to increase and the network starts to overﬁt.
3.5
Training
Validation
3

Mean Square Error

2.5

2

1.5

1

0.5

0

0

100

200

300

400

500
600
Training Epochs

700

800

900

1000

Figure 11.13 As training continues, the validation error starts to increase and
the network starts to overﬁt.

260

11 Multilayer Perceptrons

Figure 11.14 A structured MLP. Each unit is connected to a local group of units
below it and checks for a particular feature—for example, edge, corner, and so
forth—in vision. Only one hidden unit is shown for each region; typically there
are many to check for diﬀerent local features.

hierarchical cone

weight sharing

terances, for example, speech phonemes, may be deﬁned. In such a case
when designing the MLP, hidden units are not connected to all input units
because not all inputs are correlated. Instead, we deﬁne hidden units that
deﬁne a window over the input space and are connected to only a small
local subset of the inputs. This decreases the number of connections and
therefore the number of free parameters (Le Cun et al. 1989).
We can repeat this in successive layers where each layer is connected
to a small number of local units below and checks for a more complicated feature by combining the features below in a larger part of the
input space until we get to the output layer (see ﬁgure 11.14). For example, the input may be pixels. By looking at pixels, the ﬁrst hidden
layer units may learn to check for edges of various orientations. Then
by combining edges, the second hidden layer units can learn to check for
combinations of edges—for example, arcs, corners, line ends—and then
combining them in upper layers, the units can look for semi-circles, rectangles, or in the case of a face recognition application, eyes, mouth, and
so forth. This is the example of a hierarchical cone where features get
more complex, abstract, and fewer in number as we go up the network
until we get to classes.
In such a case, we can further reduce the number of parameters by
weight sharing. Taking the example of visual recognition again, we can
see that when we look for features like oriented edges, they may be

11.8 Training Procedures

261

Figure 11.15 In weight sharing, diﬀerent units have connections to diﬀerent
inputs but share the same weight value (denoted by line type). Only one set of
units is shown; there should be multiple sets of units, each checking for diﬀerent
features.

present in diﬀerent parts of the input space. So instead of deﬁning independent hidden units learning diﬀerent features in diﬀerent parts of
the input space, we can have copies of the same hidden units looking at
diﬀerent parts of the input space (see ﬁgure 11.15). During learning, we
calculate the gradients by taking diﬀerent inputs, then we average these
up and make a single update. This implies a single parameter that deﬁnes the weight on multiple connections. Also, because the update on a
weight is based on gradients for several inputs, it is as if the training set
is eﬀectively multiplied.

11.8.4

hints

Hints
The knowledge of local structure allows us to prestructure the multilayer
network, and with weight sharing it has fewer parameters. The alternative of an MLP with completely connected layers has no such structure
and is more diﬃcult to train. Knowledge of any sort related to the application should be built into the network structure whenever possible.
These are called hints (Abu-Mostafa 1995) and are the properties of the
target function that are known to us independent of the training examples.
In image recognition, there are invariance hints: the identity of an object does not change when it is rotated, translated, or scaled (see ﬁgure 11.16). Hints are auxiliary information that can be used to guide the
learning process and are especially useful when the training set is limited.
There are diﬀerent ways in which hints can be used:

262

11 Multilayer Perceptrons

A

A

A

A

Figure 11.16 The identity of the object does not change when it is translated,
rotated, or scaled. Note that this may not always be true, or may be true up to a
point: ‘b’ and ‘q’ are rotated versions of each other. These are hints that can be
incorporated into the learning process to make learning easier.

virtual examples

1. Hints can be used to create virtual examples. For example, knowing
that the object is invariant to scale, from a given training example,
we can generate multiple copies at diﬀerent scales and add them to
the training set with the same label. This has the advantage that we
increase the training set and do not need to modify the learner in any
way. The problem may be that too many examples may be needed for
the learner to learn the invariance.
2. The invariance may be implemented as a preprocessing stage. For
example, optical character readers have a preprocessing stage where
the input character image is centered and normalized for size and
slant. This is the easiest solution, when it is possible.
3. The hint may be incorporated into the network structure. Local structure and weight sharing, which we saw in section 11.8.3, is one example where we get invariance to small translations and rotations.
4. The hint may also be incorporated by modifying the error function. Let
us say we know that x and x are the same from the application’s point
of view, where x may be a “virtual example” of x. That is, f (x) = f (x ),
when f (x) is the function we would like to approximate. Let us denote
by g(x|θ), our approximation function, for example, an MLP where θ
are its weights. Then, for all such pairs (x, x ), we deﬁne the penalty
function
Eh = g(x|θ) − g(x |θ)

2

and add it as an extra term to the usual error function:
E = E + λh · Eh

11.9 Tuning the Network Size

263

This is a penalty term penalizing the cases where our predictions do
not obey the hint, and λh is the weight of such a penalty (Abu-Mostafa
1995).
Another example is the approximation hint: Let us say that for x, we
do not know the exact value, f (x), but we know that it is in the interval,
[ax , bx ]. Then our added penalty term is
⎧
⎪ 0
⎨
(g(x) − ax )2
Eh =
⎪
⎩
(g(x) − bx )2

if g(x|θ) ∈ [ax , bx ]
if g(x|θ) < ax
if g(x|θ) > bx

This is similar to the error function used in support vector regression
(section 13.10), which tolerates small approximation errors.
tangent prop

11.9

structural
adaptation

Still another example is the tangent prop (Simard et al. 1992) where
the transformation against which we are deﬁning the hint—for example, rotation by an angle—is modeled by a function. The usual error
function is modiﬁed (by adding another term) so as to allow parameters to move along this line of transformation without changing the
error.

Tuning the Network Size
Previously we saw that when the network is too large and has too many
free parameters, generalization may not be well. To ﬁnd the optimal
network size, the most common approach is to try many diﬀerent architectures, train them all on the training set, and choose the one that
generalizes best to the validation set. Another approach is to incorporate
this structural adaptation into the learning algorithm. There are two ways
this can be done:
1. In the destructive approach, we start with a large network and gradually remove units and/or connections that are not necessary.
2. In the constructive approach, we start with a small network and gradually add units and/or connections to improve performance.

weight decay

One destructive method is weight decay where the idea is to remove unnecessary connections. Ideally to be able to determine whether a unit or
connection is necessary, we need to train once with and once without and

264

11 Multilayer Perceptrons

check the diﬀerence in error on a separate validation set. This is costly
since it should be done for all combinations of such units/connections.
Given that a connection is not used if its weight is 0, we give each
connection a tendency to decay to 0 so that it disappears unless it is
reinforced explicitly to decrease error. For any weight wi in the network,
we use the update rule:
(11.32)

Δwi = −η

∂E
− λwi
∂wi

This is equivalent to doing gradient descent on the error function with
an added penalty term, penalizing networks with many nonzero weights:
(11.33)

dynamic node
creation

cascade
correlation

E =E+

λ
2

wi2
i

Simpler networks are better generalizers is a hint that we implement by
adding a penalty term. Note that we are not saying that simple networks
are always better than large networks; we are saying that if we have two
networks that have the same training error, the simpler one—namely, the
one with fewer weights—has a higher probability of better generalizing
to the validation set.
The eﬀect of the second term in equation 11.32 is like that of a spring
that pulls each weight to 0. Starting from a value close to 0, unless the
actual error gradient is large and causes an update, due to the second
term, the weight will gradually decay to 0. λ is the parameter that determines the relative importances of the error on the training set and the
complexity due to nonzero parameters and thus determines the speed of
decay: With large λ, weights will be pulled to 0 no matter what the training error is; with small λ, there is not much penalty for nonzero weights.
λ is ﬁne-tuned using cross-validation.
Instead of starting from a large network and pruning unnecessary connections or units, one can start from a small network and add units and
associated connections should the need arise (ﬁgure 11.17). In dynamic
node creation (Ash 1989), an MLP with one hidden layer with one hidden
unit is trained and after convergence, if the error is still high, another
hidden unit is added. The incoming weights of the newly added unit and
its outgoing weight are initialized randomly and trained with the previously existing weights that are not reinitialized and continue from their
previous values.
In cascade correlation (Fahlman and Lebiere 1990), each added unit

265

11.9 Tuning the Network Size

Dynamic Node Creation

Cascade Correlation

Figure 11.17 Two examples of constructive algorithms. Dynamic node creation
adds a unit to an existing layer. Cascade correlation adds each unit as a new
hidden layer connected to all the previous layers. Dashed lines denote the newly
added unit/connections. Bias units/weights are omitted for clarity.

is a new hidden unit in another hidden layer. Every hidden layer has
only one unit that is connected to all of the hidden units preceding it
and the inputs. The previously existing weights are frozen and are not
trained; only the incoming and outgoing weights of the newly added unit
are trained.
Dynamic node creation adds a new hidden unit to an existing hidden
layer and never adds another hidden layer. Cascade correlation always
adds a new hidden layer with a single unit. The ideal constructive method
should be able to decide when to introduce a new hidden layer and when
to add a unit to an existing layer. This is an open research problem.
Incremental algorithms are interesting because they correspond to modifying not only the parameters but also the model structure during learning. We can think of a space deﬁned by the structure of the multilayer
perceptron and operators corresponding to adding/removing unit(s) or
layer(s) to move in this space (Aran et al. 2009). Incremental algorithms
then do a search in this state space where operators are tried (according
to some order) and accepted or rejected depending on some goodness
measure, for example, some combination of complexity and validation error. Another example would be a setting in polynomial regression where

266

11 Multilayer Perceptrons

high-order terms are added/removed during training automatically, ﬁtting model complexity to data complexity. As the cost of computation
gets lower, such automatic model selection should be a part of the learning process done automatically without any user interference.

11.10

Bayesian View of Learning
The Bayesian approach in training neural networks considers the parameters, namely, connection weights, wi , as random variables drawn from
a prior distribution p(wi ) and computes the posterior probability given
the data

(11.34)

p(w|X) =

p(X|w)p(w)
p(X)

where w is the vector of all weights of the network. The MAP estimate w
ˆ
is the mode of the posterior
(11.35)

w MAP = arg max log p(w|X)
ˆ
w
Taking the log of equation 11.34, we get
log p(w|X) = log p(X|w) + log p(w) + C
The ﬁrst term on the right is the log likelihood, and the second is the
log of the prior. If the weights are independent and the prior is taken as
Gaussian, N (0, 1/2λ)

(11.36)

p(w) =

p(wi ) where p(wi ) = c · exp −
i

wi2
2(1/2λ)

the MAP estimate minimizes the augmented error function
(11.37)

ridge regression
regularization

E =E+λ w

2

where E is the usual classiﬁcation or regression error (negative log likelihood). This augmented error is exactly the error function we used in
weight decay (equation 11.33). Using a large λ assumes small variability
in parameters, puts a larger force on them to be close to 0, and takes
the prior more into account than the data; if λ is small, then the allowed
variability of the parameters is larger. This approach of removing unnecessary parameters is known as ridge regression in statistics.
This is another example of regularization with a cost function, combin-

11.11 Dimensionality Reduction

267

ing the ﬁt to data and model complexity
(11.38)

soft weight sharing

cost = data-misﬁt + λ · complexity
The use of Bayesian estimation in training multilayer perceptrons is
treated in MacKay 1992a, b. We are going to talk about Bayesian estimation in more detail in chapter 14.
Empirically, it has been seen that after training, most of the weights
of a multilayer perceptron are distributed normally around 0, justifying
the use of weight decay. But this may not always be the case. Nowlan
and Hinton (1992) proposed soft weight sharing where weights are drawn
from a mixture of Gaussians, allowing them to form multiple clusters, not
one. Also, these clusters may be centered anywhere and not necessarily
at 0, and have variances that are modiﬁable. This changes the prior of
equation 11.36 to a mixture of M ≥ 2 Gaussians
M

(11.39)

p(wi ) =

αj pj (wi )
j=1

2
where αj are the priors and pj (wi ) ∼ N (mj , sj ) are the component Gaussians. M is set by the user and αj , mj , sj are learned from the data.
Using such a prior and augmenting the error function with its log during training, the weights converge to decrease error and also are grouped
automatically to increase the log prior.

11.11

Dimensionality Reduction
In a multilayer perceptron, if the number of hidden units is less than the
number of inputs, the ﬁrst layer performs a dimensionality reduction.
The form of this reduction and the new space spanned by the hidden
units depend on what the MLP is trained for. If the MLP is for classiﬁcation with output units following the hidden layer, then the new space is
deﬁned and the mapping is learned to minimize classiﬁcation error (see
ﬁgure 11.18).
We can get an idea of what the MLP is doing by analyzing the weights.
We know that the dot product is maximum when the two vectors are
identical. So we can think of each hidden unit as deﬁning a template in
its incoming weights, and by analyzing these templates, we can extract
knowledge from a trained MLP. If the inputs are normalized, weights tell
us of their relative importance. Such analysis is not easy but gives us

268

11 Multilayer Perceptrons

Hidden Representation
1
7
7

0.9

7

7
777
7

7
7 7
1

9

7

1
9 99 9

0.8

1 1

9

0.7

9

9

4
4
4

4

4

1

9
3

4 4
4

1 4

1

44

4

1

3

1
9

Hidden 2

0.6

3 3
33
3

3
9
5

5

0.5
3
0.4

8

3

8

3

0.3

1

5

2
2
2
2 2 2 22
2
2

0

0

0.1

8

8

8

5

8
5

0.2

0.1

88

2

0

0
0
0

0.2

0.3

8

6

1

0.4

0.5
Hidden 1

0.6

0 0 0
0
0
0.7

0.8

66
6
6
66
6
0
0.9

1

Figure 11.18 Optdigits data plotted in the space of the two hidden units of
an MLP trained for classiﬁcation. Only the labels of one hundred data points are
shown. This MLP with sixty-four inputs, two hidden units, and ten outputs has 80
percent accuracy. Because of the sigmoid, hidden unit values are between 0 and
1 and classes are clustered around the corners. This plot can be compared with
the plots in chapter 6, which are drawn using other dimensionality reduction
methods on the same dataset.

autoassociator

some insight as to what the MLP is doing and allows us to peek into the
black box.
An interesting architecture is the autoassociator (Cottrell, Munro, and
Zipser 1987), which is an MLP architecture where there are as many outputs as there are inputs, and the required outputs are deﬁned to be equal
to the inputs (see ﬁgure 11.19). To be able to reproduce the inputs again
at the output layer, the MLP is forced to ﬁnd the best representation of
the inputs in the hidden layer. When the number of hidden units is less
than the number of inputs, this implies dimensionality reduction. Once

269

11.11 Dimensionality Reduction

y1

y1

yd

yd

Decoder
zH

zH
Encoder

x0=+
1

xd

x1
Linear

x0=+
1

xd

x1
Nonlinear

Figure 11.19 In the autoassociator, there are as many outputs as there are
inputs and the desired outputs are the inputs. When the number of hidden units
is less than the number of inputs, the MLP is trained to ﬁnd the best coding of
the inputs on the hidden units, performing dimensionality reduction. On the
left, the ﬁrst layer acts as an encoder and the second layer acts as the decoder.
On the right, if the encoder and decoder are multilayer perceptrons with sigmoid
hidden units, the network performs nonlinear dimensionality reduction.

Sammon mapping

the training is done, the ﬁrst layer from the input to the hidden layer
acts as an encoder, and the values of the hidden units make up the encoded representation. The second layer from the hidden units to the
output units acts as a decoder, reconstructing the original signal from its
encoded representation.
It has been shown (Bourlard and Kamp 1988) that an MLP with one
hidden layer of units implements principal components analysis (section 6.3), except that the hidden unit weights are not the eigenvectors
sorted in importance using the eigenvalues but span the same space as
the H principal eigenvectors. If the encoder and decoder are not one layer
but multilayer perceptrons with sigmoid nonlinearity in the hidden units,
the encoder implements nonlinear dimensionality reduction (Hinton and
Salakhutdinov 2006).
Another way to use an MLP for dimensionality reduction is through
multidimensional scaling (section 6.5). Mao and Jain (1995) show how an
MLP can be used to learn the Sammon mapping. Recalling equation 6.29,

270

11 Multilayer Perceptrons

Sammon stress is deﬁned as
(11.40)

E(θ|X) =
r ,s

g(x r |θ) − g(x s |θ) − x r − x s
xr − xs

2

An MLP with d inputs, H hidden units, and k < d output units is used to
implement g(x|θ), mapping the d-dimensional input to a k-dimensional
vector, where θ corresponds to the weights of the MLP. Given a dataset of
X = {x t }t , we can use gradient descent to minimize the Sammon stress
directly to learn the MLP, namely, g(x|θ), such that the distances between the k-dimensional representations are as close as possible to the
distances in the original space.

11.12

Learning Time
Until now, we have been concerned with cases where the input is fed
once, all together. In some applications, the input is temporal where we
need to learn a temporal sequence. In others, the output may also change
in time. Examples are as follows:
Sequence recognition. This is the assignment of a given sequence to
one of several classes. Speech recognition is one example where the
input signal sequence is the spoken speech and the output is the code
of the word spoken. That is, the input changes in time but the output
does not.
Sequence reproduction. Here, after seeing part of a given sequence, the
system should predict the rest. Time-series prediction is one example
where the input is given but the output changes.
Temporal association. This is the most general case where a particular
output sequence is given as output after a speciﬁc input sequence. The
input and output sequences may be diﬀerent. Here both the input and
the output change in time.

11.12.1

time delay neural
network

Time Delay Neural Networks
The easiest way to recognize a temporal sequence is by converting it to a
spatial sequence. Then any method discussed up to this point can be utilized for classiﬁcation. In a time delay neural network (Waibel et al. 1989),

11.12 Learning Time

271

Figure 11.20 A time delay neural network. Inputs in a time window of length T
are delayed in time until we can feed all T inputs as the input vector to the MLP.

previous inputs are delayed in time so as to synchronize with the ﬁnal input, and all are fed together as input to the system (see ﬁgure 11.20).
Backpropagation can then be used to train the weights. To extract features local in time, one can have layers of structured connections and
weight sharing to get translation invariance in time. The main restriction
of this architecture is that the size of the time window we slide over the
sequence should be ﬁxed a priori.

11.12.2
recurrent network

Recurrent Networks
In a recurrent network, additional to the feedforward connections, units
have self-connections or connections to units in the previous layers. This
recurrency acts as a short-term memory and lets the network remember
what happened in the past.
Most frequently, one uses a partially recurrent network where a limited number of recurrent connections are added to a multilayer perceptron (see ﬁgure 11.21). This combines the advantage of the nonlinear
approximation ability of a multilayer perceptron with the temporal representation ability of the recurrency, and such a network can be used to
implement any of the three temporal association tasks. It is also possible
to have hidden units in the recurrent backward connections, these being

272

11 Multilayer Perceptrons

(a)

(b)

(c)

Figure 11.21 Examples of MLP with partial recurrency. Recurrent connections
are shown with dashed lines: (a) self-connections in the hidden layer, (b) selfconnections in the output layer, and (c) connections from the output to the
hidden layer. Combinations of these are also possible.

unfolding in time

backpropagation
through time

real time recurrent
learning

11.13

known as context units. No formal results are known to determine how
to choose the best architecture given a particular application.
If the sequences have a small maximum length, then unfolding in time
can be used to convert an arbitrary recurrent network to an equivalent
feedforward network (see ﬁgure 11.22). A separate unit and connection
is created for copies at diﬀerent times. The resulting network can be
trained with backpropagation with the additional requirement that all
copies of each connection should remain identical. The solution, as in
weight sharing, is to sum up the diﬀerent weight changes in time and
change the weight by the average. This is called backpropagation through
time (Rumelhart, Hinton, and Willams 1986b). The problem with this approach is the memory requirement if the length of the sequence is large.
Real time recurrent learning (Williams and Zipser 1989) is an algorithm
for training recurrent networks without unfolding and has the advantage
that it can use sequences of arbitrary length.

Notes
Research on artiﬁcial neural networks is as old as the digital computer.
McCulloch and Pitts (1943) proposed the ﬁrst mathematical model for the
artiﬁcial neuron. Rosenblatt (1962) proposed the perceptron model and a
learning algorithm in 1962. Minsky and Papert (1969) showed the limita-

273

11.13 Notes

y
V
h3
W

R

x3

h2
W

x2

y

h1
W

V
R
h

R

x1

h0
W

W
x
(a)

R

x0
(b)

Figure 11.22 Backpropagation through time: (a) recurrent network, and (b) its
equivalent unfolded network that behaves identically in four steps.

tion of single-layer perceptrons, for example, the XOR problem, and since
there was no algorithm to train a multilayer perceptron with a hidden
layer at that time, the work on artiﬁcial neural networks almost stopped
except at a few places. The renaissance of neural networks came with
the paper by Hopﬁeld (1982). This was followed by the two-volume Parallel Distributed Processing (PDP) book written by the PDP Research Group
(Rumelhart, McClelland, and the PDP Research Group 1986). It seems as
though backpropagation was invented independently in several places almost at the same time and the limitation of a single-layer perceptron no
longer held.
Starting in the mid-1980s, there has been a huge explosion of work on
artiﬁcial neural network models from various disciplines: physics, statistics, psychology, cognitive science, neuroscience, and lingustics, not to
mention computer science, electrical engineering, and adaptive control.

274

11 Multilayer Perceptrons

projection pursuit

Perhaps the most important contribution of research on artiﬁcial neural networks is this synergy that bridged various disciplines, especially
statistics and engineering. It is thanks to this that the ﬁeld of machine
learning is now well established.
The ﬁeld is much more mature now; aims are more modest and better
deﬁned. One of the criticisms of backpropagation was that it was not
biologically plausible! Though the term “neural network” is still widely
used, it is generally understood that neural network models, for example,
multilayer perceptrons, are nonparametric estimators and that the best
way to analyze them is by using statistical methods.
For example, a statistical method similar to the multilayer perceptron
is projection pursuit (Friedman and Stuetzle 1981), which is written as
H

φh (w T x)
h

y=
h=1

the diﬀerence being that each “hidden unit” has its own separate function, φh (·), though in an MLP, all are ﬁxed to be sigmoid. In chapter 12,
we will see another neural network structure, named radial basis functions, which uses the Gaussian function at the hidden units.
There are various textbooks on artiﬁcial neural networks: Hertz, Krogh,
and Palmer 1991, the earliest, is still readable. Bishop 1995 has a pattern
recognition emphasis and discusses in detail various optimization algorithms that can be used for training, as well as the Bayesian approach,
generalizing weight decay. Ripley 1996 analyzes neural networks from a
statistical perspective.
Artiﬁcial neural networks, for example, multilayer perceptrons, have
various successful applications. In addition to their various successful
applications in adaptive control, speech recognition, and vision, two are
noteworthy: Tesauro’s TD-Gammon program (Tesauro 1994) uses reinforcement learning (chapter 18) to train a multilayer perceptron and plays
backgammon at a master level. Pomerleau’s ALVINN is a neural network
that autonomously drives a van up to 20 miles per hour after learning by
observing a driver for ﬁve minutes (Pomerleau 1991).

11.14

Exercises
1. Show the perceptron that calculates NOT of its input.
2. Show the perceptron that calculates NAND of its two inputs.

11.15 References

275

3. Show the perceptron that calculates the parity of its three inputs.
4. Derive the update equations when the hidden units use tanh, instead of the
2
sigmoid. Use the fact that tanh = (1 − tanh ).
5. Derive the update equations for an MLP with two hidden layers.
6. Consider a MLP architecture with one hidden layer where there are also direct
weights from the inputs directly to the output units. Explain when such a
structure would be helpful and how it can be trained.
7. Parity is cyclic shift invariant; for example, “0101” and “1010” have the same
parity. Propose a multilayer perceptron to learn the parity function using this
hint.
8. In cascade correlation, what are the advantages of freezing the previously
existing weights?
9. Derive the update equations for an MLP implementing Sammon mapping that
minimizes Sammon stress (equation 11.40).
10. In section 11.6, we discuss how a MLP with two hidden layers can implement
piecewise constant approximation. Show that if the weight in the last layer is
not a constant but a linear function of the input, we can implement piecewise
linear approximation.
11. Derive the update equations for soft weight sharing.
12. In the autoassociator network, how can we decide on the number of hidden
units?
13. Incremental learning of the structure of a MLP can be viewed as a state space
search. What are the operators? What is the goodness function? What type of
search strategies are appropriate? Deﬁne these in such a way that dynamic
node creation and cascade-correlation are special instantiations.
14. For the MLP given in ﬁgure 11.22, derive the update equations for the unfolded network.

11.15

References
Abu-Mostafa, Y. 1995. “Hints.” Neural Computation 7: 639–671.
Aran, O., O. T. Yıldız, and E. Alpaydın. 2009. “An Incremental Framework Based
on Cross-Validation for Estimating the Architecture of a Multilayer Perceptron.” International Journal of Pattern Recognition and Artiﬁcial Intelligence
23: 159–190.
Ash, T. 1989. “Dynamic Node Creation in Backpropagation Networks.” Connection Science 1: 365–375.

276

11 Multilayer Perceptrons

Battiti, R. 1992. “First- and Second-Order Methods for Learning: Between Steepest Descent and Newton’s Method.” Neural Computation 4: 141–166.
Bishop, C. M. 1995. Neural Networks for Pattern Recognition. Oxford: Oxford
University Press.
Bourlard, H., and Y. Kamp. 1988. “Auto-Association by Multilayer Perceptrons
and Singular Value Decomposition.” Biological Cybernetics 59: 291–294.
Cottrell, G. W., P. Munro, and D. Zipser. 1987. “Learning Internal Representations from Gray-Scale Images: An Example of Extensional Programming.” In
Ninth Annual Conference of the Cognitive Science Society, 462–473. Hillsdale,
NJ: Erlbaum.
Durbin, R., and D. E. Rumelhart. 1989. “Product Units: A Computationally
Powerful and Biologically Plausible Extension to Backpropagation Networks.”
Neural Computation 1: 133–142.
Fahlman, S. E., and C. Lebiere. 1990. “The Cascade Correlation Architecture.”
In Advances in Neural Information Processing Systems 2, ed. D. S. Touretzky,
524–532. San Francisco: Morgan Kaufmann.
Friedman, J. H., and W. Stuetzle. 1981. “Projection Pursuit Regression.” Journal
of the American Statistical Association 76: 817–823.
Geman, S., E. Bienenstock, and R. Doursat. 1992. “Neural Networks and the
Bias/Variance Dilemma.” Neural Computation 4: 1–58.
Hertz, J., A. Krogh, and R. G. Palmer. 1991. Introduction to the Theory of Neural
Computation. Reading, MA: Addison Wesley.
Hinton, G. E., and R. R. Salakhutdinov. 2006. “Reducing the dimensionality of
data with neural networks.” Science 313: 504–507.
Hopﬁeld, J. J. 1982. “Neural Networks and Physical Systems with Emergent
Collective Computational Abilities.” Proceedings of the National Academy of
Sciences USA 79: 2554–2558.
Hornik, K., M. Stinchcombe, and H. White. 1989. “Multilayer Feedforward Networks Are Universal Approximators.” Neural Networks 2: 359–366.
Le Cun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
and L. D. Jackel. 1989. “Backpropagation Applied to Handwritten Zipcode
Recognition.” Neural Computation 1: 541–551.
MacKay, D. J. C. 1992a. “Bayesian Interpolation.” Neural Computation 4: 415–
447.
MacKay, D. J. C. 1992b. “A Practical Bayesian Framework for Backpropagation
Networks” Neural Computation 4: 448–472.
Mao, J., and A. K. Jain. 1995. “Artiﬁcial Neural Networks for Feature Extraction
and Multivariate Data Projection.” IEEE Transactions on Neural Networks 6:
296–317.

11.15 References

277

Marr, D. 1982. Vision. New York: Freeman.
McCulloch, W. S., and W. Pitts. 1943. “A Logical Calculus of the Ideas Immenent
in Nervous Activity.” Bulletin of Mathematical Biophysics 5: 115–133.
Minsky, M. L., and S. A. Papert. 1969. Perceptrons. Cambridge, MA: MIT Press.
(Expanded ed. 1990.)
Nowlan, S. J., and G. E. Hinton. 1992. “Simplifying Neural Networks by Soft
Weight Sharing.” Neural Computation 4: 473–493.
Pomerleau, D. A. 1991. “Eﬃcient Training of Artiﬁcial Neural Networks for
Autonomous Navigation.” Neural Computation 3: 88–97.
Posner, M. I., ed. 1989. Foundations of Cognitive Science. Cambridge, MA: MIT
Press.
Richard, M. D., and R. P. Lippmann. 1991. “Neural Network Classiﬁers Estimate
Bayesian a Posteriori Probabilities.” Neural Computation 3: 461–483.
Ripley, B. D. 1996. Pattern Recognition and Neural Networks. Cambridge, UK:
Cambridge University Press.
Rosenblatt, F. 1962. Principles of Neurodynamics: Perceptrons and the Theory
of Brain Mechanisms. New York: Spartan.
Rumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986a. “Learning Representations by Backpropagating Errors.” Nature 323: 533–536.
Rumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986b. “Learning Internal
Representations by Error Propagation.” In Parallel Distributed Processing, ed.
D. E. Rumelhart, J. L. McClelland, and the PDP Research Group, 318–362. Cambridge, MA: MIT Press.
Rumelhart, D. E., J. L. McClelland, and the PDP Research Group, eds. 1986.
Parallel Distributed Processing. Cambridge, MA: MIT Press.
Simard, P., B. Victorri, Y, Le Cun, and J. Denker. 1992. “Tangent Prop: A Formalism for Specifying Selected Invariances in an Adaptive Network.” In Advances
in Neural Information Processing Systems 4, ed. J. E. Moody, S. J. Hanson, and
R. P. Lippman, 895–903. San Francisco: Morgan Kaufmann.
Tesauro, G. 1994. “TD-Gammon, A Self-Teaching Backgammon Program, Achieves
Master-Level Play.” Neural Computation 6: 215–219.
Thagard, P. 2005. Mind: Introduction to Cognitive Science. 2nd ed. Cambridge,
MA: MIT Press.
Waibel, A., T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. 1989. “Phoneme
Recognition Using Time-Delay Neural Networks.” IEEE Transactions on Acoustics, Speech, and Signal Processing 37: 328–339.
Williams, R. J., and D. Zipser. 1989. “A Learning Algorithm for Continually
Running Fully Recurrent Neural Networks.” Neural Computation 1: 270–280.

12

Local Models

We continue our discussion of multilayer neural networks with models where the ﬁrst layer contains locally receptive units that respond
to instances in a localized region of the input space. The second layer
on top learns the regression or classiﬁcation function for these local
regions. We discuss learning methods for ﬁnding the local regions of
importance as well as the models responsible in there.

12.1

Introduction
On e w ay to do function approximation is to divide the input space into
local patches and learn a separate ﬁt in each local patch. In chapter 7,
we discussed statistical methods for clustering that allowed us to group
input instances and model the input distribution. Competitive methods
are neural network methods for online clustering. In this chapter, we
discuss the online version of k-means, as well as two neural network
extensions, adaptive resonance theory (ART), and the self-organizing map
(SOM).
We then discuss how supervised learning is implemented once the inputs are localized. If the ﬁt in a local patch is constant, then the technique
is named the radial basis function (RBF) network; if it is a linear function
of the input, it is called the mixture of experts (MoE). We discuss both
regression and classiﬁcation, and also compare this approach with MLP,
which we discussed in chapter 11.

280

12 Local Models

12.2

competitive
learning

winner-take-all

12.2.1

Competitive Learning
In chapter 7, we used the semiparametric Gaussian mixture density, which
assumes that the input comes from one of k Gaussian sources. In this
section, we make the same assumption that there are k groups (or clusters) in the data, but our approach is not probabilistic in that we do not
enforce a parametric model for the sources. Another diﬀerence is that
the learning methods we propose are online. We do not have the whole
sample at hand during training; we receive instances one by one and update model parameters as we get them. The term competitive learning
is used because it is as if these groups, or rather the units representing
these groups, compete among themselves to be the one responsible for
representing an instance. The model is also called winner-take-all; it is
as if one group wins and gets updated, and the others are not updated at
all.
These methods can be used by themselves for online clustering, as
opposed to the batch methods discussed in chapter 7. An online method
has the usual advantages that (1) we do not need extra memory to store
the whole training set; (2) updates at each step are simple to implement,
for example, in hardware; and (3) the input distribution may change in
time and the model adapts itself to these changes automatically. If we
were to use a batch algorithm, we would need to collect a new sample
and run the batch method from scratch over the whole sample.
Starting in section 12.3, we will also discuss how such an approach can
be followed by a supervised method to learn regression or classiﬁcation
problems. This will be a two-stage system that can be implemented by a
two-layer network, where the ﬁrst stage (-layer) models the input density
and ﬁnds the responsible local model, and the second stage is that of the
local model generating the ﬁnal output.

Online k-Means
In equation 7.3, we deﬁned the reconstruction error as

(12.1)

E({mi }k |X) =
i=1

1
2

t
bi xt − m i
t

2

i

where
(12.2)

t
bi =

1
0

if xt − m i = minl xt − m l
otherwise

281

12.2 Competitive Learning

t
X = {xt }t is the sample and m i , i = 1, . . . , k are the cluster centers. bi
t
is 1 if m i is the closest center to x in Euclidean distance. It is as if all
m l , l = 1, . . . , k compete and m i wins the competition because it is the
closest.
The batch algorithm, k-means, updates the centers as

(12.3)

online k-means

(12.4)

mi =

t

t
bi x t
t
t bi

which minimizes equation 12.1, once the winners are chosen using equat
tion 12.2. As we saw before, these two steps of calculating bi and updating m i are iterated until convergence.
We can obtain online k-means by doing stochastic gradient descent,
considering the instances one by one, and doing a small update at each
step, not forgetting the eﬀect of the previous updates. The reconstruction error for a single instance is
E t ({m i }k |x t ) =
i=1

1
2

t
bi x t − m i
i

2

=

1
2

d
t
bi (xt − mij )2
j
i j=1

t
bi

where
is deﬁned as in equation 12.2. Using gradient descent on this,
we get the following update rule for each instance x t :
(12.5)

stability-plasticity
dilemma

(12.6)

Δmij = −η

∂E t
t
= ηbi (xt − mij )
j
∂mij

t
This moves the closest center (for which bi = 1) toward the input by
a factor given by η. The other centers have their blt , l = i equal to 0 and
are not updated (see ﬁgure 12.1). A batch procedure can also be deﬁned
by summing up equation 12.5 over all t. Like in any gradient descent
procedure, a momentum term can also be added. For convergence, η is
gradually decreased to 0. But this implies the stability-plasticity dilemma:
If η is decreased toward 0, the network becomes stable but we lose adaptivity to novel patterns that may occur in time because updates become
too small. If we keep η large, m i may oscillate.
The pseudocode of online k-means is given in ﬁgure 12.2. This is the
online version of the batch algorithm given in ﬁgure 7.3.
The competitive network can be implemented as a one-layer recurrent
network as shown in ﬁgure 12.3. The input layer contains the input vector
x; note that there is no bias unit. The values of the output units are the
bi and they are perceptrons:

bi = m T x
i

282

12 Local Models

x2
mi
x

x1
Figure 12.1 Shaded circles are the centers and the empty circle is the input
instance. The online version of k-means moves the closest center along the direction of (x − m i ) by a factor speciﬁed by η.

lateral inhibition

Then we need to choose the maximum of the bi and set it equal to
1, and set the others, bl , l = i to 0. If we would like to do everything
purely neural, that is, using a network of concurrently operating processing units, the choosing of the maximum can be implemented through
lateral inhibition. As shown in ﬁgure 12.3, each unit has an excitatory
recurrent connection (i.e., with a positive weight) to itself, and inhibitory
recurrent connections (i.e., with negative weights) to the other output
units. With an appropriate nonlinear activation function and positive
and negative recurrent weight values, such a network, after some iterations, converges to a state where the maximum becomes 1 and all others
become 0 (Grossberg 1980; Feldman and Ballard 1982).
The dot product used in equation 12.6 is a similarity measure, and we
saw in section 5.5 (equation 5.26) that if m i have the same norm, then
the unit with the minimum Euclidean distance, m i − x , is the same as
the one with the maximum dot product, m T x.
i
Here, and later when we discuss other competitive methods, we use
the Euclidean distance, but we should keep in mind that using the Euclidean distance implies that all input attributes have the same variance
and that they are not correlated. If this is not the case, this should be
reﬂected in the distance measure, that is, by using the Mahalanobis distance, or suitable normalization should be done, for example, by PCA, at

12.2 Competitive Learning

283

Initialize m i , i = 1, . . . , k, for example, to k random x t
Repeat
For all x t ∈ X in random order
i ← arg minj xt − m j
m i ← m i + η(x t − m i )
Until m i converge
Figure 12.2 Online k-means algorithm. The batch version is given in ﬁgure 7.3.

a preprocessing stage before the Euclidean distance is used.
We can rewrite equation 12.5 as
(12.7)

t
t
t
Δmij = ηbi xt − ηbi mij
j

Let us remember that mij is the weight of the connection from xj to bi .
An update of the form, as we see in the ﬁrst term
(12.8)
Hebbian learning

t
t
Δmij = ηbi xt
j

is Hebbian learning, which deﬁnes the update as the product of the values
of the presynaptic and postsynaptic units. It was proposed as a model for
neural plasticity: A synapse becomes more important if the units before
and after the connection ﬁre simultaneously, indicating that they are correlated. However, with only Hebbian learning, the weights grow without
bound (xt ≥ 0), and we need a second force to decrease the weights that
j
are not updated. One possibility is to explicitly normalize the weights to
have m i = 1; if Δmij > 0 and Δmil = 0, l = i, once we normalize m i
to unit length, mil decrease. Another possibility is to introduce a weight
decay term (Oja 1982), and the second term of equation 12.7 can be seen
as such. Hertz, Krogh, and Palmer (1991) discuss competitive networks
and Hebbian learning in more detail and show, for example, how such
networks can learn to do PCA. Mao and Jain (1995) discuss online algorithms for PCA and LDA.
As we saw in chapter 7, one problem is to avoid dead centers, namely,
the ones that are there but are not eﬀectively utilized. In the case of competitive networks, this corresponds to centers that never win the competition because they are initialized far away from any input. There are
various ways we can avoid this:
1. We can initialize m i by randomly chosen input instances, and make
sure that they start from where there is data.

284

12 Local Models

b2
b1

bk
m1

x1

m2

mk

xd

Figure 12.3 The winner-take-all competitive neural network, which is a network
of k perceptrons with recurrent connections at the output. Dashed lines are recurrent connections, of which the ones that end with an arrow are excitatory and
the ones that end with a circle are inhibitory. Each unit at the output reinforces
its value and tries to suppress the other outputs. Under a suitable assignment of
these recurrrent weights, the maximum suppresses all the others. This has the
net eﬀect that the one unit whose mi is closest to x ends up with its bi equal to
1 and all others, namely, bl , l = i are 0.

2. We can use a leader-cluster algorithm and add units one by one, always
adding them at a place where they are needed. One example is the ART
model, which we discuss in section 12.2.2.
3. When we update, we do not update only the center of the closest unit
but some others as well. As they are updated, they also move toward
the input, move gradually toward parts of the input space where there
are inputs, and eventually win the competition. One example that we
discuss in section 12.2.3 is SOM.
4. Another possibility is to introduce a conscience mechanism (DeSieno
1988): A unit that has won the competition recently feels guilty and
allows others to win.

285

12.2 Competitive Learning

ρ

x2
mi
xa

xb
x1

Figure 12.4 The distance from x a to the closest center is less than the vigilance
value ρ and the center is updated as in online k-means. However, x b is not close
enough to any of the centers and a new group should be created at that position.

12.2.2

adaptive resonance
theory

vigilance

Adaptive Resonance Theory
The number of groups, k, should be known and speciﬁed before the parameters can be calculated. Another approach is incremental, where one
starts with a single group and adds new groups as they are needed. We
discuss the adaptive resonance theory (ART) algorithm (Carpenter and
Grossberg 1988) as an example of an incremental algorithm. In ART,
given an input, all of the output units calculate their values and the one
most similar to the input is chosen. This is the unit with the maximum
value if the unit uses the dot product as in equation 12.6, or it is the unit
with the minimum value if the unit uses the Euclidean distance.
Let us assume that we use the Euclidean distance. If the minimum value
is smaller than a certain threshold value, named the vigilance, the update
is done as in online k-means. If this distance is larger than vigilance, a
new output unit is added and its center is initialized with the instance.
This deﬁnes a hypersphere whose radius is given by the vigilance deﬁning
the volume of scope of each unit; we add a new unit whenever we have
an input that is not covered by any unit (see ﬁgure 12.4).

286

12 Local Models

Denoting vigilance by ρ, we use the following equations at each update:
k

(12.9)

bi = m i − x t = min m l − x t
l=1

m k+1 ← x t
Δm i = η(x t − m i )

if bi > ρ
otherwise

Putting a threshold on distance is equivalent to putting a threshold on
the reconstruction error per instance, and if the distance is Euclidean and
the error is deﬁned as in equation 12.4, this indicates that the maximum
reconstruction error allowed per instance is the square of vigilance.

12.2.3

self-organizing map

(12.10)

Self-Organizing Maps
One way to avoid having dead units is by updating not only the winner but also some of the other units as well. In the self-organizing map
(SOM) proposed by Kohonen (1990, 1995), unit indices, namely, i as in
m i , deﬁne a neighborhood for the units. When m i is the closest center,
in addition to m i , its neighbors are also updated. For example, if the
neighborhood is of size 2, then m i−2 , m i−1 , m i+1 , m i+2 are also updated
but with less weight as the neighborhood increases. If i is the index of
the closest center, the centers are updated as
Δm l = η e(l, i)(x t − m l )
where e(l, i) is the neighborhood function. e(l, i) = 1 when l = i and
decreases as |l − i| increases, for example, as a Gaussian, N (i, σ ):

(12.11)

(l − i)2
1
exp −
e(l, i) = √
2σ 2
2π σ
For convergence, the support of the neighborhood function decreases
in time, for example, σ decreases, and at the end, only the winner is
updated.
Because neighboring units are also moved toward the input, we avoid
dead units since they get to win competition sometime later, after a little
bit of initial help from their neighboring friends (see ﬁgure 12.5).
Updating the neighbors has the eﬀect that, even if the centers are randomly initialized, because they are moved toward the same input together, once the system converges, units with neighboring indices will
also be neighbors in the input space.

287

12.2 Competitive Learning

mi+2

x2
mi

mi+1
x

mi-1
mi-2
x1
Figure 12.5 In the SOM, not only the closest unit but also its neighbors, in
terms of indices, are moved toward the input. Here, neighborhood is 1; mi and
its 1-nearest neighbors are updated. Note here that m i+1 is far from m i , but as
it is updated with mi , and as mi will be updated when mi+1 is the winner, they
will become neighbors in the input space as well.

In most applications, the units are organized as a two-dimensional
map. That is, each unit will have two indices, m i,j , and the neighborhood will be deﬁned in two dimensions. If m i,j is the closest center, the
centers are updated as
(12.12)

topographical map

Δm k,l = ηe(k, l, i, j)(x t − m k,l )
where the neighborhood function is now in two dimensions. After convergence, this forms a two-dimensional topographical map of the original
d-dimensional input space. The map contains many units in parts of
the space where density is high, and no unit will be dedicated to parts
where there is no input. Once the map converges, inputs that are close
in the original space are mapped to units that are close in the map. In
this regard, the map can be interpreted as doing a nonlinear form of
multidimensional scaling, mapping from the original x space to the two
dimensions, (i, j). Similarly, if the map is one-dimensional, the units are
placed on the curve of maximum density in the input space, as a principal
curve.

288

12 Local Models

12.3

distributed
representation

local
representation

receptive field

(12.13)

Radial Basis Functions
In a multilayer perceptron (chapter 11) where hidden units use the dot
product, each hidden unit deﬁnes a hyperplane and with the sigmoid
nonlinearity, a hidden unit has a value between 0 and 1, coding the position of the instance with respect to the hyperplane. Each hyperplane
divides the input space in two, and typically for a given input, many of
the hidden units have nonzero output. This is called a distributed representation because the input is encoded by the simultaneous activation of
many hidden units.
Another possibility is to have a local representation where for a given
input, only one or a few units are active. It is as if these locally tuned
units partition the input space among themselves and are selective to
only certain inputs. The part of the input space where a unit has nonzero
response is called its receptive ﬁeld. The input space is then paved with
such units.
Neurons with such response characteristics are found in many parts
of the cortex. For example, cells in the visual cortex respond selectively
to stimulation that is both local in retinal position and local in angle
of visual orientation. Such locally tuned cells are typically arranged in
topogrophical cortical maps in which the values of the variables to which
the cells respond vary by their position in the map, as in a SOM.
The concept of locality implies a distance function to measure the similarity between the given input x and the position of unit h, m h . Frequently
this measure is taken as the Euclidean distance, x − m h . The response
function is chosen to have a maximum where x = m h and decreasing
as they get less similar. Commonly we use the Gaussian function (see
ﬁgure 12.6):
t
ph = exp −

xt − m h
2
2sh

2

Strictly speaking, this is not Gaussian density, but we use the same
name anyway. m j and sj respectively denote the center and the spread
of the local unit j, and as such deﬁne a radially symmetric basis function. One can use an elliptic one with diﬀerent spreads on diﬀerent dimensions, or even use the full Mahalanobis distance to allow correlated
inputs, at the expense of using a more complicated model (exercise 2).
The idea in using such local basis functions is that in the input data,
there are groups or clusters of instances and for each such cluster, we

289

12.3 Radial Basis Functions

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
−5

−4

−3

−2

−1

0

1

2

3

4

5

Figure 12.6 The one-dimensional form of the bell-shaped function used in the
radial basis function network. This one has m = 0 and s = 1. It is like a Gaussian
but it is not a density; it does not integrate to 1. It is nonzero between (m −
3s, m + 3s), but a more conservative interval is (m − 2s, m + 2s).

Distributed vs
local
representation

t
deﬁne a basis function, ph , which becomes nonzero if instance x t belongs to cluster h. One can use any of the online competitive methods
discussed in section 12.2 to ﬁnd the centers, m h . There is a simple and
eﬀective heuristic to ﬁnd the spreads: Once we have the centers, for each
cluster, we ﬁnd the most distant instance covered by that cluster and set
sh to half its distance from the center. We could have used one-third,
but we prefer to be conservative. We can also use the statistical clustering method, for example, EM on Gaussian mixtures, that we discussed in
chapter 7 to ﬁnd the cluster parameters, namely, means, variances (and
covariances).
t
ph , h = 1, . . . , H deﬁne a new H-dimensional space and form a new
t
representation of x t . We can also use bh (equation 12.2) to code the
t
t
input but bh are 0/1; ph have the additional advantage that they code the
distance to their center by a value in (0, 1). How fast the value decays
to 0 depends on sh . Figure 12.7 gives an example and compares such
a local representation with a distributed representation as used by the
multilayer perceptron. Because Gaussians are local, typically we need
many more local units than what we would need if we were to use a
distributed representation, especially if the input is high-dimensional.
In the case of supervised learning, we can then use this new local rep-

290

12 Local Models

w2
m 1 xa

m3

+

x2

w1

x2
xa

xb

xc

xb

xc

+
m2
x1
Local representation in the
space of (p1, p2, p3)
xa : (1.0, 0.0, 0.0)
xb : (0.0, 0.0, 1.0)
xc : (1.0, 1.0, 0.0)

x1
Distributed representation in the
space of (h1, h2)
xa : (1.0, 1.0)
xb : (0.0, 1.0)
xc : (1.0, 0.0)

Figure 12.7 The diﬀerence between local and distributed representations. The
values are hard, 0/1, values. One can use soft values in (0, 1) and get a more informative encoding. In the local representation, this is done by the Gaussian RBF
that uses the distance to the center, m i , and in the distributed representation,
this is done by the sigmoid that uses the distance to the hyperplane, w i .

resentation as the input. If we use a perceptron, we have
H

(12.14)

t
wh ph + w0

yt =
h=1

radial basis
function

where H is the number of basis functions. This structure is called a
radial basis function (RBF) network (Broomhead and Lowe 1988; Moody
and Darken 1989). Normally, people do not use RBF networks with more
than one layer of Gaussian units. H is the complexity parameter, like
the number of hidden units in a multilayer perceptron. Previously we
denoted it by k, when it corresponded to the number of centers in the
case of unsupervised learning.
Here, we see the advantage of using ph instead of bh . Because bh are
0/1, if equation 12.14 contained bh instead of the ph , it would give a
piecewise constant approximation with discontuinities at the unit region
boundaries. ph values are soft and lead to a smooth approximation, taking a weighted average while passing from one region to another. We can
easily see that such a network is a universal approximator in that it can

12.3 Radial Basis Functions

hybrid learning

anchor

291

approximate any function with desired accuracy, given enough units. We
can form a grid in the input space to our desired accuracy, deﬁne a unit
that will be active for each cell, and set its outgoing weight, wh , to the
desired output value.
This architecture bears much similarity to the nonparametric estimators, for example, Parzen windows, we saw in chapter 8, and ph may be
seen as kernel functions. The diﬀerence is that now we do not have a
kernel function over all training instances but group them using a clustering method to make do with fewer kernels. H, the number of units,
is the complexity parameter, trading oﬀ simplicity and accuracy. With
more units, we approximate the training data better, but we get a complex model and risk overﬁtting; too few may underﬁt. Again, the optimal
value is determined by cross-validation.
Once m h and sh are given and ﬁxed, ph are also ﬁxed. Then wh can be
trained easily batch or online. In the case of regression, this is a linear
regression model (with ph as the inputs) and the wh can be solved analytically without any iteration (section 4.6). In the case of classiﬁcation, we
need to resort to an iterative procedure. We discussed learning methods
for this in chapter 10 and do not repeat them here.
What we do here is a two-stage process: we use an unsupervised method
for determining the centers, then build a supervised layer on top of that.
This is called hybrid learning. We can also learn all parameters, including
m h and sh , in a supervised manner. The radial basis function of equation 12.13 is diﬀerentiable and we can backpropagate, just as we backpropagated in a multilayer perceptron to update the ﬁrst-layer weights.
The structure is similar to a multilayer perceptron with ph as the hidden
units, m h and sh as the ﬁrst-layer parameters, the Gaussian as the activation function in the hidden layer, and wh as the second-layer weights (see
ﬁgure 12.8).
But before we discuss this, we should remember that training a twolayer network is slow. Hybrid learning trains one layer at a time and is
faster. Another technique, called the anchor method, sets the centers to
the randomly chosen patterns from the training set without any further
update. It is adequate if there are many units.
On the other hand, the accuracy normally is not as high as when a
completely supervised method is used. Consider the case when the input is uniformly distributed. Then k-means clustering places the units
uniformly. If the function is changing signiﬁcantly in a small part of the
space, it is a better idea to have as many centers in places where the func-

292

12 Local Models

yi

wih
ph
p0=+1
mhj , s h

x1

xj

xd

Figure 12.8 The RBF network where ph are the hidden units using the bellshaped activation function. m h , sh are the ﬁrst-layer parameters, and w i are the
second-layer weights.

tion changes fast, to make the error as small as possible; this is what the
completely supervised method would do.
Let us discuss how all of the parameters can be trained in a fully supervised manner. The approach is the same as backpropagation applied
to multilayer perceptrons. Let us see the case of regression with multiple
outputs. The batch error is
(12.15)

E({mh , sh , wih }i,h |X) =

1
2

(rit − yit )2
t

i

where
H

(12.16)

yit =

t
wih ph + wi0
h=1

Using gradient descent, we get the following update rule for the second-

293

12.3 Radial Basis Functions

layer weights:
(12.17)

t
(rit − yit )ph

Δwih = η
t

(12.18)

(12.19)

This is the usual perceptron update rule, with ph as the inputs. Typically, ph do not overlap much and at each iteration, only a few ph are
nonzero and only their wh are updated. That is why RBF networks learn
very fast, and faster than multilayer perceptrons that use a distributed
representation.
Similarly, we can get the update equations for the centers and spreads
by backpropagation (chain rule):
⎡
⎤
(xt − mhj )
j
t
t
t
Δmhj = η ⎣ (ri − yi )wih ⎦ ph
2
sh
t
i
⎡
⎤
t
2
t
t
t x − mh
Δsh = η ⎣ (ri − yi )wih ⎦ ph
3
sh
t
i
Let us compare equation 12.18 with equation 12.5: First, here we use
ph instead of bh , which means that not only the closest one but all units
are updated, depending on their centers and spreads. Second, here the
update is supervised and contains the backpropagated error term. The
update depends not only on the input but also on the ﬁnal error (rit − yit ),
the eﬀect of the unit on the output, wih , the activation of the unit, ph , and
the input, (x − m h ).
In practice, equations 12.18 and 12.19 need some extra control. We
need to explicitly check that sh do not become very small or very large to
be useless; we also need to check that m h stay in the valid input range.
In the case of classiﬁcation, we have

(12.20)

yit =

exp
k

exp

t
h wih ph

+ wi0

t
h wkh ph

+ wk0

and the cross-entropy error is
(12.21)

rit log yit

E({mh , sh , wih }i,h |X) = −
t

i

Update rules can similarly be derived using gradient descent (exercise 3).
Let us look again at equation 12.14. For any input, if ph is nonzero,
then it contributes wh to the output. Its contribution is a constant ﬁt, as

294

12 Local Models

given by wh . Normally Gaussians do not overlap much, and one or two of
them have a nonzero ph value. In any case, only few units contribute to
the output. w0 is the constant oﬀset and is added to the weighted sum
of the active (nonzero) units. We also see that y = w0 if all ph are 0. We
can therefore view w0 as the “default” value of y: If no Gaussian is active,
then the output is given by this value. So a possibility is to make this
“default model” a more powerful “rule.” For example, we can write
H

(12.22)

t
wh ph + v T x t + v0

yt =
h=1

r ule

exceptions

In this case, the rule is linear: v T x t + v0 . When they are nonzero, Gaussians work as localized “exceptions” and modify the output to make up
for the diﬀerence between the desired output and the rule output. Such a
model can be trained in a supervised manner, and the rule can be trained
together with the exceptions (exercise 4). We discuss a similar model, cascading, in section 17.11 where we see it as a combination of two learners,
one general rule and the other formed by a set of exceptions.

12.4

prior knowledge

Incorporating Rule-Based Knowledge
The training of any learning system can be much simpler if we manage to
incorporate prior knowledge to initialize the system. For example, prior
knowledge may be available in the form of a set of rules that specify the
input/output mapping that the model, for example, the RBF network, has
to learn. This occurs frequently in industrial and medical applications
where rules can be given by experts. Similarly, once a network has been
trained, rules can be extracted from the solution in such a way as to better
understand the solution to the problem.
The inclusion of prior knowledge has the additional advantage that if
the network is required to extrapolate into regions of the input space
where it has not seen any training data, it can rely on this prior knowledge. Furthermore, in many control applications, the network is required
to make reasonable predictions right from the beginning. Before it has
seen suﬃcient training data, it has to rely primarily on this prior knowledge.
In many applications we are typically told some basic rules that we try
to follow in the beginning but that are then reﬁned and altered through

295

12.5 Normalized Basis Functions

rule extraction

(12.23)

experience. The better our initial knowledge of a problem, the faster we
can achieve good performance and the less training that is required.
Such inclusion of prior knowledge or extraction of learned knowledge
is easy to do with RBF networks because the units are local. This makes
rule extraction easier (Tresp, Hollatz, and Ahmad 1997). An example is
IF ((x1 ≈ a) AND (x2 ≈ b)) OR (x3 ≈ c) THEN y = 0.1
where x1 ≈ a means “x1 is approximately a.” In the RBF framework, this
rule is encoded by two Gaussian units as
p1

fuzzy rule

12.5

exp −

(x1 − a)2
2
2s1

· exp −

p2

fuzzy membership
function

=

(x2 − b)2
2
2s2

=

exp −

(x3 − c)2
2
2s3

with w2 = 0.1

with w1 = 0.1

“Approximately equal to” is modeled by a Gaussian where the center
is the ideal value and the spread denotes the allowed diﬀerence around
this ideal value. Conjunction is the product of two univariate Gaussians
that is a bivariate Gaussian. Then, the ﬁrst product term can be handled
by a two-dimensional, namely, x = [x1 , x2 ], Gaussian centered at (a, b),
and the spreads on the two dimensions are given by s1 and s2 . Disjunction is modeled by two separate Gaussians, each one handling one of the
disjuncts.
Given labeled training data, the parameters of the RBF network so constructed can be ﬁne-tuned after the initial construction, using a small
value of η.
This formulation is related to the fuzzy logic approach where equation 12.23 is named a fuzzy rule. The Gaussian basis function that checks
for approximate equality corresponds to a fuzzy membership function
(Berthold 1999; Cherkassky and Mulier 1998).

Normalized Basis Functions
In equation 12.14, for an input, it is possible that all of the ph are 0. In
some applications, we may want to have a normalization step to make
sure that the values of the local units sum up to 1, thus making sure that
for any input there is at least one nonzero unit:

(12.24)

t
gh =

t
ph
H
t
l=1 pl

=

exp[− xt − m h
l

exp[− x t − m l

2

2
/2sh ]
2 /2s 2 ]
l

296

12 Local Models

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

Figure 12.9 (-) Before and (- -) after normalization for three Gaussians whose
centers are denoted by ‘*’. Note how the nonzero region of a unit depends also on
the positions of other units. If the spreads are small, normalization implements
a harder split; with large spreads, units overlap more.

An example is given in ﬁgure 12.9. Taking ph as p(x|h), gh correspond
to p(h|x), the posterior probability that x belongs to unit h. It is as if
the units divide the input space among themselves. We can think of gh
as a classiﬁer in itself, choosing the responsible unit for a given input.
This classiﬁcation is done based on distance, as in a parametric Gaussian
classiﬁer (chapter 5).
The output is a weighted sum
H

(12.25)

yit =

t
wih gh
h=1

where there is no need for a bias term because there is at least one
nonzero gh for each x. Using gh instead of ph does not introduce any
extra parameters; it only couples the units together: ph depends only on
m h and sh , but gh , because of normalization, depends on the centers and
spreads of all of the units.

297

12.6 Competitive Basis Functions

In the case of regression, we have the following update rules using
gradient descent:
(12.26)

=

Δwih

t
(rit − yit )gh

η
t

(12.27)

Δmhj

=

t
(rit − yit )(wih − yit )gh

η
t

i

(xt − mhj )
j
2
sh

The update rule for sh as well as the rules for classiﬁcation can similarly
be derived. Let us compare these with the update rules for the RBF with
unnormalized Gaussians (equation 12.17). Here, we use gh instead of ph ,
which makes a unit’s update dependent not only on its own parameters,
but also on the centers and spreads of other units as well. Comparing
equation 12.27 with equation 12.18, we see that instead of wih , we have
(wih − yit ), which shows the role of normalization on the output. The
“responsible” unit wants to decrease the diﬀerence between its output,
wih , and the ﬁnal output, yit , proportional to its responsibility, gh .

12.6

Competitive Basis Functions
As we have seen up until now, in an RBF network the ﬁnal output is
determined as a weighted sum of the contributions of the local units.
Though the units are local, it is the ﬁnal weighted sum that is important
and that we want to make as close as possible to the required output. For
example, in regression we minimize equation 12.15, which is based on
the probabilistic model

(12.28)

√

p(r t |x t ) =
i

(r t − yit )2
1
exp − i
2σ 2
2π σ

yit

competitive basis
functions

is given by equation 12.16 (unnormalized) or equation 12.25
where
(normalized). In either case, we can view the model as a cooperative one
since the units cooperate to generate the ﬁnal output, yit . We now discuss
the approach using competitive basis functions where we assume that the
output is drawn from a mixture model
H

(12.29)

p(r t |x t ) =

p(h|x t )p(r t |h, x t )
h=1

p(h|x ) are the mixture proportions and p(r t |h, x t ) are the mixture components generating the output if that component is chosen. Note that
both of these terms depend on the input x.
t

298

12 Local Models

The mixture proportions are
(12.30)

p(h|x)

(12.31)

t
gh

p(x|h)p(h)
l p(x|l)p(l)

=
=

(12.32)

ah exp[− x t − m h
l

2

2
/2sh ]
2 /2s 2 ]
l

al exp[− xt − m l

We generally assume ah to be equal and ignore them. Let us ﬁrst take
the case of regression where the components are Gaussian. In equation 12.28, noise is added to the weighted sum; here, one component
t
is chosen and noise is added to its output, yih .
Using the mixture model of equation 12.29, the log likelihood is
⎤
⎡
1
t
t
L({m h , sh , wih }i,h |X) =
log gh exp ⎣−
(r t − yih )2 ⎦
2 i i
t
h
t
where yih = wih is the constant ﬁt done by component h for output i,
which, strictly speaking, does not depend on x. (In section 12.8.2, we
discuss the case of competitive mixture of experts where the local ﬁt is
t
a linear function of x.) We see that if gh is 1, then it is responsible for
generating the right output and needs to minimize the squared error of
t
its prediction, i (rit − yih )2 .
Using gradient ascent to maximize the log likelihood, we get

(12.33)

t
t
(rit − yih )fh

Δwih = η
t

where
(12.34)

t
fh

=

1
t
gh exp[− 2
l

(12.35)

p(h|r, x)

=

1
glt exp[− 2

t
i (ri

t
− yih )2 ]

t
i (ri

t
− yil )2 ]

p(h|x)p(r|h, x)
l p(l|x)p(r|l, x)

t
gh ≡ p(h|x t ) is the posterior probability of unit h given the input, and
t
it depends on the centers and spreads of all of the units. fh ≡ p(h|r, x t ) is
the posterior probability of unit h given the input and the desired output,
also taking the error into account in choosing the responsible unit.
Similarly, we can derive a rule to update the centers:

(12.36)

t
t
(fh − gh )

Δmhj = η
t

(xt − mhj )
j
2
sh

299

12.6 Competitive Basis Functions

fh is the posterior probability of unit h also taking the required output
into account, whereas gh is the posterior probability using only the input
space information. Their diﬀerence is the error term for the centers. Δsh
can be similarly derived. In the cooperative case, there is no force on the
units to be localized. To decrease the error, means and spreads can take
any value; it is even possible sometimes for the spreads to increase and
ﬂatten out. In the competitive case, however, to increase the likelihood,
units are forced to be localized with more separation between them and
smaller spreads.
In classiﬁcation, each component by itself is a multinomial. Then the
log likelihood is
(12.37)

L({m h , sh , wih }i,h |X)

=
t

=

(12.38)

h

t
(yih )ri
i

⎤

⎡

t
rit log yih ⎦

t
gh exp ⎣

log
t

t

t
gh

log

h

i

where
(12.39)

exp wih
k exp wkh

t
yih =

Update rules for wih , m h , and sh can be derived using gradient ascent,
which will include
(12.40)

t
fh =

t
t
gh exp[ i rit log yih ]
t
t
t
l gl exp[ i ri log yil ]

In chapter 7, we discussed the EM algorithm for ﬁtting Gaussian mixtures to data. It is possible to generalize EM for supervised learning as
t
t
well. Actually, calculating fh corresponds to the E-step. fh ≡ p(r|h, x t )
t
replaces p(h|x ), which we used in the E-step in chapter 7 when the application was unsupervised. In the M-step for regression, we update the
parameters as
(12.41)

mh

=

(12.42)

Sh

=

(12.43)

wih

=

t t
t fh x
t
t fh
t
t
t fh (x

− m h )(x t − m h )T
t
t fh

t t
t fh ri
t
t fh

We see that wih is a weighted average where weights are the posterior
probabilities of units, given the input and the desired output. In the case

300

12 Local Models

of classiﬁcation, the M-step has no analytical solution and one needs to
resort to an iterative procedure, for example, gradient ascent (Jordan and
Jacobs 1994).

12.7

Learning Vector Quantization
Let us say we have H units for each class, already labeled by those classes.
These units are initialized with random instances from their classes. At
each iteration, we ﬁnd the unit, m i , that is closest to the input instance
in Euclidean distance and use the following update rule:

(12.44)

learning vector
quantization

12.8

Δm i = η(x t − m i )
Δm i = −η(x t − m i )

if x t and m i have the same class label
otherwise

If the closest center has the correct label, it is moved toward the input
to better represent it. If it belongs to the wrong class, it is moved away
from the input in the expectation that if it is moved suﬃciently away, a
center of the correct class will be the closest in a future iteration. This
is the learning vector quantization (LVQ) model proposed by Kohonen
(1990, 1995).
The LVQ update equation is analogous to equation 12.36 where the
direction in which the center is moved depends on the diﬀerence between
two values: our prediction of the winner unit based on the input distances
and what the winner should be based on the required output.

Mixture of Experts
In RBFs, corresponding to each local patch we give a constant ﬁt. In
the case where for any input, we have one gh 1 and all others 0, we get
a piecewise constant approximation where for output i, the local ﬁt by
patch h is given by wih . From the Taylor expansion, we know that at each
point, the function can be written as

(12.45)

f (x) = f (a) + (x − a)f (a) + · · ·
Thus a constant approximation is good if x is close enough to a and
f (a) is close to 0—that is, if f (x) is ﬂat around a. If this is not the
case, we need to divide the space into a large number of patches, which
is particularly serious when the input dimensionality is high, due to the
curse of dimensionality.

301

12.8 Mixture of Experts

yi

wih
gh
mh , sh

vih

x1

xj

xd

Figure 12.10 The mixture of experts can be seen as an RBF network where the
second-layer weights are outputs of linear models. Only one linear model is
shown for clarity.

piecewise linear
approximation
mixture of experts

An alternative is to have a piecewise linear approximation by taking into
account the next term in the Taylor expansion, namely, the linear term.
This is what is done by mixture of experts (Jacobs et al. 1991). We write
H

(12.46)

yit =

t
wih gh
h=1

which is the same as equation 12.25 but here, wih , the contribution of
patch h to output i is not a constant but a linear function of the input:
(12.47)

t
wih = v T x t
ih

v ih is the parameter vector that deﬁnes the linear function and includes
a bias term, making the mixture of experts a generalization of the RBF
network. The unit activations can be taken as normalized RBFs:
(12.48)

t
gh =

exp[− xt − m h
l

exp[− x t − m l

2

2
/2sh ]
2 /2s 2 ]
l

This can be seen as an RBF network except that the second-layer weights
are not constants but are outputs of linear models (see ﬁgure 12.10). Jacobs et al. (1991) view this in another way: They consider w h as linear

302

12 Local Models

yi

gh

wih

Local
experts

Gating
network

wh
vih

x1

xj

mhj

xd

Figure 12.11 The mixture of experts can be seen as a model for combining
multiple models. w h are the models and the gating network is another model
determining the weight of each model, as given by gh . Viewed in this way, neither
the experts nor the gating are restricted to be linear.

models, each taking the input, and call them experts. gh are considered
to be the outputs of a gating network. The gating network works as a
classiﬁer does with its outputs summing to 1, assigning the input to one
of the experts (see ﬁgure 12.11).
Considering the gating network in this manner, any classiﬁer can be
used in gating. When x is high-dimensional, using local Gaussian units
may require a large number of experts and Jacobs et al. (1991) propose
to take
(12.49)

t
gh =

exp[m T x t ]
h
l

exp[m T x t ]
l

which is a linear classiﬁer. Note that m h are no longer centers but hyperplanes, and as such include bias values. This gating network is implementing a classiﬁcation where it is dividing linearly the input region for
which expert h is responsible from the expertise regions of other experts.
As we will see again in chapter 17, the mixture of experts is a general
architecture for combining multiple models; the experts and the gating

303

12.8 Mixture of Experts

may be nonlinear, for example, contain multilayer perceptrons, instead
of linear perceptrons (exercise 6).
An architecture similar to the mixture of experts and running line
smoother (section 8.6.3) has been proposed by Bottou and Vapnik (1992).
In their approach, no training is done initially. When a test instance is
given, a subset of the data close to the test instance is chosen from the
training set (as in the k-nearest neighbor, but with a large k), a simple
model, for example, a linear classiﬁer, is trained with this local data, the
prediction is made for the instance, and then the model is discarded. For
the next instance, a new model is created, and so on. On a handwritten
digit recognition application, this model has less error than the multilayer
perceptron, k-nearest neighbor, and Parzen windows; the disadvantage is
the need to train a new model on the ﬂy for each test instance.

12.8.1

Cooperative Experts
In the cooperative case, yit is given by equation 12.46, and we would like
to make it as close as possible to the required output, rit . In regression,
the error function is

(12.50)

E({mh , sh , wih }i,h |X) =

1
2

(rit − yit )2
t

i

Using gradient descent, second-layer (expert) weight parameters are
updated as
(12.51)

t
(rit − yit )gh x t

Δv ih = η
t

Compared with equation 12.26, we see that the only diﬀerence is that
this new update is a function of the input.
If we use softmax gating (equation 12.49), using gradient descent we
have the following update rule for the hyperplanes:
(12.52)

t
t
(rit − yit )(wih − yit )gh xt
j

Δmhj = η
t

i

If we use radial gating (equation 12.48), only the last term, ∂ph /∂mhj ,
diﬀers.
In classiﬁcation, we have
(12.53)

yi =

exp
k

exp

h

t
wih gh
t
h wkh gh

304

12 Local Models

with wih = v T x, and update rules can be derived to minimize the crossih
entropy using gradient descent (exercise 7).

12.8.2

Competitive Experts
Just like the competitive RBFs, we have
⎡

(12.54)

L({m h , sh , wih }i,h |X) =

log
t

where
(12.55)

t
yih

Δv ih

=

=

t
wih

h

⎤

1
t
gh exp ⎣−
2

t
(rit − yih )2 ⎦
i

= v ih x . Using gradient ascent, we get
t

t
t
(rit − yih )fh x t

η
t

(12.56)

Δm h

=

t
t
(fh − gh )x t

η
t

assuming softmax gating as given in equation 12.49.
In classiﬁcation, we have
(12.57)

L({m h , sh , wih }i,h |X)

=
t

=

(12.58)

t
(yih )ri
i

h

⎤

⎡

t
gh exp ⎣

log
t

t

t
gh

log

h

rit

t
log yih ⎦

i

where
(12.59)

t
yih =

t
exp wih
t =
k exp wkh

exp[v ih x t ]
t
k exp[v kh x ]

Jordan and Jacobs (1994) generalize EM for the competitive case with
local linear models. Alpaydın and Jordan (1996) compare cooperative
and competitive models for classiﬁcation tasks and see that the cooperative model is generally more accurate but the competitive version learns
faster. This is because in the cooperative case, models overlap more and
implement a smoother approximation, and thus it is preferable in regression problems. The competitive model makes a harder split; generally
only one expert is active for an input and therefore learning is faster.

12.9

hierarchical
mixture of experts

Hierarchical Mixture of Experts
In ﬁgure 12.11, we see a set of experts and a gating network that chooses
one of the experts as a function of the input. In a hierarchical mixture

12.10 Notes

305

of experts, we replace each expert with a complete system of mixture of
experts in a recursive manner (Jordan and Jacobs 1994). This architecture
may be seen as a decision tree (chapter 9) where gating networks can be
seen as decision nodes. When the gating network is linear, this is like the
linear multivariate decision tree discussed in section 9.6. The diﬀerence
is that the gating network does not make a hard decision but takes a
weighted sum of contributions coming from the children. Leaf nodes are
linear models, and their predictions are averaged and propagated up the
tree. The root gives the ﬁnal output, which is a weighted average of all of
the leaves. This is a soft decision tree as opposed to the decision trees we
saw before where only one path from the root to a leaf is taken.
Once an architecture is chosen—namely, the depth, the experts, and
the gating models—the whole tree can be learned from a labeled sample.
Jordan and Jacobs (1994) derive both gradient descent and EM learning
rules for such an architecture.

12.10

Notes
An RBF network can be seen as a neural network, implemented by a network of simple processing units. It diﬀers from a multilayer perceptron
in that the ﬁrst and second layers implement diﬀerent functions. Omohundro (1987) discusses how local models can be implemented as neural
networks and also addresses hierarchical data structures for fast localization of relevant local units. Specht (1991) shows how Parzen windows
can be implemented as a neural network.
Platt (1991) proposed an incremental version of RBF where new units
are added as necessary. Fritzke (1995) similarly proposed a growing version of SOM.
Lee (1991) compares k-nearest neighbor, multilayer perceptron, and
RBF network on a handwritten digit recognition application and concludes that these three methods all have small error rates. RBF networks learn faster than backpropagation on a multilayer perceptron but
use more parameters. Both of these methods are superior to the k-NN
in terms of classiﬁcation speed and memory need. Such practical constraints like time, memory, and computational complexity may be more
important than small diﬀerences in error rate in real-world applications.
Kohonen’s SOM (1990, 1995) was one of the most popular neural network methods, having been used in a variety of applications including

306

12 Local Models

generative
topographic
mapping

12.11

exploratory data analysis and as a preprocessing stage before a supervised learner. One interesting and successful application is the traveling salesman problem (Angeniol, Vaubois, and Le Texier 1988). Just like
the diﬀerence between k-means clustering and EM on Gaussian mixtures
(chapter 7), generative topographic mapping (GTM) (Bishop, Svensén, and
Williams 1998) is a probabilistic version of SOM that optimizes the log
likelihood of the data using a mixture of Gaussians whose means are constrained to lie on a two-dimensional manifold (for topological ordering in
low dimensions).
In an RBF network, once the centers and spreads are ﬁxed (for example,
by choosing a random subset of training instances as centers, as in the
anchor method), training the second layer is a linear model. This model is
equivalent to support vector machines with Gaussian kernels where during learning the best subset of instances, named the support vectors, are
chosen; we discuss them in chapter 13. Gaussian processes (chapter 14)
where we interpolate from stored training instances are also similar.

Exercises
1. Show an RBF network that implements XOR.
2. Write down the RBF network that uses elliptic units instead of radial units as
in equation 12.13.
3. Derive the update equations for the RBF network for classiﬁcation (equations
12.20 and 12.21).
4. Show how the system given in equation 12.22 can be trained.
5. Compare the number of parameters of a mixture of experts architecture with
an RBF network.
6. Formalize a mixture of experts architecture where the experts and the gating
network are multilayer perceptrons. Derive the update equations for regression and classiﬁcation.
7. Derive the update equations for the cooperative mixture of experts for classiﬁcation.
8. Derive the update equations for the competitive mixture of experts for classiﬁcation.
9. Formalize the hierarchical mixture of experts architecture with two levels.
Derive the update equations using gradient descent for regression and classiﬁcation.

12.12 References

307

10. In mixture of experts, because diﬀerent experts specialize in diﬀerent parts
of the input space, they may need to focus on diﬀerent inputs. Discuss how
dimensionality can be locally reduced in the experts.

12.12

References
Alpaydın, E., and M. I. Jordan. 1996. “Local Linear Perceptrons for Classiﬁcation.” IEEE Transactions on Neural Networks 7: 788–792.
Angeniol, B., G. Vaubois, and Y. Le Texier. 1988. “Self Organizing Feature Maps
and the Travelling Salesman Problem.” Neural Networks 1: 289–293.
Berthold, M. 1999. “Fuzzy Logic.” In Intelligent Data Analysis: An Introduction,
ed. M. Berthold and D. J. Hand, 269–298. Berlin: Springer.
Bishop, C. M., M. Svensén, and C. K. I. Williams. 1998. “GTM: The Generative
Topographic Mapping.” Neural Computation 10: 215–234.
Bottou, L., and V. Vapnik. 1992. “Local Learning Algorithms.” Neural Computation 4: 888–900.
Broomhead, D. S., and D. Lowe. 1988. “Multivariable Functional Interpolation
and Adaptive Networks.” Complex Systems 2: 321–355.
Carpenter, G. A., and S. Grossberg. 1988. “The ART of Adaptive Pattern Recognition by a Self-Organizing Neural Network.” IEEE Computer 21(3): 77–88.
Cherkassky, V., and F. Mulier. 1998. Learning from Data: Concepts, Theory,
and Methods. New York: Wiley.
DeSieno, D. 1988. “Adding a Conscience Mechanism to Competitive Learning.”
In IEEE International Conference on Neural Networks, 117–124. Piscataway,
NJ: IEEE Press.
Feldman, J. A., and D. H. Ballard. 1982. “Connectionist Models and their Properties.” Cognitive Science 6: 205–254.
Fritzke, B. 1995. “Growing Cell Structures: A Self Organizing Network for Unsupervised and Supervised Training.” Neural Networks 7: 1441–1460.
Grossberg, S. 1980. “How does the Brain Build a Cognitive Code?” Psychological
Review 87: 1–51.
Hertz, J., A. Krogh, and R. G. Palmer. 1991. Introduction to the Theory of Neural
Computation. Reading, MA: Addison Wesley.
Jacobs, R. A., M. I. Jordan, S. J. Nowlan, and G. E. Hinton. 1991. “Adaptive
Mixtures of Local Experts.” Neural Computation 3: 79–87.
Jordan, M. I., and R. A. Jacobs. 1994. “Hierarchical Mixtures of Experts and the
EM Algorithm.” Neural Computation 6: 181–214.

308

12 Local Models

Kohonen, T. 1990. “The Self-Organizing Map.” Proceedings of the IEEE 78:
1464–1480.
Kohonen, T. 1995. Self-Organizing Maps. Berlin: Springer.
Lee, Y. 1991. “Handwritten Digit Recognition Using k-Nearest Neighbor, Radial
Basis Function, and Backpropagation Neural Networks.” Neural Computation
3: 440–449.
Mao, J., and A. K. Jain. 1995. “Artiﬁcial Neural Networks for Feature Extraction
and Multivariate Data Projection.” IEEE Transactions on Neural Networks 6:
296–317.
Moody, J., and C. Darken. 1989. “Fast Learning in Networks of Locally-Tuned
Processing Units.” Neural Computation 1: 281–294.
Oja, E. 1982. “A Simpliﬁed Neuron Model as a Principal Component Analyzer.”
Journal of Mathematical Biology 15: 267–273.
Omohundro, S. M. 1987. “Eﬃcient Algorithms with Neural Network Behavior.”
Complex Systems 1: 273–347.
Platt, J. 1991. “A Resource Allocating Network for Function Interpolation.” Neural Computation 3: 213–225.
Specht, D. F. 1991. “A General Regression Neural Network.” IEEE Transactions
on Neural Networks 2: 568–576.
Tresp, V., J. Hollatz, and S. Ahmad. 1997. “Representing Probabilistic Rules
with Networks of Gaussian Basis Functions.” Machine Learning 27: 173–200.

13

Kernel Machines

Kernel machines are maximum margin methods that allow the model
to be written as a sum of the inﬂuences of a subset of the training instances. These inﬂuences are given by application-speciﬁc similarity
kernels, and we discuss “kernelized” classiﬁcation, regression, outlier detection, and dimensionality reduction, as well as how to choose
and use kernels.

13.1

Introduction
We now discuss a diﬀerent approach for linear classiﬁcation and regression. We should not be surprised to have so many diﬀerent methods
even for the simple case of a linear model. Each learning algorithm has
a diﬀerent inductive bias, makes diﬀerent assumptions, and deﬁnes a
diﬀerent objective function and thus may ﬁnd a diﬀerent linear model.
The model that we will discuss in this chapter, called the support vector
machine (SVM), and later generalized under the name kernel machine, has
been popular in recent years for a number of reasons:
1. It is a discriminant-based method and uses Vapnik’s principle to never
solve a more complex problem as a ﬁrst step before the actual problem (Vapnik 1995). For example, in classiﬁcation, when the task is to
learn the discriminant, it is not necessary to estimate where the class
densities p(x|Ci ) or the exact posterior probability values P (Ci |x); we
only need to estimate where the class boundaries lie, that is, x where
P (Ci |x) = P (Cj |x). Similarly, for outlier detection, we do not need to
estimate the full density p(x); we only need to ﬁnd the boundary separating those x that have low p(x), that is, x where p(x) < θ, for some
threshold θ ∈ (0, 1).

310

13 Kernel Machines

2. After training, the parameter of the linear model, the weight vector,
can be written down in terms of a subset of the training set, which are
the so-called support vectors. In classiﬁcation, these are the cases that
are close to the boundary and as such, knowing them allows knowledge extraction: those are the uncertain or erroneous cases that lie in
the vicinity of the boundary between two classes. Their number gives
us an estimate of the generalization error, and, as we see below, being
able to write the model parameter in terms of a set of instances allows
kernelization.
3. As we will see shortly, the output is written as a sum of the inﬂuences of support vectors and these are given by kernel functions that
are application-speciﬁc measures of similarity between data instances.
Previously, we talked about nonlinear basis functions allowing us to
map the input to another space where a linear (smooth) solution is
possible; the kernel function uses the same idea.
4. Typically in most learning algorithms, data points are represented as
vectors, and either dot product (as in the multilayer perceptrons) or
Euclidean distance (as in radial basis function networks) is used. A
kernel function allows us to go beyond that. For example, G1 and G2
may be two graphs and K(G1 , G2 ) may correspond to the number of
shared paths, which we can calculate without needing to represent G1
or G2 explicitly as vectors.
5. Kernel-based algorithms are formulated as convex optimization problems, and there is a single optimum that we can solve for analytically.
Therefore we are no longer bothered with heuristics for learning rates,
initializations, checking for convergence, and such. Of course, this
does not mean that we do not have any hyperparameters for model
selection; we do—any method needs them, to match the algorithm to
the data at hand.
We start our discussion with the case of classiﬁcation, and then generalize to regression, outlier (novelty) detection, and then dimensionality
reduction. We see that in all cases basically we have the similar quadratic
program template to maximize the separability, or margin, of instances
subject to a constraint of the smoothness of solution. Solving for it, we
get the support vectors. The kernel function deﬁnes the space according
to its notion of similarity and a kernel function is good if we have better
separation in its corresponding space.

13.2 Optimal Separating Hyperplane

13.2

311

Optimal Separating Hyperplane
Let us start again with two classes and use labels −1/ + 1 for the two
classes. The sample is X = {x t , r t } where r t = +1 if x t ∈ C1 and r t = −1
if x t ∈ C2 . We would like to ﬁnd w and w0 such that
w T x t + w0 ≥ +1

for

r t = +1

w T x t + w0 ≤ −1

for

r t = −1

which can be rewritten as
(13.1)

r t (w T x t + w0 ) ≥ +1
Note that we do not simply require
r t (w T x t + w0 ) ≥ 0

margin

optimal separating
hyperplane

Not only do we want the instances to be on the right side of the hyperplane, but we also want them some distance away, for better generalization. The distance from the hyperplane to the instances closest to it
on either side is called the margin, which we want to maximize for best
generalization.
Very early on, in section 2.1, we talked about the concept of the margin
when we were talking about ﬁtting a rectangle, and we said that it is
better to take a rectangle halfway between S and G, to get a breathing
space. This is so that in case noise shifts a test instance slightly, it will
still be on the right side of the boundary.
Similarly, now that we are using the hypothesis class of lines, the optimal separating hyperplane is the one that maximizes the margin.
We remember from section 10.3 that the distance of x t to the discriminant is
|w T x t + w0 |
w
which, when r t ∈ {−1, +1}, can be written as
r t (w T x t + w0 )
w
and we would like this to be at least some value ρ:

(13.2)

r t (w T x t + w0 )
≥ ρ, ∀t
w

312

13 Kernel Machines

We would like to maximize ρ but there are an inﬁnite number of solutions that we can get by scaling w and for a unique solution, we ﬁx
ρ w = 1 and thus, to maximize the margin, we minimize w . The task
can therefore be deﬁned (see Cortes and Vapnik 1995; Vapnik 1995) as
to
(13.3)

min

1
w
2

2

subject to r t (w T x t + w0 ) ≥ +1, ∀t

This is a standard quadratic optimization problem, whose complexity
depends on d, and it can be solved directly to ﬁnd w and w0 . Then, on
both sides of the hyperplane, there will be instances that are 1/ w away
from the hyperplane and the total margin will be 2/ w .
We saw in section 10.2 that if the problem is not linearly separable,
instead of ﬁtting a nonlinear function, one trick is to map the problem to
a new space by using nonlinear basis functions. It is generally the case
that this new space has many more dimensions than the original space,
and, in such a case, we are interested in a method whose complexity does
not depend on the input dimensionality.
In ﬁnding the optimal hyperplane, we can convert the optimization
problem to a form whose complexity depends on N, the number of training instances, and not on d. Another advantage of this new formulation
is that it will allow us to rewrite the basis functions in terms of kernel
functions, as we will see in section 13.5.
To get the new formulation, we ﬁrst write equation 13.3 as an unconstrained problem using Lagrange multipliers αt :

(13.4)

N

=

1
w
2

2

=

Lp

1
w
2

2

αt [r t (w T x t + w0 ) − 1]

−
t=1

αt r t (w T x t + w0 ) +

−
t

αt
t

This should be minimized with respect to w, w0 and maximized with
respect to αt ≥ 0. The saddle point gives the solution.
This is a convex quadratic optimization problem because the main term
is convex and the linear constraints are also convex. Therefore, we can
equivalently solve the dual problem, making use of the Karush-KuhnTucker conditions. The dual is to maximize Lp with respect to αt , subject
to the constraints that the gradient of Lp with respect to w and w0 are 0

313

13.2 Optimal Separating Hyperplane

and also that αt ≥ 0:
(13.5)

∂Lp
=0
∂w

(13.6)

∂Lp
=0
∂w0

⇒

αt r t x t

w=
t
t t

⇒

α r =0
t

Plugging these into equation 13.4, we get the dual
=

1 T
(w w) − w T
2

=

Ld

1
− (w T w) +
2

=

(13.7)

−

1
2

αt r t x t − w0
t

t

αt
t

αt
t

αt αs r t r s (x t )T x s +
t

αt r t +

s

αt
t

which we maximize with respect to αt only, subject to the constraints
αt r t = 0, and αt ≥ 0, ∀t
t

This can be solved using quadratic optimization methods. The size of
the dual depends on N, sample size, and not on d, the input dimensionality. The upper bound for time complexity is O(N 3 ), and the upper bound
for space complexity is O(N 2 ).
Once we solve for αt , we see that though there are N of them, most
vanish with αt = 0 and only a small percentage have αt > 0. The set of x t
whose αt > 0 are the support vectors, and as we see in equation 13.5, w is
written as the weighted sum of these training instances that are selected
as the support vectors. These are the x t that satisfy
r t (w T x t + w0 ) = 1
and lie on the margin. We can use this fact to calculate w0 from any
support vector as
(13.8)

support vector
machine

w0 = r t − w T x t
For numerical stability, it is advised that this be done for all support
vectors and an average be taken. The discriminant thus found is called
the support vector machine (SVM) (see ﬁgure 13.1).
The majority of the αt are 0, for which r t (w T x t + w0 ) > 1. These
are the x t that lie more than suﬃciently away from the discriminant,

314

13 Kernel Machines

2

1.5

1

1
0.5

0
0

−1
0.5

1

1.5

2

Figure 13.1 For a two-class problem where the instances of the classes are
shown by plus signs and dots, the thick line is the boundary and the dashed lines
deﬁne the margins on either side. Circled instances are the support vectors.

and they have no eﬀect on the hyperplane. The instances that are not
support vectors carry no information; even if any subset of them are
removed, we would still get the same solution. From this perspective,
the SVM algorithm can be likened to the condensed nearest neighbor algorithm (section 8.5), which stores only the instances neighboring (and
hence constraining) the class discriminant.
Being a discriminant-based method, the SVM cares only about the instances close to the boundary and discards those that lie in the interior.
Using this idea, it is possible to use a simpler classiﬁer before the SVM
to ﬁlter out a large portion of such instances, thereby decreasing the
complexity of the optimization step of the SVM (exercise 1).
During testing, we do not enforce a margin. We calculate g(x) = w T x +
w0 , and choose according to the sign of g(x):
Choose C1 if g(x) > 0 and C2 otherwise

315

13.3 The Nonseparable Case: Soft Margin Hyperplane

13.3

slack variables

(13.9)

soft error

The Nonseparable Case: Soft Margin Hyperplane
If the data is not linearly separable, the algorithm we discussed earlier
will not work. In such a case, if the two classes are not linearly separable
such that there is no hyperplane to separate them, we look for the one
that incurs the least error. We deﬁne slack variables, ξ t ≥ 0, which store
the deviation from the margin. There are two types of deviation: An
instance may lie on the wrong side of the hyperplane and be misclassiﬁed.
Or, it may be on the right side but may lie in the margin, namely, not
suﬃciently away from the hyperplane. Relaxing equation 13.1, we require
r t (w T x t + w0 ) ≥ 1 − ξ t
If ξ t = 0, there is no problem with x t . If 0 < ξ t < 1, x t is correctly
classiﬁed but in the margin. If ξ t ≥ 1, x t is misclassiﬁed (see ﬁgure 13.2).
The number of misclassiﬁcations is #{ξ t > 1}, and the number of nonseparable points is #{ξt > 0}. We deﬁne soft error as
ξt
t

and add this as a penalty term:
(13.10)

Lp =

1
w
2

2

ξt

+C
t

subject to the constraint of equation 13.9. C is the penalty factor as in
any regularization scheme trading oﬀ complexity, as measured by the L2
norm of the weight vector (similar to weight decay in multilayer perceptrons; see section 11.9), and data misﬁt, as measured by the number of
nonseparable points. Note that we are penalizing not only the misclassiﬁed points but also the ones in the margin for better generalization,
though these latter would be correctly classiﬁed during testing.
Adding the constraints, the Lagrangian of equation 13.4 then becomes
(13.11)

Lp =

1
w
2

2

ξt −

+C
t

αt [r t (w T x t + w0 ) − 1 + ξ t ] −
t

μt ξ t
t

where μt are the new Lagrange parameters to guarantee the positivity of
ξ t . When we take the derivatives with respect to the parameters and set
them to 0, we get:
(13.12)

∂Lp
∂w

=

αt r t x t = 0 ⇒ w =

w−
t

αt r t x t
t

316

13 Kernel Machines

2
(a)

1.5
(b)

1

(c)

(d)

0.5
−1
0
0

0.5

1
1

1.5

2

Figure 13.2 In classifying an instance, there are four possible cases: In (a), the
instance is on the correct side and far away from the margin; r t g(x t ) > 1, ξ t = 0.
In (b), ξ t = 0; it is on the right side and on the margin. In (c), ξ t = 1 − g(x t ), 0 <
ξ < 1; it is on the right side but is in the margin and not suﬃciently away. In (d),
ξ t = 1 + g(x t ) > 1; it is on the wrong side—this is a misclassiﬁcation. All cases
except (a) are support vectors. In terms of the dual variable, in (a), αt = 0; in (b),
αt < C; in (c) and (d), αt = C.

(13.13)

∂Lp
∂w0

=

(13.14)

∂Lp
∂ξ t

=

αt r t = 0
t

C − αt − μ t = 0

Since μ t ≥ 0, this last implies that 0 ≤ αt ≤ C. Plugging these into
equation 13.11, we get the dual that we maximize with respect to αt :
(13.15)

αt −

Ld =
t

1
2

αt αs r t r s (x t )T x s
t

s

subject to
αt r t = 0 and 0 ≤ αt ≤ C, ∀t
t

13.3 The Nonseparable Case: Soft Margin Hyperplane

317

Solving this, we see that as in the separable case, instances that lie on
the correct side of the boundary with suﬃcient margin vanish with their
αt = 0 (see ﬁgure 13.2). The support vectors have their αt > 0 and they
deﬁne w, as given in equation 13.12. Of these, those whose αt < C are
the ones that are on the margin, and we can use them to calculate w0 ;
they have ξ t = 0 and satisfy r t (w T x t + w0 ) = 1. Again, it is better to
take an average over these w0 estimates. Those instances that are in the
margin or misclassiﬁed have their αt = C.
The nonseparable instances that we store as support vectors are the
instances that we would have trouble correctly classifying if they were
not in the training set; they would either be misclassiﬁed or classiﬁed
correctly but not with enough conﬁdence. We can say that the number
of support vectors is an upper-bound estimate for the expected number
of errors. And, actually, Vapnik (1995) has shown that the expected test
error rate is
EN [P (er r or )] ≤

hinge loss

(13.16)

EN [# of support vectors]
N

where EN [·] denotes expectation over training sets of size N. The nice
implication of this is that it shows that the error rate depends on the
number of support vectors and not on the input dimensionality.
Equation 13.9 implies that we deﬁne error if the instance is on the
wrong side or if the margin is less than 1. This is called the hinge loss. If
y t = w T x t + w0 is the output and r t is the desired output, hinge loss is
deﬁned as
Lhinge (y t , r t ) =

0
1 − ytrt

if y t r t ≥ 1
otherwise

In ﬁgure 13.3, we compare hinge loss with 0/1 loss, squared error,
and cross-entropy. We see that diﬀerent from 0/1 loss, hinge loss also
penalizes instances in the margin even though they may be on the correct
side, and the loss increases linearly as the instance moves away on the
wrong side. This is diﬀerent from the squared loss that therefore is not as
robust as the hinge loss. We see that cross-entropy minimized in logistic
discrimination (section 10.7) or by the linear perceptron (section 11.3), is
a good continuous approximation to the hinge loss.
C of equation 13.10 is the regularization parameter ﬁne-tuned using
cross-validation. It deﬁnes the trade-oﬀ between margin maximization
and error minimization: If it is too large, we have a high penalty for
nonseparable points, and we may store many support vectors and overﬁt.

318

13 Kernel Machines

9
8

loss for rt = 1

7
6
5

squared error

4
3

hinge loss

2

cross entropy

1
0/1 loss

0
−2

−1

0
t
y

1

2

Figure 13.3 Comparison of diﬀerent loss functions for r t = 1: 0/1 loss is 0 if
y t = 1, 1 otherwise. Hinge loss is 0 if y t > 1, 1 − y t otherwise. Squared error is
(1 − y t )2 . Cross-entropy is log(1/(1 + exp(−y t ))).

If it is too small, we may ﬁnd too simple solutions that underﬁt. Typically,
one chooses from [10−6 , 10−5 , . . . , 10+5 , 10+6 ] in the log scale by looking
at the accuracy on a validation set.

13.4

ν-SVM
There is another, equivalent formulation of the soft margin hyperplane
that uses a parameter ν ∈ [0, 1] instead of C (Schölkopf et al. 2000). The
objective function is

(13.17)

min

1
w
2

2

− νρ +

1
N

ξt
t

subject to
(13.18)

r t (w T x t + w0 ) ≥ ρ − ξ t , ξ t ≥ 0, ρ ≥ 0
ρ is a new parameter that is a variable of the optimization problem and
scales the margin: the margin is now 2ρ/ w . ν has been shown to be a

319

13.5 Kernel Trick

lower bound on the fraction of support vectors and an upper bound on
the fraction of instances having margin errors ( t #{ξ t > 0}). The dual is
(13.19)

Ld = −

1
2

αt αs r t r s (x t )T x s
t

s

subject to
αt r t = 0, 0 ≤ αt ≤
t

1
,
N

αt ≤ ν
t

When we compare equation 13.19 with equation 13.15, we see that
the term t αt no longer appears in the objective function but is now
a constraint. By playing with ν, we can control the fraction of support
vectors, and this is advocated to be more intuitive than playing with C.

13.5

Kernel Trick
Section 10.2 demonstrated that if the problem is nonlinear, instead of
trying to ﬁt a nonlinear model, we can map the problem to a new space
by doing a nonlinear transformation using suitably chosen basis functions and then use a linear model in this new space. The linear model
in the new space corresponds to a nonlinear model in the original space.
This approach can be used in both classiﬁcation and regression problems, and in the special case of classiﬁcation, it can be used with any
scheme. In the particular case of support vector machines, it leads to
certain simpliﬁcations that we now discuss.
Let us say we have the new dimensions calculated through the basis
functions
z = φ(x) where zj = φj (x), j = 1, . . . , k
mapping from the d-dimensional x space to the k-dimensional z space
where we write the discriminant as
g(z)

=

wT z

g(x)

=

w T φ(x)
k

(13.20)

=

wj φj (x)
j=1

where we do not use a separate w0 ; we assume that z1 = φ1 (x) ≡ 1. Generally, k is much larger than d and k may also be larger than N, and there

320

13 Kernel Machines

lies the advantage of using the dual form whose complexity depends on
N, whereas if we used the primal it would depend on k. We also use the
more general case of the soft margin hyperplane here because we have
no guarantee that the problem is linearly separable in this new space.
The problem is the same
(13.21)

Lp =

1
w
2

2

ξt

+C
t

except that now the constraints are deﬁned in the new space
(13.22)

r t w T φ(x t ) ≥ 1 − ξ t
The Lagrangian is

(13.23)

Lp =

1
w
2

2

ξt −

+C
t

αt r t w T φ(x t ) − 1 + ξ t −
t

μt ξ t
t

When we take the derivatives with respect to the parameters and set
them to 0, we get
(13.24)
(13.25)

∂Lp
∂w

=

∂Lp
∂ξ t

=

αt r t φ(x t )

w=
t

C − αt − μ t = 0

The dual is now
(13.26)

αt −

Ld =
t

1
2

αt αs r t r s φ(x t )T φ(x s )
t

s

subject to
αt r t = 0 and 0 ≤ αt ≤ C, ∀t
t

kernel function

(13.27)

The idea in kernel machines is to replace the inner product of basis functions, φ(x t )T φ(x s ), by a kernel function, K(x t , x s ), between instances in the original input space. So instead of mapping two instances
x t and x s to the z-space and doing a dot product there, we directly apply
the kernel function in the original space.
αt −

Ld =
t

1
2

αt αs r t r s K(x t , x s )
t

s

321

13.6 Vectorial Kernels

The kernel function also shows up in the discriminant
g(x)

=

w T φ(x) =

αt r t φ(x t )T φ(x)
t

(13.28)

αt r t K(x t , x)

=
t

kernelization

Gram matrix

13.6

This implies that if we have the kernel function, we do not need to map
it to the new space at all. Actually, for any valid kernel, there does exist
a corresponding mapping function, but it may be much simpler to use
K(x t , x) rather than calculating φ(x t ), φ(x) and taking the dot product.
Many algorithms have been kernelized, as we will see in later sections,
and that is why we have the name “kernel machines.”
The matrix of kernel values, K, where Kts = K(x t , x s ), is called the Gram
matrix, which should be symmetric and positive semideﬁnite. Recently,
it has become standard practice in sharing data sets to have available
only the K matrices without providing x t or φ(x t ). Especially in bioinformatics or natural language processing applications where x (or φ(x)) has
hundreds or thousands of dimensions, storing/downloading the N × N
matrix is much cheaper (Vert, Tsuda, and Schölkopf 2004); this, however,
implies that we can use only those available for training/testing and cannot use the trained model to make predictions outside this data set.

Vectorial Kernels
The most popular, general-purpose kernel functions are
polynomials of degree q:

(13.29)

K(x t , x) = (x T x t + 1)q
where q is selected by the user. For example, when q = 2 and d = 2,
=

(x T y + 1)2

=

(x1 y1 + x2 y2 + 1)2

=

K(x, y)

2
2
1 + 2x1 y1 + 2x2 y2 + 2x1 x2 y1 y2 + x2 y1 + x2 y2
1
2

corresponds to the inner product of the basis function (Cherkassky
and Mulier 1998):
√
√
√
φ(x) = [1, 2x1 , 2x2 , 2x1 x2 , x2 , x2 ]T
1
2

322

13 Kernel Machines

2

1.5

−1

1

1

0.5

0
0

0.5

1

1.5

2

Figure 13.4 The discriminant and margins found by a polynomial kernel of
degree 2. Circled instances are the support vectors.

An example is given in ﬁgure 13.4. When q = 1, we have the linear
kernel that corresponds to the original formulation.
radial-basis functions:
(13.30)

K(x t , x) = exp −

xt − x
2s 2

2

deﬁnes a spherical kernel as in Parzen windows (chapter 8) where x t is
the center and s, supplied by the user, deﬁnes the radius. This is also
similar to radial basis functions that we discuss in chapter 12.
An example is shown in ﬁgure 13.5 where we see that larger spreads
smooth the boundary; the best value is found by cross-validation.
Note that when there are two parameters to be optimized using crossvalidation, for example, here C and s 2 , one should do a grid (factorial)
search in the two dimensions; we will discuss methods for searching
the best combination of such factors in section 19.2.
One can have a Mahalanobis kernel, generalizing from the Euclidean
distance:
(13.31)

1
K(x t , x) = exp − (x t − x)T S−1 (x t − x)
2

323

13.6 Vectorial Kernels

2

2

(b) s =0.5

(a) s =2
2

2
−1
1

1

1

1

−1

0
0

1

2

0
0

2

1

2

2

(c) s =0.25

(d) s =0.1

2

2
1

−1

1

1

1
−1

0
0

−1

−1

1

2

0
0

1

2

Figure 13.5 The boundary and margins found by the Gaussian kernel with different spread values, s 2 . We get smoother boundaries with larger spreads.

where S is a covariance matrix. Or, in the most general case,

(13.32)

K(x t , x) = exp −

D(x t , x)
2s 2

for some distance function D(x t , x).
sigmoidal functions:
(13.33)

K(x t , x) = tanh(2x T x t + 1)
where tanh(·) has the same shape with sigmoid, except that it ranges
between −1 and +1. This is similar to multilayer perceptrons that we
discussed in chapter 11.

324

13 Kernel Machines

13.7

bag of words

edit distance

alignment

Deﬁning Kernels
It is also possible to deﬁne application-speciﬁc kernels. Kernels are generally considered to be measures of similarity in the sense that K(x, y)
takes a larger value as x and y are more “similar,” from the point of view
of the application. This implies that any prior knowledge we have regarding the application can be provided to the learner through appropriately
deﬁned kernels—“kernel engineering”—and such use of kernels can be
seen as another example of a “hint” (section 11.8.4).
There are string kernels, tree kernels, graph kernels, and so on (Vert,
Tsuda, and Schölkopf 2004), depending on how we represent the data
and how we measure similarity in that representation.
For example, given two documents, the number of words appearing
in both may be a kernel. Let us say D1 and D2 are two documents and
one possible representation is called bag of words where we predeﬁne
M words relevant for the application, and we deﬁne φ(D1 ) as the Mdimensional binary vector whose dimension i is 1 if word i appears in
D1 and is 0 otherwise. Then, φ(D1 )T φ(D2 ) counts the number of shared
words. Here, we see that if we directly deﬁne and implement K(D1 , D2 )
as the number of shared words, we do not need to preselect M words
and can use just any word in the vocabulary (of course, after discarding
uninformative words like “of,” “and,” etc.) and we would not need to
generate the bag-of-words representation explicitly and it would be as if
we allowed M to be as large as we want.
Sometimes—for example, in bioinformatics applications—we can calculate a similarity score between two objects, which may not necessarily be
positive semideﬁnite. Given two strings (of genes), a kernel measures the
edit distance, namely, how many operations (insertions, deletions, substitutions) it takes to convert one string into another; this is also called
alignment. In such a case, a trick is to deﬁne a set of M templates and
represent an object as the M-dimensional vector of scores to all the templates. That is, if m i , i = 1, . . . , M are the templates and s(x t , m i ) is the
score between x t and m i , then we deﬁne
φ(x t ) = [s(x t , m 1 ), s(x t , m 2 ), . . . , s(x t , m M )]T

empirical kernel
map

and we deﬁne the empirical kernel map as
K(x t , x s ) = φ(x t )T φ(x s )
which is a valid kernel.

325

13.8 Multiple Kernel Learning

diffusion kernel

Sometimes, we have a binary score function; for example, two proteins
may interact or not, and we want to be able to generalize from this to
scores for two arbitrary instances. In such a case, a trick is to deﬁne a
graph where the nodes are the instances and two nodes are linked if they
interact, that is, if the binary score returns 1. Then we say that two nodes
that are not immediately linked are “similar” if the path between them is
short or if they are connected by many paths. This converts pairwise local
interactions to a global similarity measure, rather like deﬁning a geodesic
distance used in Isomap (section 6.7), and it is called the diﬀusion kernel.
If p(x) is a probability density, then
K(x t , x) = p(xt )p(x)

Fisher kernel

13.8

(13.34)

is a valid kernel. This is used when p(x) is a generative model for x measuring how likely it is that we see x. For example, if x is a sequence, p(x)
can be a hidden Markov model (chapter 15). With this kernel, K(x t , x) will
take a high value if both x t and x are likely to have been generated by the
same model. It is also possible to parametrize the generative model as
p(x|θ) and learn θ from data; this is called the Fisher kernel (Jaakkola
and Haussler 1998).

Multiple Kernel Learning
It is possible to construct new kernels by combining simpler kernels. If
K1 (x, y) and K2 (x, y) are valid kernels and c a constant, then
⎧
⎪ cK1 (x, y)
⎨
K (x, y) + K2 (x, y)
K(x, y) =
⎪ 1
⎩
K1 (x, y) · K2 (x, y)
are also valid.
Diﬀerent kernels may also be using diﬀerent subsets of x. We can
therefore see combining kernels as another way to fuse information from
diﬀerent sources where each kernel measures similarity according to its
domain. When we have input from two representations A and B

(13.35)

=

φA (x A )T φA (y A ) + φB (x B )T φB (y B )

=

KA (x A , y A ) + KB (x B , y B )

φ(x)T φ(y)

=

K(x, y)

326

13 Kernel Machines

where x = [x A , x B ] is the concatenation of the two representations. That
is, taking a sum of two kernels corresponds to doing a dot product in the
concatenated feature vectors. One can generalize to a number of kernels
m

(13.36)

Ki (x, y)

K(x, y) =
i=1

which, similar to taking an average of classiﬁers (section 17.4), this time
averages over kernels and frees us from the need to choose one particular
kernel. It is also possible to take a weighted sum and also learn the
weights from data (Lanckriet et al. 2004; Sonnenburg et al. 2006):
m

(13.37)

K(x, y) =

ηi Ki (x, y)
i=1

multiple kernel
learning

(13.38)

subject to ηi ≥ 0, with or without the constraint of i ηi = 1, respectively
known as convex or conic combination. This is called multiple kernel
learning where we replace a single kernel with a weighted sum. The single
kernel objective function of equation 13.27 becomes
αt −

Ld =
t

1
2

αt αs r t r s
t

s

ηi Ki (x t , x s )
i

which we solve for both the support vector machine parameters αt and
the kernel weights ηi . Then, the combination of multiple kernels also
appear in the discriminant
(13.39)

αt r t

g(x) =
t

ηi Ki (x t , x)
i

After training, ηi will take values depending on how the corresponding
kernel Ki (x t , x) is useful in discriminating. It is also possible to localize
kernels by deﬁning kernel weights as a parameterized function of the
input x, rather like the gating function in mixture of experts (section 17.8)
(13.40)

αt r t

g(x) =
t

ηi (x|θi )Ki (x t , x)
i

and the gating parameters θi are learned together with the support vector
machine parameters (Gönen and Alpaydın 2008).
When we have information coming from multiple sources in diﬀerent
representations or modalities—for example, in speech recognition where
we may have both acoustic and visual lip image—the usual approach is to
feed them separately to diﬀerent classiﬁers and then fuse the decisions;

13.9 Multiclass Kernel Machines

327

we will discuss methods for this in detail in chapter 17. Combining multiple kernels provides us with another way of integrating input from multiple sources, where there is a single classiﬁer that uses diﬀerent kernels
for inputs of diﬀerent sources, for which there are diﬀerent notions of
similarity (Noble 2004). The localized version can then seen be an extension of this where we can choose between sources, and hence similarity
measures, depending on the input.

13.9

Multiclass Kernel Machines
When there are K > 2 classes, the straightforward, one-vs.-all way is to
deﬁne K two-class problems, each one separating one class from all other
classes combined and learn K support vector machines gi (x), i = 1, . . . , K.
That is, in training gi (x), examples of Ci are labeled +1 and examples of
Ck , k = i are labeled as −1. During testing, we calculate all gi (x) and
choose the maximum.
Platt (1999) proposed to ﬁt a sigmoid to the output of a single (2-class)
SVM output to convert to a posterior probability. Similarly, one can train
one layer of softmax outputs to minimize cross-entropy to generate K >
2 posterior probabilities (Mayoraz and Alpaydın 1999):
K

(13.41)

yi (x) =

vij fj (x) + vi0
j=1

ECOC

where fj (x) are the SVM outputs and yi are the posterior probability outputs. Weights vij are trained to minimize cross-entropy. Note, however,
that as in stacking (section 17.9), the data on which we train vij should
be diﬀerent from the data used to train the base SVMs fj (x), to alleviate
overﬁtting.
Instead of the usual approach of building K two-class SVM classiﬁers to
separate one from all the rest, as with any other classiﬁer, one can build
K(K − 1)/2 pairwise classiﬁers (see also section 10.4), each gij (x) taking
examples of Ci with the label +1, examples of Cj with the label −1, and
not using examples of the other classes. Separating classes in pairs is
normally expected to be an easier job, with the additional advantage that
because we use less data, the optimizations will be faster, noting however
that we have O(K 2 ) discriminants to train instead of O(K).
In the general case, both one-vs.-all and pairwise separation are special
cases of the error-correcting output codes that decompose a multiclass

328

13 Kernel Machines

problem to a set of two-class problems (Dietterich and Bakiri 1995) (see
also section 17.6). SVMs being two-class classiﬁers are ideally suited to
this (Allwein, Schapire, and Singer 2000), and it is also possible to have
an incremental approach where new two-class SVMs are added to better
separate pairs of classes that are confused, to ameliorate a poor ECOC
matrix (Mayoraz and Alpaydın 1999).
Another possibility is to write a single multiclass optimization problem
involving all classes (Weston and Watkins 1998):
K

(13.42)

min

1
wi
2 i=1

2

ξit

+C
i

t

subject to
w z t x t + wz t 0 ≥ w i x t + wi0 + 2 − ξit , ∀i = z t and ξit ≥ 0
where z t contains the class index of x t . The regularization terms minimizes the norms of all hyperplanes simultaneously, and the constraints
are there to make sure that the margin between the actual class and any
other class is at least 2. The output for the correct class should be at
least +1, the output of any other class should be at least −1, and the
slack variables are deﬁned to make up any diﬀerence.
Though this looks neat, the one-vs.-all approach is generally preferred
because it solves K separate N variable problems whereas the multiclass
formulation uses K · N variables.

13.10

Kernel Machines for Regression
Now let us see how support vector machines can be generalized for regression. We see that the same approach of deﬁning acceptable margins,
slacks, and a regularizing function that combines smoothness and error
is also applicable here. We start with a linear model, and later on we see
how we can use kernel functions here as well:
f (x) = w T x + w0
In regression proper, we use the square of the diﬀerence as error:
e2 (r t , f (x t )) = [r t − f (x t )]2

329

13.10 Kernel Machines for Regression

70

60

50

40

30

20

10

0
−8

−6

−4

−2

0

2

4

6

8

Figure 13.6 Quadratic and -sensitive error functions. We see that -sensitive
error function is not aﬀected by small errors and also is aﬀected less by large
errors and thus is more robust to outliers.

whereas in support vector regression, we use the -sensitive loss function:
(13.43)

robust regression

(13.44)

e (r t , f (x t )) =

0
|r t − f (x t )| −

if |r t − f (x t )| <
otherwise

which means that we tolerate errors up to and also that errors beyond
have a linear eﬀect and not a quadratic one. This error function is therefore more tolerant to noise and is thus more robust (see ﬁgure 13.6). As
in the hinge loss, there is a region of no error, which causes sparseness.
Analogous to the soft margin hyperplane, we introduce slack variables
to account for deviations out of the -zone and we get (Vapnik 1995)
min

1
w
2

2

t
t
(ξ+ + ξ− )

+C
t

subject to
r t − (w T x + w0 )

≤

t
+ ξ+

(w T x + w0 ) − r t

≤

t
+ ξ−

t
t
ξ+ , ξ−

≥

0

where we use two types of slack variables, for positive and negative deviations, to keep them positive. Actually, we can see this as two hinges

330

13 Kernel Machines

added back to back, one for positive and one for negative slacks. This
formulation corresponds to the -sensitive loss function given in equation 13.43. The Lagrangian is
Lp

1
w
2

=

2

t
t
(ξ+ + ξ− )

+C
t

αt
+

t
+ ξ+ − r t + (w T x + w0 )

αt
−

−

t
+ ξ− + (w T x + w0 ) − r t

t

−
t

t t
t t
(μ+ ξ+ + μ− ξ− )

−

(13.45)

t

Taking the partial derivatives, we get
(13.46)

∂Lp
∂w

=

(13.47)

∂Lp
∂w0

=

(13.48)
(13.49)

(αt − αt )x t = 0 ⇒ w =
+
−

w−
t

(αt − αt )x t
+
−
t

(αt − αt )x t = 0
+
−
t

∂Lp
t
∂ξ+
∂Lp
t
∂ξ−

=

t
C − αt − μ+ = 0
+

=

t
C − αt − μ− = 0
−

The dual is
Ld
(13.50)

=

−

1
2

t

s

(αt − αt )(αs − αs )(x t )T x s
+
−
+
−

(αt + αt ) −
+
−

−
t

r t (αt − αt )
+
−
t

subject to
0 ≤ αt ≤ C , 0 ≤ αt ≤ C ,
+
−

(αt − αt ) = 0
+
−
t

Once we solve this, we see that all instances that fall in the tube have
αt = αt = 0; these are the instances that are ﬁtted with enough precision
+
−
(see ﬁgure 13.7). The support vectors satisfy either αt > 0 or αt > 0 and
+
−
are of two types. They may be instances that are on the boundary of the
tube (either αt or αt is between 0 and C), and we use these to calculate
+
−
w0 . For example, assuming that αt > 0, we have r t = x T x t + w0 + .
+
Instances that fall outside the -tube are of the second type; these are

331

13.10 Kernel Machines for Regression

2.4

(b)
(c)

2.2
(a)

2
1.8
1.6
1.4
1.2
0

2

4

6

8

Figure 13.7 The ﬁtted regression line to data points shown as crosses and the tube are shown (C = 10, = 0.25). There are three cases: In (a), the instance is in
the tube; in (b), the instance is on the boundary of the tube (circled instances); in
t
(c), it is outside the tube with a positive slack, that is, ξ+ > 0 (squared instances).
(b) and (c) are support vectors. In terms of the dual variable, in (a), αt = 0, αt =
+
−
0, in (b), αt < C, and in (c), αt = C.
+
+

instances for which we do not have a good ﬁt (αt = C), as shown in
+
ﬁgure 13.7.
Using equation 13.46, we can write the ﬁtted line as a weighted sum of
the support vectors:
(13.51)

f (x) = w T x + w0 =

(αt − αt )(x t )T x + w0
+
−
t

Again, the dot product (x t )T x s in equation 13.50 can be replaced with
a kernel K(x t , x s ), and similarly (x t )T x be replaced with K(x t , x) and we
can have a nonlinear ﬁt. Using a polynomial kernel would be similar to
ﬁtting a polynomial (ﬁgure 13.8), and using a Gaussian kernel (ﬁgure 13.9)
would be similar to nonparametric smoothing models (section 8.6) except
that because of the sparsity of solution, we would not need the whole
training set but only a subset.
There is also an equivalent ν-SVM formulation for regression (Schölkopf
et al. 2000), where instead of ﬁxing , we ﬁx ν to bound the fraction of
support vectors. There is still a need for C though.

332

13 Kernel Machines

3.5

3

2.5

2

1.5

1
0

2

4

6

8

Figure 13.8 The ﬁtted regression line and the -tube using a quadratic kernel
are shown (C = 10, = 0.25). Circled instances are the support vectors on the
margins, squared instances are support vectors which are outliers.

(a) s2 = 5
3
2
1
0
0

2

4

6

8

6

8

(b) s2 = 0.1
2.5
2
1.5
1
0

2

4

Figure 13.9 The ﬁtted regression line and the -tube using a Gaussian kernel
with two diﬀerent spreads are shown (C = 10, = 0.25). Circled instances are
the support vectors on the margins, and squared instances are support vectors
that are outliers.

333

13.11 One-Class Kernel Machines

13.11

outlier detection
one-class
classification

(13.52)

One-Class Kernel Machines
Support vector machines, originally proposed for classiﬁcation, are extended to regression by deﬁning slack variables for deviations around the
regression line, instead of the discriminant. We now see how SVM can be
used for a restricted type of unsupervised learning, namely, for estimating regions of high density. We are not doing a full density estimation;
rather, we want to ﬁnd a boundary (so that it reads like a classiﬁcation
problem) that separates volumes of high density from volumes of low
density (Tax and Duin 1999). Such a boundary can then be used for novelty or outlier detection. This is also called one-class classiﬁcation.
We consider a sphere with center a and radius R that we want to enclose as much as possible of the density, measured empirically as the
enclosed training set percentage. At the same time, trading oﬀ with it,
we want to ﬁnd the smallest radius (see ﬁgure 13.10). We deﬁne slack
variables for instances that lie outside (we only have one type of slack
variable because we have examples from one class and we do not have
any penalty for those inside), and we have a smoothness measure that is
proportional to the radius:
min R 2 + C

ξt
t

subject to
xt − a

2

≤ R 2 + ξ t and ξ t ≥ 0, ∀t

Adding the constraints, we get the Lagrangian, which we write keeping
in mind that xt − a 2 = (x t − a)T (x t − a):
(13.53)

Lp = R 2 + C

ξt −
t

αt R 2 + ξ t − (x t )T x t − 2a T x t + a T a
t

γt ξt

−
t

with αt ≥ 0 and γt ≥ 0 being the Lagrange multipliers. Taking the derivative with respect to the parameters, we get
(13.54)

∂L
∂R

=

(13.55)

∂L
∂a

=

∂L
∂ξ t

=

(13.56)

αt = 0 ⇒

2R − 2R
t

αt = 1
t

αt (2x t − 2a) = 0 ⇒ a =
t

C − αt − γ t = 0

αt x t
t

334

13 Kernel Machines

2

1.5
(b)
(c)

1

(a)

0.5

0
0

0.5

1

1.5

2

Figure 13.10 One-class support vector machine places the smoothest boundary
(here using a linear kernel, the circle with the smallest radius) that encloses as
much of the instances as possible. There are three possible cases: In (a), the
instance is a typical instance. In (b), the instance falls on the boundary with
ξ t = 0; such instances deﬁne R. In (c), the instance is an outlier with ξ t > 0. (b)
and (c) are support vectors. In terms of the dual variable, we have, in (a), αt = 0;
in (b), 0 < αt < C; in (c), αt = C.

Since γ t ≥ 0, we can write this last as the constraint: 0 ≤ αt ≤ C.
Plugging these into equation 13.53, we get the dual that we maximize
with respect to αt :
(13.57)

αt (x t )T x t −

Ld =
t

αt αs (x t )T x s
t

s

subject to
0 ≤ αt ≤ C and

αt = 1
t

When we solve this, we again see that most of the instances vanish
with their αt = 0; these are the typical, highly likely instances that fall
inside the sphere (ﬁgure 13.10). There are two type of support vectors
with αt > 0: There are instances that satisfy 0 < αt < C and lie on the
boundary, xt − a 2 = R 2 (ξ t = 0), which we use to calculate R. Instances

13.12 Kernel Dimensionality Reduction

335

that satisfy αt = C (ξ t > 0) lie outside the boundary and are the outliers.
From equation 13.55, we see that the center a is written as a weighted
sum of the support vectors.
Then given a test input x, we say that it is an outlier if
x−a

2

> R2

or
x t x − 2a T x + a T a > R 2
Using kernel functions, allow us to go beyond a sphere and deﬁne
boundaries of arbitrary shapes. Replacing the dot product with a kernel
function, we get (subject to the same constraints):
(13.58)

αt K(x t , x t ) −

Ld =
t

αt αs K(x t , x s )
t

s

For example, using a polynomial kernel of degree 2 allows arbitrary
quadratic surfaces to be used. If we use a Gaussian kernel (equation 13.30),
we have a union of local spheres. We reject x as an outlier if
αt K(x, x t ) +

K(x, x) − 2
t

αt αs K(x t , x s ) > R 2
t

s

The third term does not depend on x and is therefore a constant (we
use this as an equality to solve for R where x is an instance on the margin). In the case of a Gaussian kernel where K(x, x) = 1, the condition
reduces to
αt KG (x, x t ) < Rc
t

for some constant Rc , which is analogous to the kernel density estimator
(section 8.2.2)—except for the sparseness of the solution—with a probability threshold Rc (see ﬁgure 13.11).
There is also an alternative, equivalent ν-SVM type of formulation of
one-class support vector machines that uses the canonical (1/2) w 2
type of smoothness (Schölkopf et al. 2001).

13.12

Kernel Dimensionality Reduction
We know from section 6.3 that principal components analysis (PCA) reduces dimensionality by projecting on the eigenvectors of the covariance

336

13 Kernel Machines

2

2

(a) s = 1

(a) s = 0.1

2

2

1.5

1.5

1

1

0.5

0.5

0
0

1

2

0
0

1

2

Figure 13.11 One-class support vector machine using a Gaussian kernel with
diﬀerent spreads.

Kernel PCA

matrix Σ with the largest eigenvalues, which, if data instances are centered (E[x] = 0), can be written as XT X. In the kernelized version, we
work in the space of φ(x) instead of the original x and because, as usual,
the dimensionality d of this new space may be much larger than the data
set size N, we prefer to work with the N × N matrix XXT instead of the
d × d matrix XT X. The projected data matrix is Φ = φ(X), and hence we
work on the eigenvectors of ΦT Φ and hence of the kernel matrix K.
Kernel PCA uses the eigenvectors and eigenvalues of the kernel matrix and this corresponds to doing a linear dimensionality reduction in
the φ(x) space. When c i and λi are the corresponding eigenvectors and
eigenvalues, the projected new k-dimensional values can be calculated as
z t = λi c t , j = 1, . . . , k, t = 1, . . . , N
j
j
An example is given in ﬁgure 13.12 where we ﬁrst use a quadratic kernel and then decrease dimensionality to two (out of ﬁve) using kernel PCA
and implement a linear SVM there. Note that in the general case (e.g., with
a Gaussian kernel), the eigenvalues do not necessarily decay and there is
no guarantee that we can reduce dimensionality using kernel PCA.
What we are doing here is multidimensonal scaling (section 6.5) using
kernel values as the similarity values. For example, by taking k = 2,
one can visualize the data in the space induced by the kernel matrix,
which can give us information as to how similarity is deﬁned by the used
kernel. Linear discriminality reduction (LDA) (section 6.6) can similarly

337

13.13 Notes

(a) Quadratic kernel in the x space
−1

1

1
0

−1
−1

−0.5

0

0.5

1

(b) Linear kernel in the z space
2
−1
0

1

−2
0.9

0.95

1

1.05

1.1

1.15

Figure 13.12 Instead of using a quadratic kernel in the original space (a), we
can use kernel PCA on the quadratic kernel values to map to a two-dimensional
new space where we use a linear discriminant (b); these two dimensions (out of
ﬁve) explain 80 percent of the variance.

be kernelized (Müller et al. 2001).
In chapter 6, we discussed nonlinear dimensionality reduction methods, Isomap and LLE. In fact, by viewing the elements of the cost matrix
in equation 6.47 as kernel evaluations for pairs of inputs, LLE can be seen
as kernel PCA for a particular choice of kernel. The same also holds for
Isomap when a kernel function is deﬁned as a function of the geodesic
distance on the graph.

13.13

dual
representation

Notes
The idea of generalizing linear models by mapping the data to a new
space through nonlinear basis functions is old, but the novelty of support vector machines is that of integrating this into a learning algorithm
whose parameters are deﬁned in terms of a subset of data instances (the
so-called dual representation), hence also without needing to explicitly

338

13 Kernel Machines

evaluate the basis functions and thereby also limiting complexity by the
size of the training set; this is also true for Gaussian processes where the
kernel function is called the covariance function (section 14.4).
The sparsity of the solution shows the advantage over nonparametric
estimators, such as k-nearest neighbor and Parzen windows, or Gaussian
processes, and the ﬂexibility to use kernel functions allows working with
nonvectorial data. Because there is a unique solution to the optimization
problem, we do not need any iterative optimization procedure as we do in
neural networks. Because of all these reasons, support vector machines
are now considered to be the best, oﬀ-the-shelf learners and are widely
used in many domains, especially bioinformatics (Schölkopf, Tsuda, and
Vert 2004) and natural language processing applications, where an increasing number of tricks are being developed to derive kernels (ShaweTaylor and Cristianini 2004).
The use of kernel functions implies a diﬀerent data representation; we
no longer deﬁne an instance (object/event) as a vector of attributes by
itself, but in terms of how it is similar to or diﬀers from other instances;
this is akin to the diﬀerence between multidimensional scaling that uses
a matrix of distances (without any need to know how they are calculated)
and principal components analysis that uses vectors in some space.
More information on support vector machines can be found in books
by Vapnik (1995, 1998) and Schölkopf and Smola (2002). The chapter
on SVM in Cherkassky and Mulier 1998 is a very readable introduction.
Burges 1998 and Smola and Schölkopf 1998 are good tutorials on SVM
classiﬁcation and regression, respectively. There is a dedicated Web site
http://www.kernel-machines.org and many free software packages are
available, such as SVMlight (Joachims 2004) and LIBSVM (Chang and Lin
2008).

13.14

Exercises
1. Propose a ﬁltering algorithm to ﬁnd training instances that are very unlikely
to be support vectors.
2. In equation 13.31, how can we estimate S?
3. In the empirical kernel map, how can we choose the templates?
4. In the localized multiple kernel of equation 13.40, propose a suitable model
for ηi (x|θi ) and discuss how it can be trained.
5. In kernel regression, what is the relation, if any, between

and noise variance?

339

13.15 References

6. In kernel regression, what is the eﬀect of using diﬀerent
ance?

on bias and vari-

7. How can we use one-class SVM for classiﬁcation?
8. In a setting as in ﬁgure 13.12, use kernel PCA with a Gaussian kernel.
9. Let us say we have two representations for the same object and associated
with each, we have a diﬀerent kernel. How can we use both to implement a
joint dimensionality reduction using kernel PCA?

13.15

References
Allwein, E. L., R. E. Schapire, and Y. Singer. 2000. “Reducing Multiclass to Binary:
A Unifying Approach for Margin Classiﬁers.” Journal of Machine Learning
Research 1: 113–141.
Burges, C. J. C. 1998. “A Tutorial on Support Vector Machines for Pattern Recognition.” Data Mining and Knowledge Discovery 2: 121–167.
Chang, C.-C., and C.-J. Lin. 2008. LIBSVM: A Library for Support Vector Machines.
http://www.csie.ntu.edu.tw/∼cjlin/libsvm/.
Cherkassky, V., and F. Mulier. 1998. Learning from Data: Concepts, Theory,
and Methods. New York: Wiley.
Cortes, C., and V. Vapnik. 1995. “Support Vector Networks.” Machine Learning
20: 273–297.
Dietterich, T. G., and G. Bakiri. 1995. “Solving Multiclass Learning Problems via
Error-Correcting Output Codes.” Journal of Artiﬁcial Intelligence Research 2:
263–286.
Gönen, M., and E. Alpaydın. 2008. “Localized Multiple Kernel Learning.” In
25th International Conference on Machine Learning, ed. A. McCallum and S.
Roweis, 352–359. Madison, WI: Omnipress.
Jaakkola, T., and D. Haussler. 1999. “Exploiting Generative Models in Discriminative Classiﬁers.” In Advances in Neural Information Processing Systems 11,
ed. M. J. Kearns, S. A. Solla, and D. A. Cohn, 487-493. Cambridge, MA: MIT
Press.
Joachims, T. 2004. SVMlight, http://svmlight.joachims.org.
Lanckriet, G. R. G, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. 2004.
“Learning the Kernel Matrix with Semideﬁnite Programming.” Journal of Machine Learning Research 5: 27–72.
Mayoraz, E., and E. Alpaydın. 1999. “Support Vector Machines for Multiclass
Classiﬁcation.” In Foundations and Tools for Neural Modeling, Proceedings of
IWANN’99, LNCS 1606, ed. J. Mira and J. V. Sanchez-Andres, 833–842. Berlin:
Springer.

340

13 Kernel Machines

Müller, K. R., S. Mika, G. Rätsch, K. Tsuda, and B. Schölkopf. 2001. “An Introduction to Kernel-Based Learning Algorithms.” IEEE Transactions on Neural
Networks 12: 181–201.
Noble, W. S. 2004. “Support Vector Machine Applications in Computational Biology.” In Kernel Methods in Computational Biology, ed. B. Schölkopf, K. Tsuda,
and J.-P. Vert, 71–92. Cambridge, MA: MIT Press.
Platt, J. 1999. “Probabilities for Support Vector Machines.” In Advances in Large
Margin Classiﬁers, ed. A. J. Smola, P. Bartlett, B. Schölkopf, and D. Schuurmans, 61–74. Cambridge, MA: MIT Press.
Schölkopf, B., J. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. 2001.
“Estimating the Support of a High-Dimensional Distribution.” Neural Computation 13: 1443–1471.
Schölkopf, B., and A. J. Smola. 2002. Learning with Kernels: Support Vector
Machines, Regularization, Optimization, and Beyond. Cambridge, MA: MIT
Press.
Schölkopf, B., A. J. Smola, R. C. Williamson, and P. L. Bartlett. 2000. “New
Support Vector Algorithms.” Neural Computation 12: 1207–1245.
Schölkopf, B., K. Tsuda, and J.-P. Vert, eds. 2004. Kernel Methods in Computational Biology. Cambridge, MA: MIT Press.
Shawe-Taylor, J., and N. Cristianini. 2004. Kernel Methods for Pattern Analysis.
Cambridge, UK: Cambridge University Press.
Smola, A., and B. Schölkopf. 1998. A Tutorial on Support Vector Regression,
NeuroCOLT TR-1998-030, Royal Holloway College, University of London, UK.
Sonnenburg, S., G. Rätsch, C. Schäfer, and B. Schölkopf. 2006. “Large Scale
Multiple Kernel Learning.” Journal of Machine Learning Research 7: 1531–
1565.
Tax, D. M. J., and R. P. W. Duin. 1999. “Support Vector Domain Description.”
Pattern Recognition Letters 20: 1191–1199.
Vapnik, V. 1995. The Nature of Statistical Learning Theory. New York: Springer.
Vapnik, V. 1998. Statistical Learning Theory. New York: Wiley.
Vert, J.-P., K. Tsuda, and B. Schölkopf. 2004. “A Primer on Kernel Methods.”
In Kernel Methods in Computational Biology, ed. B. Schölkopf, K. Tsuda, and
J.-P. Vert, 35–70. Cambridge, MA: MIT Press.
Weston, J., and C. Watkins. 1998. “Multiclass Support Vector Machines.” Technical Report CSD-TR-98-04, Department of Computer Science, Royal Holloway,
University of London.

14

Bayesian Estimation

In the Bayesian approach, we consider parameters as random variables having a prior distribution. We continue from where we left oﬀ
in section 4.4 and discuss three cases: estimating the parameters of
a distribution, estimating the parameters of a model, and Gaussian
processes.

14.1

prior probability

posterior
probability

(14.1)

Introduction
Bayes ian es tim ation is used when we have some prior information
regarding a parameter. For example, before looking at a sample to estimate the mean μ of a distribution, we may have some prior belief that
it is close to 2, between 1 and 3. Such prior beliefs are especially important when we have a small sample. In such a case, we are interested in
combining what the data tells us, namely, the value calculated from the
sample, and our prior information.
The maximum likelihood approach we discuss in section 4.2 treats a
parameter as an unknown constant. In Bayesian estimation, as we started
discussing in section 4.4, a parameter is treated as a random variable,
which allows us to code any prior information we have using a prior
probability distribution. For example, knowing that μ is very likely to
be between 1 and 3, we write p(μ) in such a way that the bulk of the
density lies in the interval [1, 3].
Using Bayes’ rule, we combine the prior and the likelihood and calculate
the posterior probability distribution:
p(θ|X) =

p(θ)p(X|θ)
p(X)

342

14 Bayesian Estimation

Figure 14.1 The generative graphical model. The arcs are in the direction of
sampling; ﬁrst we pick θ from p(θ) and then we generate data by sampling from
p(xt |θ). The new instance x and the past sample X are independent given θ:
this is the iid assumption. If we do not know θ, they are dependent: we infer θ
from given X (shown shaded) using Bayes’ rule, which inverts the direction to
calculate p(θ|X), which can then be used to ﬁll in x.

generative model

p(θ) is the prior density; it is what we know regarding the possible
values that θ may take before looking at the sample. p(X|θ) is the sample
likelihood; it tells us how likely our sample X is if the parameter of the
distribution takes the value θ. For example, if the instances in our sample
are between 5 and 10, such a sample is likely if μ is 7 but is less likely
if μ is 3 and even less likely if μ is 1. p(X) in the denominator is a
normalizer to make sure that the posterior p(θ|X) integrates to 1. It
is called the posterior probability because it tells us how likely θ takes
a certain value after looking at the sample. The Bayes’ rule takes the
prior distribution, combines it with what the data reveals, and generates
the posterior distribution. We then use this posterior distribution in our
later inferences.
For example, let us say that we have a past sample X drawn from some
distribution with unknown parameter θ. We can then draw one more
instance x, and we would like to calculate its probability distribution.
We can visualize this as a graphical model (chapter 16) as shown in ﬁgure 14.1. What is depicted is a generative model which represents how
the data is generated: We ﬁrst pick θ from p(θ) and use it to sample X
and also the new instance x. We write the joint as
p(x, X, θ) = p(θ)p(X|θ)p(x|θ)

343

14.2 Estimating the Parameter of a Distribution

which we use in estimating the probability of a new instance x given the
past sample X:
p(x|X)
(14.2)

maximum a
posteriori (MAP)
estimate

=

p(x, X)
=
p(X)

=

θ

p(x, X, θ)dθ
=
p(X)

θ

p(θ)p(X|θ)p(x|θ)dθ
p(X)

p(θ|X)p(x|θ)dθ
θ

In calculating p(θ|X), Bayes’ rule allows us to invert the direction of
the arc and do a diagnostic inference. The inferred θ distribution is then
used to derive a prediction distribution for the new x.
We see that our estimate is a weighted sum (we replace dθ by θ if
θ is discrete valued) of estimates for all possible values of θ weighted by
how likely θ is, given the sample X.
This is the full Bayesian treatment that may not be possible if the posterior is not easy to integrate. As we saw in section 4.4, in the case of the
maximum a posteriori (MAP) estimate, we use the mode of the posterior:
θMAP = arg max p(θ|X) and pMAP (x|X) = p(x|θMAP )
θ

The MAP estimate corresponds to assuming that the posterior makes
a very narrow peak around a single point, that is, the mode. If the prior
p(θ) is uniform over all θ, then the mode of the posterior p(θ|X) and
the mode of the likelihood p(X|θ) are at the same point, and the MAP
estimate is equal to the maximum likelihood (ML) estimate. This implies
that using ML corresponds to assuming no a priori distinction between
diﬀerent values of θ.
Let us now see how Bayesian estimation is used in diﬀerent types of
distributions and applications.

14.2
14.2.1

Estimating the Parameter of a Distribution
Discrete Variables
Let us say that each instance is a multinomial variable taking one of K
distinct states (section 4.2.2). We say xt = 1 if instance t is in state i
i
and xt = 0, ∀j = i. The parameters are the probabilities of states, q =
j
[q1 , q2 , . . . , qk ]T with qi , i = 1, . . . , K satisfying qi ≥ 0, ∀i and i qi = 1.
The sample likelihood is
N

K

p(X|q) =

xt

qi i
t=1 i=1

344

14 Bayesian Estimation

Dirichlet
distribution

The prior distribution we use is the Dirichlet distribution
K

Dirichlet(q|α) =

Gamma function

Γ (α0 )
α −1
q i
Γ (α1 ) · · · Γ (αK ) i=1 i

where α = [α1 , . . . , αK ]T and α0 = i αi . αi being the parameters of
the prior are called the hyperparameters. Γ (x) is the Gamma function
deﬁned as
Γ (x) ≡

∞

ux−1 e−u du

0

For example, xt may correspond to news documents and states may
correspond to K diﬀerent news categories: sports, politics, arts, and so
on. The probabilities qi then correspond to the proportions of diﬀerent
news categories, and priors on them allow us to code our prior beliefs in
these proportions; for example, we may expect to have more news related
to sports than news related to arts.
Given the prior and the likelihood, we can derive the posterior
p(q|X)

∝

p(X|q)p(q|α)
αi +Ni −1

∝

(14.3)

qi
i
N

conjugate prior

where Ni = t=1 xt . We see that the posterior has the same form as the
i
prior and we call such a prior a conjugate prior. Both the prior and the
likelihood have the form of product of powers of qi , and we combine
them to make up the posterior:

(14.4)

=

Γ (α0 + N)
Γ (α1 + N1 ) · · · Γ (αK + NK )

=

p(q|X)

K

Dirichlet(q|α + n)

αi +Ni −1

qi
i=1

where n = [N1 , . . . , NK ]T and i Ni = N.
Looking at equation 14.3, we can bring an interpretation to the hyperparameters αi (Bishop 2006). Just as ni are counts of occurrences of state
i in a sample of N, we can view αi as counts of occurences of state i in
some imaginary sample of α0 instances. In deﬁning the prior, we are
subjectively saying the following: in a sample of α0 , I would expect αi
of them to belong to state i. Note that larger α0 implies that we have a
higher conﬁdence (a more peaked distribution) in our subjective proportions: saying that I expect to have 60 out of 100 occurrences belong to

14.2 Estimating the Parameter of a Distribution

345

state 1 has higher conﬁdence than saying that I expect to have 6 out of
10. The posterior then is another Dirichlet that sums up the counts of
the occurences of states, imagined and actual, given by the prior and the
likelihood, respectively.
The conjugacy has a nice implication. In a sequential setting where we
receive a sequence of instances, because the posterior and the prior have
the same form, the current posterior accumulates information from all
past instances and becomes the prior for the next instance.
When the variable is binary, xt ∈ {0, 1}, the multinomial sample becomes Bernoulli
t

q x (1 − q)1−x

p(X|q) =

t

t

beta distribution

and the Dirichlet prior reduces to the beta distribution:
beta(q|α, β) =

Γ (α + β) α−1
q
(1 − q)β−1
Γ (α)Γ (β)

For example, xt may be 0 or 1 depending on whether email with index t in a random sample of size N is legitimate or spam, respectively.
Then deﬁning a prior on q allows us to deﬁne a prior belief on the spam
probability: I would expect, on the average, α/(α + β) of my emails to be
spam.
Beta is a conjugate prior, and for the posterior we get
p(q|A, N, α, β) ∝ q A+α−1 (1 − p)N−A+β−1
where A = t xt , and we see again that we combine the occurrences in the
imaginary and the actual samples. Note that when α = β = 1, we have a
uniform prior and the posterior has the same shape as the likelihood. As
the two counts, whether α and β for the prior or α+A and β+N−A for the
posterior, increase and their diﬀerence increases, we get a distribution
that is more peaked with smaller variance (see ﬁgure 14.2). As we see
more data (imagined or actual), the variance decreases.

14.2.2

Continuous Variables
We now consider the case where instances are Gaussian distributed, p(x) ∼
N (μ, σ 2 ), and the parameters are μ and σ 2 ; we have already discussed
this brieﬂy section 4.4. The sample likelihood is

(14.5)

√

p(X|μ, σ 2 ) =
t

1
(xt − μ)2
exp −
2σ 2
2πσ

346

14 Bayesian Estimation

6
5
← beta(20,30)

4
3
2

← beta(4,6)
beta(1,1)

1

← beta(2,3)
0
0

0.2

0.4

0.6

0.8

1

Figure 14.2 Plots of beta distributions for diﬀerent sets of (α, β).

2
2
The conjugate prior for μ is Gaussian, p(μ) ∼ N (μ0 , σ0 ), and we write
the posterior as

∝

p(μ)p(X|μ)

∼

p(μ|X)

2
N (μN , σN )

where
(14.6)

μN

=

(14.7)

1
2
σN

=

2
Nσ0
σ2
μ0 +
m
2
2
Nσ0 + σ 2
Nσ0 + σ 2
1
N
2 +
σ2
σ0

where m = t xt /N is the sample average. We see that the mean of
the posterior density (which is the Bayesian estimate), μN , is a weighted
average of the prior mean μ0 and the sample mean m, with weights being
inversely proportional to their variances (see ﬁgure 14.3 for an example).
Note that because both coeﬃcients are between 0 and 1 and sum to 1, μN
is always between μ0 and m. When the sample size N or the variance of
2
the prior σ0 is large, the Bayes’ estimator is close to m, relying more on
2
the information provided by the sample. When σ0 is small—that is, when
we have little prior uncertainty regarding the correct value of μ, or when
we have a small sample—our prior guess μ0 has higher eﬀect.
σN gets smaller when either of σ0 or σ gets smaller or if N is larger.
√
Note also that σN is smaller than both σ0 and σ / N, that is, the posterior

347

14.2 Estimating the Parameter of a Distribution

1.4
1.2
1
← p(μ|X)

0.8
0.6
0.4

p(μ)→

0.2
0
0

← p(x|μ)
2

4

6

8

10

Figure 14.3 20 data points are drawn from p(x) ∼ N (6, 1.52), prior is p(μ) ∼
N (4, 0.82), and posterior is then p(μ|X) ∼ N (5.7, 0.32).

precision

variance is smaller than both prior variance and that of m. Incorporating
both results in a better posterior estimate than using any of the prior or
sample alone.
For the case of variance, we work with the precision, the reciprocal of
the variance, λ ≡ 1/σ 2 . Using this, the sample likelihood is written as
p(X|λ)

=

(14.8)
Gamma distribution

=

λ1/2
λ
√
exp − (xt − μ)2
2
2π
t
⎡
⎤
λ
λN/2 (2π )−N/2 exp ⎣−
(xt − μ)2 ⎦
2 t

The conjugate prior for the precision is the Gamma distribution:
p(λ) ∼ Gamma(a0 , b0 ) =

1
a
b 0 λa0 −1 exp(−b0 λ)
Γ (a0 ) 0

and the posterior is
∝

p(X|λ)p(λ)

∼

p(λ|X)

Gamma(aN , bN )

where
(14.9)

aN

=

bN

=

a0 + N/2
N
b0 + s 2
2

348

14 Bayesian Estimation

where s 2 = t (xt − μ)2 /N is the sample variance. Again, we see that
posterior estimates are weighted sum of priors and sample statistics.

14.3

Bayesian Estimation of the Parameters of a Function
We now discuss the case where we estimate the parameters, not of a distribution, but some function of the input, for regression or classiﬁcation.
Again, our approach is to consider these parameters as random variables
with a prior distribution and use Bayes’ rule to calculate a posterior distribution. We can then either evaluate the full integral, approximate it, or
use the MAP estimate.

14.3.1

Regression
Let us take the case of a linear regression model:

(14.10)

r = wT x +

where

∼ N (0, β−1 )

where β is the precision of the additive noise.
The parameters are the weights w and we have a sample X = {x t , r t }N
t=1
where x ∈ d and r t ∈ , which we can break down into a matrix of inputs and a vector of desired outputs as X = [X, r]. From equation 14.10,
we have
p(r t |x t , w, β) ∼ N (w T x, β−1 )
We saw previously in section 4.6 that the log likelihood is
L(X|w) ≡ log p(X|w)

=

log p(r, X|w)

=

log p(r|X, w) + log p(X)

where the second term is a constant, independent of the parameters. We
expand the ﬁrst term as
L(r|X, w, β)

=

p(r t |x t , w, β)

log
t

(14.11)

=

√
β
−N log( 2π ) + N log β −
2

(r t − w T x t )2
t

For the case of the ML estimate, we ﬁnd w that maximizes this, or
equivalently, minimizes the last term that is the sum of the squared error,
which can be rewritten as
E = (r − Xw)T (r − Xw)

14.3 Bayesian Estimation of the Parameters of a Function

349

Taking the derivative with respect to w and setting it to 0, we get the
maximum likelihood estimator (we have previously derived this in section 5.8):
(14.12)

w ML = (XT X)−1 XT r
Having calculated the parameters, we can now do prediction. Given
new input x , the response is calculated as

(14.13)

r = wT x
ML
For nonlinear models, g(x|w), for example, a multilayer perceptron
where w are all the weights, we minimize, for example, using gradient
descent,
E(X|w) = r t − g(xt |w)

2

and w LSQ that minimize it is called the least squares estimator. Then, the
prediction is calculated as
r = g(x |w LSQ )

Gaussian prior

In the case of the Bayesian approach, for the parameters, we deﬁne a
Gaussian prior:
p(w) ∼ N (0, α−1 I)
which is a conjugate prior and for the posterior, we get
p(w|X) ∼ N (μN , ΣN )
where

(14.14)

μN

=

βΣN XT r

(14.15)

ΣN

=

(αI + βXT X)−1

To calculate the overall output, we integrate over the full posterior
r =

w T x p(w|X)dw

If we want to use a point estimate, the MAP (or Bayes’, because the
posterior is Gaussian) estimator is
(14.16)

w MAP = μN = β(αI + βXT X)−1 XT r

350

14 Bayesian Estimation

and we replace the density with a single point, namely, the mean,
r = wT x
MAP
with variance
(14.17)

Var(r ) = β−1 + (x )T ΣN x
Comparing equation 14.16 with the ML estimate of equation 14.12, this
can be seen as regularization—that is, we add a constant α to the diagonal
to better condition the matrix to be inverted.
The prior, p(w) ∼ N (0, α−1 I), says that we expect the parameters to
be close to 0 with spread inversely proportional to α. When α → 0, we
have a ﬂat prior and the MAP estimate converges to the ML estimate.
We see in ﬁgure 14.4 that if we increase α, we force parameters to be
closer to 0 and the posterior distribution moves closer to the origin and
shrinks. If we decrease β, we assume noise with higher variance and the
posterior also has higher variance.
If we take the log of the posterior, we have
log p(w|X, r)

∝

log p(X, r|w) + log p(w)

∝

log p(r|X, w) + log p(w)
β
α
−
(r t − w T x t )2 − w T w + c
2 t
2

=

which we maximize to ﬁnd the MAP estimate. In the general case, given
our model g(x|w), we can write an augmented error function
t

ridge regression

Laplacian prior

wi2

[r t − g(x t |w)]2 + λ

Er idge (w|X) =

i

with λ ≡ α/β. This is known as parameter shrinkage or ridge regression in statistics. In section 4.8, we called this regularization and in
section 11.9, we called this weight decay in neural networks. The ﬁrst
term is the negative log of the likelihood, and the second term penalizes
wi away from 0 (as dictated by α of the prior).
Though this approach reduces i wi2 , it does not force individual wi to
0; that is, it cannot be used for feature selection, namely, to determine
which xi are redundant. For this, one can use a Laplacian prior that uses
the L1 norm instead of the L2 norm (Figueiredo 2003):
⎛
⎞
α
α d
exp ⎝−α |wi |⎠
exp(−α|wi |) =
p(w|α) =
2
2
i
i

351

14.3 Bayesian Estimation of the Parameters of a Function

α=1, β=2

prior

3

posterior

2

2

w

w

2
0

0

1
0

0

−2
−2

5
α=10, β=2

0
w0

−2
−2

2

prior

3

0
w0

2

posterior

2

2

w

w

2
0

0

1
0

0

−2
−2

5
α=1, β=1

0
w0

−2
−2

2

prior

3

0
w0

2

posterior

2

2

w

w

2
0

0

1
0

0

5

−2
−2

0
w

0

2

−2
−2

0
w

2

0

Figure 14.4 Bayesian linear regression for diﬀerent values of α and β. To the
left: crosses are the data points and straight line is the ML solution. The MAP
solution with one standard deviation error bars are also shown dashed. Center:
prior density centered at 0 and variance 1/α. To the right: posterior density
whose mean is the MAP solution. We see that when α is increased, the variance
of the prior shrinks and the line moves closer to the ﬂat 0 line. When β is
decreased, more noise is assumed and the posterior density has higher variance.

352

14 Bayesian Estimation

The posterior probability is no longer Gaussian and the MAP estimate
is found by minimizing
(r t − w T x t )2 + 2σ 2 α

Elasso (w|X) =
t

lasso

14.3.2

|wi |
i

where σ 2 is the variance of noise (for which we plug in our estimate).
This is known as lasso (least absolute shrinkage and selection operator) (Tibshirani 1996). To see why L1 induces sparseness, let us consider the case with two weights [w1 , w2 ]T (Figueiredo 2003): [1, 0T 2 =
√
√
√
√
√
[1/ 2, 1/ 2]T 2 = 1, whereas [1, 0]T 1 = 1 < [1/ 2, 1/ 2]T 1 = 2,
and therefore L1 prefers to set w2 to 0 and use a large w1 , rather than
having small values for both.

The Use of Basis/Kernel Functions
Using the Bayes’ estimate of equation 14.14, the prediction is written as
=

(x )T w

=

r

β(x )T ΣN XT r
β(x )T ΣN x t r t

=
t

dual
representation

(14.18)

This is the dual representation. When we can write the parameter in
terms of the training data, or a subset of it as in support vector machines
(chapter 13), we can write the prediction as a function of the current
input and past data. We can rewrite this as
K(x , x t )r t

r =
t

where we deﬁne
(14.19)

basis function

K(x , x t ) = β(x )T ΣN x t
We know that we can generalize the linear kernel of equation 14.19 by
using a nonlinear basis function φ(x) to map to a new space where we
ﬁt the linear model. In such a case, instead of the d-dimensional x we
have the k-dimensional φ(x) where k is the number of basis functions
and instead of N × d data matrix X, we have N × k image of the basis
functions Φ.
During test, we have
r

=

φ

φ

φ(x )T w where w = βΣN ΦT r and ΣN = αI + βΦ T Φ

−1

14.3 Bayesian Estimation of the Parameters of a Function

=

353

φ

βφ(x )T ΣN ΦT r
φ

βφ(x )T ΣN φ(x t )r t

=
t

(14.20)

K(x , x t )r t

=
t

where we deﬁne
(14.21)

kernel function

φ

K(x , x t ) = βφ(x )T ΣN φ(x t )
as the equivalent kernel. This is the dual representation in the space of
φ(x). We see that we can write our estimate as a weighted sum of the
eﬀects of instances in the training set where the eﬀect is given by the kernel function K(x , x t ); this is similar to nonparametric kernel smoothers
we discuss in chapter 8, or kernel machines of chapter 13.
Error bars can be deﬁned using
φ

Var(r ) = β−1 + φ(x )T ΣN φ(x )
An example is given in ﬁgure 14.5 for the linear, quadratic, and fourthdegree kernels.
Just as in regression proper where we can work on the original x or
φ(x), in Bayesian regression too we can work on the preprocessed φ(x),
deﬁning parameters in that space. Later on in this chapter, we are going
to see Gaussian processes where we can deﬁne and use K(x, x t ) directly
without needing to calculate φ(x).

14.3.3

Bayesian Classiﬁcation
In a two-class problem, we have a single output and assuming a linear
model, we have
P (C1 |x t ) = y t = sigmoid(w T x t )
The log likelihood of a Bernoulli sample is given as
r t log yt + (1 − r t ) log(1 − y t )

L(r|X) =
t

which we maximize, or minimize its negative log—the cross-entropy—to
ﬁnd the ML estimate, for example, using gradient descent. This is called
logistic discrimination (section 10.7).
In the case of the Bayesian approach, we assume a Gaussian prior
(14.22)

p(w) = N (m 0 , S0 )

354

14 Bayesian Estimation

(a) Linear (α = 1 β = 1)
3
2
1
0
0

1

2

3

4
5
(b) Quadratic

6

7

8

1

2

3
4
5
(c) Fourth−degree

6

7

8

1

2

3

6

7

8

3
2
1
0
0
3
2
1
0
0

4

5

Figure 14.5 Bayesian regression using kernels with one standard deviation error bars: (a) linear: φ(x) = [1, x]T , (b) quadratic: φ(x) = [1, x, x2 ]T , and (c)
fourth degree: φ(x) = [1, x, x2 , x3 , x4 ]T .

and the log of the posterior is given as
log p(w|r, X)

∝
=

(14.23)

log p(w) + log p(r|w, X)
1
− (w − m 0 )T S−1 (w − m 0 )
0
2
r t log yt + (1 − r t ) log(1 − y t ) + c

+
t

Laplace
approximation

This posterior distribution is no longer Gaussian and we cannot integrate exactly. We can use Laplace approximation, which works as follows
(MacKay 2003). Let us say we want to approximate some distribution
f (x), not necessarily normalized (to integrate to 1). In Laplace approximation, we ﬁnd the mode of f (x), x0 , ﬁt a Gaussian q(x) centered there,
and then if we want to integrate, we integrate this ﬁtted Gaussian instead.
To ﬁnd the variance of the Gaussian, we consider the Taylor expansion
of f (·) at x = x0
log f (x) = log f (x0 ) −

1
a(x − x0 )2 + · · ·
2

14.3 Bayesian Estimation of the Parameters of a Function

355

where
a≡−

d
log f (x)
dx2

x=x0

Note that the ﬁrst, linear term disappears because the ﬁrst derivative
is 0 at the mode. Taking exp, we have
a
f (x) = f (x0 ) exp − (x − x0 )2
2
To normalize f (x), we consider that in a Gaussian distribution
√

a
exp − (x − x0 )2 = a/2π
2

a
1
√ exp − (x − x0 )2 = 1 ⇒
2
2π(1/ a)

and therefore
a
q(x) = a/2π exp − (x − x0 )2 ∼ N (x0 , 1/a)
2
In the multivariate setting where x ∈
log f (x) = log f (x 0 ) −

d

, we have

1
(x − x 0 )T A(x − x 0 ) + · · ·
2

where A is the (Hessian) matrix of second derivatives:
A = − ∇∇ log f (x) x =x 0
The Laplace approximation is then
f (x) =

|A|1/2
1
exp − (x − x 0 )T A(x − x 0 ) ∼ Nd (x 0 , A−1 )
d/2
(2π )
2

Having now discussed how to approximate, we can now use it for the
posterior density. w MAP , which is the mode of p(w|r, X), is taken as the
mean and the covariance matrix is given by the inverse of the matrix of
the second derivatives of the negative log likelihood:
SN = −∇∇ log p(w|r, X) = S−1 +
0

y t (1 − y t )x t (x t )T
t

We then integrate over this Gaussian to estimate the class probability:
P (C1 |x) = y =

probit function

sigmoid(w T x)q(w)dw

where q(w) ∼ N (w MAP , S−1 ). A further complication is that we cannot
N
integrate analytically over a Gaussian convolved with a sigmoid. If we use
the probit function instead, which has the same S-shape as the sigmoid,
an analytical solution is possible (Bishop 2006).

356

14 Bayesian Estimation

14.4

Gaussian Processes
Let us say we have the linear model y = w T x. Then, for each w, we have
one line. Given a prior distribution p(w), we get a distribution of lines,
or to be more speciﬁc, for any w, we get a distribution of y values calculated at x as y(x|w) when w is sampled from p(w), and this is what we
mean when we talk about a Gaussian process. We know that if p(w) is
Gaussian, each y is a linear combination of Gaussians and is also Gaussian; in particular, we are interested in the joint distribution of y values
calculated at the N input data points, x t , t = 1, . . . , N (MacKay 1998).
We assume a zero mean Gaussian prior
p(x) ∼ N (0, α−1 I)
Given the N × d data points X and the d × 1 weight vector, we write the
y outputs as

(14.24)

y = Xw
which is N-variate Gaussian with
E[y]

=

XE[w] = 0

Cov(y)

(14.25)

=

E[yy T ] = XE[ww T ]XT =

1
XXT ≡ K
α

where K is the (Gram) matrix with elements
Ki,j ≡ K(x i , x j ) =
covariance
function

(x i )T x j
α

This is known as the covariance function in the literature of Gaussian
processes and the idea is the same as in kernel functions: If we use a
set of basis functions φ(x), we generalize from the dot product of the
original inputs to the dot product of basis functions by a kernel
Ki,j =

φ(x i )T φ(x j )
α

The actual observed output r is given by the line with added noise,
r = y + where ∼ N (0, β−1 ). For all N data points, we write it as
(14.26)

r ∼ NN (0, CN ) where CN = β−1 I + K
To make a prediction, we consider the new data as the (N + 1)st data
point pair (x , r ), and write the joint using all N + 1 data points. We have

(14.27)

r N+1 ∼ NN (0, CN+1 )

357

14.4 Gaussian Processes

(a) Linear (α = 1 β = 5)
3
2
1
0
0

1

2

3

4
5
(b) Quadratic

6

7

8

1

2

3

4
5
(c) Gaussian

6

7

8

1

2

3

6

7

8

3
2
1
0
0
3
2
1
0
0

4

5

Figure 14.6 Gaussian process regression with one standard deviation error
bars: (a) linear kernel, (b) quadratic kernel, (c) Gaussian kernel with spread
s 2 = 0.5.

where
CN+1 =

CN
kT

k
c

with k being the N × 1 dimensional vector of K(x , x t ), t = 1, . . . , N and
c = K(x , x ) + β−1 . Then to make a prediction, we calculate p(r |x , X, r),
which is Gaussian with
E[r |x ]

=

k T C−1 r
N

Var(r |x )

=

c − k T C−1 k
N

An example is shown in ﬁgure 14.6 using linear, quadratic, and Gaussian kernels. The ﬁrst two are deﬁned as the dot product of their corresponding basis functions; the Gaussian kernel is deﬁned directly as
KG (x i , x j ) = exp −

xi − x j
s2

2

358

14 Bayesian Estimation

The mean, which is our point estimate (if we do not integrate over the
full distribution), can also be written as a weighted sum of the kernel
eﬀects
(14.28)

at K(x t , x )

E[r |x ] =
t

where at is the tth component of C−1 r. Or, we can write it as a weighted
N
sum of the outputs of the training data points where weights are given
by the kernel function
(14.29)

rtwt

E[r |x ] =
t

where w t is the tth component of k T C−1 .
N
Note that we can also calculate the variance of a prediction at a point
to get an idea about uncertainty in there, and it depends on the instances
that aﬀect the prediction in there. In the case of a Gaussian kernel, only
instances within a locality are eﬀective and prediction variance is high
where there is little data in the vicinity (see ﬁgure 14.7).
Kernel functions can be deﬁned and used, depending on the application, as we have prevously discussed in the context of kernel machines
in chapter 13. The possibility of using kernel functions directly without
needing to calculate or store the basis functions oﬀers a great ﬂexibility. Normally, given a training set, we ﬁrst calculate the parameters, for
example using equation 14.12, and then use the parameters to make predictions using equation 14.13, never needing the training set any more.
This makes sense because generally the dimensionality of the parameters, which is generally O(d), is much lower than the size of the training
set N.
When we work with basis functions, however, calculating the parameter explicitly may no longer be the case, because the dimensionality of
the basis functions may be very high, even inﬁnite. In such a case, it is
cheaper to use the dual representation, taking into account the eﬀects
of training instances using kernel functions, as we do here. This idea is
also used in nonparametric smoothers (chapter 8) and kernel machines
(chapter 13).
The requirement here is that CN be invertible and hence positive definite. For this, K should be semideﬁnite so that after adding β−1 > 0 to
the diagonals, we get positive deﬁniteness. We also see that the costliest
operation is this inversion of N × N matrix, which fortunately needs to be

359

14.5 Notes

N = 20
1
0
−1
−1

−0.5

0
N = 10

0.5

1

−0.5

0
N=5

0.5

1

−0.5

0

0.5

1

1
0
−1
−1

1
0
−1
−1

Figure 14.7 Gaussian process regression using a Gaussian kernel with s 2 = 0.5
and varying number of training data. We see how variance of the prediction is
larger where there is few data.

calculated only once (during training) and stored. Still, for large N, one
may need an approximation.
When we use it for classiﬁcation for a two-class problem, the output is
ﬁltered through a sigmoid, y = sigmoid(w T x), and the distribution of y is
no longer Gaussian. The derivation is similar except that the conditional
p(rN+1 |x N+1 , X, r) is not Gaussian either and we need to approximate,
for example, using Laplace approximation (Bishop 2006; Rasmussen and
Williams 2006).

14.5

Notes
Bayesian approaches have become popular recently with advances in computational power allowing us to sample from or approximate the posterior probabilities. Truth has many cloaks. This preference of simplicity
appears in many contexts as the Bayesian approach, regularization, min-

360

14 Bayesian Estimation

type 2 maximum
likelihood
procedure

14.6

imum description length, or smoothing, and is at the heart of statistical
inference and hence machine learning.
On the other hand, the subjectivity of priors is disturbing and there
are objections to the Bayesian approach; see Gelman 2008, for example.
What is the use of a ﬂat prior, and why collect data if we already have a
peaked prior? Is a conjugate prior true or merely convenient?
Just like with support vector machines, in Gaussian processes too,
there are methods by which one can construct new kernels as functions
(e.g., weighted sums) of some other kernels and these weights or kernel parameters (e.g., spreads) can be optimized by a type 2 maximum
likelihood procedure, so called because we are now optimizing not the
parameters (which are the at or w t above) but the hyperparameters on a
second level (Bishop 2006; Rasmussen and Williams 2006).

Exercises
1. For the setting of ﬁgure 14.3, observe how the posterior changes as we change
2
N, σ 2 , and σ0 .
2. Let us denote by x the number of spam emails I receive in a random sample
of n. Assume that the prior for q, the proportion of spam emails is uniform
in [0, 1]. Find the posterior distribution for p(q|x).
2
3. As above, except that assume that p(q) ∼ N (μ0 , σ0 ). Also assume n is large
so that you can use central limit theorem and approximate binomial by a
Gaussian. Derive p(q|x).

4. What is Var(r ) when the maximum likelihood estimator is used? Compare it
with equation 14.17.
5. In ﬁgure 14.6, how does the ﬁt change when we change s 2 ?
6. Propose a ﬁltering algorithm to choose a subset of the training set in Gaussian
processes.
active learning

7. Active learning is when the learner is able to generate x itself and ask a supervisor to provide the corresponding r value during learning one by one,
instead of passively being given a training set. How can we implement active learning using Gaussian processes? (Hint: Where do we have the largest
uncertainty?)
8. Let us say we have inputs from two diﬀerent representations. How can we
use the approaches discussed in this chapter in such a case?

14.7 References

14.7

361

References
Bishop, C. M. 2006. Pattern Recognition and Machine Learning. New York:
Springer.
Figueiredo, M. A. T. 2003. “Adaptive Sparseness for Supervised Learning.” IEEE
Transactions on Pattern Analysis and Machine Intelligence 25: 1150–1159.
Gelman, A. 2008. “Objections to Bayesian statistics.” Bayesian Statistics 3: 445–
450.
MacKay, D. J. C. 1998. “Introduction to Gaussian Processes.” In Neural Networks
and Machine Learning, ed. C. M. Bishop, 133–166. Berlin: Springer.
MacKay, D. J. C. 2003. Information Theory, Inference, and Learning Algorithms.
Cambridge, UK: Cambridge University Press.
Rasmussen, C. E. , and C. K. I. Williams. 2006. Gaussian Processes for Machine
Learning. Cambridge, MA: MIT Press.
Tibshirani, R. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal
of the Royal Statistical Society B 58: 267–288.

15

Hidden Markov Models

We relax the assumption that instances in a sample are independent
and introduce Markov models to model input sequences as generated
by a parametric random process. We discuss how this modeling is
done as well as introduce an algorithm for learning the parameters
of such a model from example sequences.

15.1

Introduction
U n ti l n ow , we assumed that the instances that constitute a sample
are iid. This has the advantage that the likelihood of the sample is simply
the product of the likelihoods of the individual instances. This assumption, however, is not valid in applications where successive instances are
dependent. For example, in a word successive letters are dependent; in
English ‘h’ is very likely to follow ‘t’ but not ‘x’. Such processes where
there is a sequence of observations—for example, letters in a word, base
pairs in a DNA sequence—cannot be modeled as simple probability distributions. A similar example is speech recognition where speech utterances are composed of speech primitives called phonemes; only certain
sequences of phonemes are allowed, which are the words of the language.
At a higher level, words can be written or spoken in certain sequences to
form a sentence as deﬁned by the syntactic and semantic rules of the
language.
A sequence can be characterized as being generated by a parametric
random process. In this chapter, we discuss how this modeling is done
and also how the parameters of such a model can be learned from a
training sample of example sequences.

364

15 Hidden Markov Models

15.2

Discrete Markov Processes
Consider a system that at any time is in one of a set of N distinct states:
S1 , S2 , . . . , SN . The state at time t is denoted as qt , t = 1, 2, . . ., so, for
example, qt = Si means that at time t, the system is in state Si . Though we
write “time” as if this should be a temporal sequence, the methodology is
valid for any sequencing, be it in time, space, position on the DNA string,
and so forth.
At regularly spaced discrete times, the system moves to a state with a
given probability, depending on the values of the previous states:
P (qt+1 = Sj |qt = Si , qt−1 = Sk , · · ·)

Markov model

(15.1)

transition
probabilities

(15.2)

For the special case of a ﬁrst-order Markov model, the state at time t +1
depends only on state at time t, regardless of the states in the previous
times:
P (qt+1 = Sj |qt = Si , qt−1 = Sk , · · ·) = P (qt+1 = Sj |qt = Si )
This corresponds to saying that, given the present state, the future
is independent of the past. This is just a mathematical version of the
saying, Today is the ﬁrst day of the rest of your life.
We further simplify the model—that is, regularize—by assuming that
these transition probabilities are independent of time:
aij ≡ P (qt+1 = Sj |qt = Si )
satisfying
N

(15.3)

aij ≥ 0 and

aij = 1
j=1

stochastic
automaton

initial probabilities

(15.4)

So, going from Si to Sj has the same probability no matter when it
happens, or where it happens in the observation sequence. A = [aij ] is a
N × N matrix whose rows sum to 1.
This can be seen as a stochastic automaton (see ﬁgure 15.1). From
each state Si , the system moves to state Sj with probability aij , and this
probability is the same for any t. The only special case is the ﬁrst state.
We deﬁne initial probabilities, πi , which is the probability that the ﬁrst
state in the sequence is Si :
πi ≡ P (q1 = Si )

365

15.2 Discrete Markov Processes

a11

a12

π1

1

a21

a13
3

π2

2

π3

Figure 15.1 Example of a Markov model with three states. This is a stochastic
automaton where πi is the probability that the system starts in state Si , and aij
is the probability that the system moves from state Si to state Sj .

satisfying
N

πi = 1

(15.5)
i=1

observable Markov
model

Π = [πi ] is a vector of N elements that sum to 1.
In an observable Markov model, the states are observable. At any time
t, we know qt , and as the system moves from one state to another, we
get an observation sequence that is a sequence of states. The output of
the process is the set of states at each instant of time where each state
corresponds to a physical observable event.
We have an observation sequence O that is the state sequence O = Q =
{q1 q2 · · · qT }, whose probability is given as
T

(15.6)

P (O = Q|A, Π) = P (q1 )

P (qt |qt−1 ) = πq1 aq1 q2 · · · aqT −1 qT
t=2

πq1 is the probability that the ﬁrst state is q1 , aq1 q2 is the probability of
going from q1 to q2 , and so on. We multiply these probabilities to get the
probability of the whole sequence.
Let us now see an example (Rabiner and Juang 1986) to help us demonstrate. Assume we have N urns where each urn contains balls of only one
color. So there is an urn of red balls, another of blue balls, and so forth.

366

15 Hidden Markov Models

Somebody draws balls from urns one by one and shows us their color.
Let qt denote the color of the ball drawn at time t. Let us say we have
three states:
S1 : red, S2 = blue, S3 : green
with initial probabilities:
Π = [0.5, 0.2, 0.3]T
aij is the probability of drawing from urn j (a ball of color j) after
drawing a ball of color i from urn i. The transition matrix is, for example,
⎡
⎤
0.4 0.3 0.3
⎢
⎥
A = ⎣ 0.2 0.6 0.2 ⎦
0.1 0.1 0.8
Given Π and A, it is easy to generate K random sequences each of
length T . Let us see how we can calculate the probability of a sequence.
Assume that the ﬁrst four balls are “red, red, green, green.” This corresponds to the observation sequence O = {S1 , S1 , S3 , S3 }. Its probability
is
P (O|A, Π)

P (S1 ) · P (S1 |S1 ) · P (S3 |S1 ) · P (S3 |S3 )

=

π1 · a11 · a13 · a33

=

(15.7)

=

0.5 · 0.4 · 0.3 · 0.8 = 0.048

Now, let us see how we can learn the parameters, Π, A. Given K sek
quences of length T , where qt is the state at time t of sequence k, the
initial probability estimate is the number of sequences starting with Si
divided by the number of sequences:
(15.8)

ˆ
πi =

k
k 1(q1

#{sequences starting with Si }
=
#{sequences}

= Si )

K

where 1(b) is 1 if b is true and 0 otherwise.
As for the transition probabilities, the estimate for aij is the number of
transitions from Si to Sj divided by the total number of transitions from
Si over all sequences:
(15.9)

ˆ
aij =

#{transitions from Si to Sj }
=
#{transitions from Si }

k

T −1
k
k
t=1 1(qt = Si and qt+1
T −1
k
k
t=1 1(qt = Si )

= Sj )

ˆ
a12 is the number of times a blue ball follows a red ball divided by the
total number of red ball draws over all sequences.

15.3 Hidden Markov Models

15.3
hidden Markov
model

(15.10)
observation
probability
emission
probability

367

Hidden Markov Models
In a hidden Markov model (HMM), the states are not observable, but when
we visit a state, an observation is recorded that is a probabilistic function
of the state. We assume a discrete observation in each state from the set
{v1 , v2 , . . . , vM }:
bj (m) ≡ P (Ot = vm |qt = Sj )
bj (m) is the observation, or emission probability, that we observe vm , m =
1, . . . , M in state Sj . We again assume a homogeneous model in which the
probabilities do not depend on t. The values thus observed constitute
the observation sequence O. The state sequence Q is not observed, that
is what makes the model “hidden,” but it should be inferred from the observation sequence O. Note that there are typically many diﬀerent state
sequences Q that could have generated the same observation sequence
O, but with diﬀerent probabilities; just as, given an iid sample from a
normal distribution, there are an inﬁnite number of (μ, σ ) value pairs
possible, we are interested in the one having the highest likelihood of
generating the sample.
Note also that in this case of a hidden Markov model, there are two
sources of randomness. In addition to randomly moving from one state
to another, the observation in a state is also random.
Let us go back to our example. The hidden case corresponds to the
urn-and-ball example where each urn contains balls of diﬀerent colors.
Let bj (m) denote the probability of drawing a ball of color m from urn
j. We again observe a sequence of ball colors but without knowing the
sequence of urns from which the balls were drawn. So it is as if now the
urns are placed behind a curtain and somebody picks a ball at random
from one of the urns and shows us only the ball, without showing us the
urn from which it is picked. The ball is returned to the urn to keep the
probabilities the same. The number of ball colors may be diﬀerent from
the number of urns. For example, let us say we have three urns and the
observation sequence is
O = {red, red, green, blue, yellow}
In the previous case, knowing the observation (ball color), we knew the
state (urn) exactly because there were separate urns for separate colors
and each urn contained balls of only one color. The observable model is
a special case of the hidden model where M = N and bj (m) is 1 if j = m

368

15 Hidden Markov Models

π1

1

a 11

1

a 11

a 11

1

O2
πi

i

i

1

OT-1

i

a 11

O1

i
OT

πN
N

N

N

N

1

2

T-1

T

Figure 15.2 An HMM unfolded in time as a lattice (or trellis) showing all the
possible trajectories. One path, shown in thicker lines, is the actual (unknown)
state trajectory that generated the observation sequence.

and 0 otherwise. But in the case of a hidden model, a ball could have been
picked from any urn. In this case, for the same observation sequence O,
there may be many possible state sequences Q that could have generated
O (see ﬁgure 15.2).
To summarize and formalize, an HMM has the following elements:
1. N: Number of states in the model
S = {S1 , S2 , . . . , SN }
2. M: Number of distinct observation symbols in the alphabet
V = {v1 , v2 , . . . , vM }
3. State transition probabilities:
A = [aij ] where aij ≡ P (qt+1 = Sj |qt = Si )
4. Observation probabilities:
B = [bj (m)] where bj (m) ≡ P (Ot = vm |qt = Sj )

15.4 Three Basic Problems of HMMs

369

5. Initial state probabilities:
Π = [πi ] where πi ≡ P (q1 = Si )
N and M are implicitly deﬁned in the other parameters so λ = (A, B, Π)
is taken as the parameter set of an HMM. Given λ, the model can be
used to generate an arbitrary number of observation sequences of arbitrary length, but as usual, we are interested in the other direction, that of
estimating the parameters of the model given a training set of sequences.

15.4

Three Basic Problems of HMMs
Given a number of sequences of observations, we are interested in three
problems:
1. Given a model λ, we would like to evaluate the probability of any given
observation sequence, O = {O1 O2 · · · OT }, namely, P (O|λ).
2. Given a model λ and an observation sequence O, we would like to ﬁnd
out the state sequence Q = {q1 q2 · · · qT }, which has the highest probability of generating O; namely, we want to ﬁnd Q∗ that maximizes
P (Q|O, λ).
3. Given a training set of observation sequences, X = {O k }k , we would
like to learn the model that maximizes the probability of generating
X; namely, we want to ﬁnd λ∗ that maximizes P (X|λ).
Let us see solutions to these one by one, with each solution used to
solve the next problem, until we get to calculating λ or learning a model
from data.

15.5

Evaluation Problem
Given an observation sequence O = {O1 O2 · · · OT } and a state sequence
Q = {q1 q2 · · · qT }, the probability of observing O given the state sequence Q is simply
T

(15.11)

P (O|Q, λ) =

P (Ot |qt , λ) = bq1 (O1 ) · bq2 (O2 ) · · · bqT (OT )
t=1

370

15 Hidden Markov Models

which we cannot calculate because we do not know the state sequence.
The probability of the state sequence Q is
T

(15.12)

P (qt |qt−1 ) = πq1 aq1 q2 · · · aqT −1 qT

P (Q|λ) = P (q1 )
t=2

Then the joint probability is
T

=

P (O, Q|λ)

T

P (qt |qt−1 )

P (q1 )
t=2

=

(15.13)

P (Ot |qt )
t=1

πq1 bq1 (O1 )aq1 q2 bq2 (O2 ) · · · aqT −1 qT bqT (OT )

We can compute P (O|λ) by marginalizing over the joint, namely, by
summing up over all possible Q:
P (O|λ) =

P (O, Q|λ)
all possible Q

forward-backward
procedure

forward variable

(15.14)

However, this is not practical since there are N T possible Q, assuming
that all the probabilities are nonzero. Fortunately, there is an eﬃcient
procedure to calculate P (O|λ), which is called the forward-backward procedure (see ﬁgure 15.3). It is based on the idea of dividing the observation
sequence into two parts: the ﬁrst one starting from time 1 until time t,
and the second one from time t + 1 until T .
We deﬁne the forward variable αt (i) as the probability of observing the
partial sequence {O1 · · · Ot } until time t and being in Si at time t, given
the model λ:
αt (i) ≡ P (O1 · · · Ot , qt = Si |λ)
The nice thing about this is that it can be calculated recursively by
accumulating results on the way.
Initialization:

(15.15)

≡

P (O1 , q1 = Si |λ)

=

P (O1 |q1 = Si , λ)P (q1 = Si |λ)

=

α1 (i)

πi bi (O1 )

Recursion (see ﬁgure 15.3a):
αt+1 (j) ≡ P (O1 · · · Ot+1 , qt+1 = Sj |λ)

371

15.5 Evaluation Problem

1

αi

1
aij

i

j

aij

i

j

βj

Ot+1

Ot+1

N
N
t

t+1

t

(a) Forward

t+1
(b) Backward

Figure 15.3 Forward-backward procedure: (a) computation of αt (j) and (b)
computation of βt (i).

=

P (O1 · · · Ot+1 |qt+1 = Sj , λ)P (qt+1 = Sj |λ)

=

P (O1 · · · Ot |qt+1 = Sj , λ)P (Ot+1 |qt+1 = Sj , λ)P (qt+1 = Sj |λ)

=

P (O1 · · · Ot , qt+1 = Sj |λ)P (Ot+1 |qt+1 = Sj , λ)

=

P (Ot+1 |qt+1 = Sj , λ)

P (O1 · · · Ot , qt = Si , qt+1 = Sj |λ)
i

=

P (Ot+1 |qt+1 = Sj , λ)
P (O1 · · · Ot , qt+1 = Sj |qt = Si , λ)P (qt = Si |λ)
i

=

P (Ot+1 |qt+1 = Sj , λ)
P (O1 · · · Ot |qt = Si , λ)P (qt+1 = Sj |qt = Si , λ)P (qt = Si |λ)
i

=

P (Ot+1 |qt+1 = Sj , λ)
P (O1 · · · Ot , qt = Si |λ)P (qt+1 = Sj |qt = Si , λ)
⎡

(15.16)

=

⎣

i
N

⎤
αt (i)aij ⎦ bj (Ot+1 )

i=1

αt (i) explains the ﬁrst t observations and ends in state Si . We multiply
this by the probability aij to move to state Sj , and because there are

372

15 Hidden Markov Models

N possible previous states, we need to sum up over all such possible
previous Si . bj (Ot+1 ) then is the probability we generate the (t + 1)st
observation while in state Sj at time t + 1.
When we calculate the forward variables, it is easy to calculate the probability of the observation sequence:
N

P (O|λ)

=

P (O, qT = Si |λ)
i=1
N

=

(15.17)

αT (i)
i=1

backward variable

(15.18)

αT (i) is the probability of generating the full observation sequence and
ending up in state Si . We need to sum up over all such possible ﬁnal
states.
Computing αt (i) is O(N 2 T ), and this solves our ﬁrst evaluation problem in a reasonable amount of time. We do not need it now but let us
similarly deﬁne the backward variable, βt (i), which is the probability of
being in Si at time t and observing the partial sequence Ot+1 · · · OT :
βt (i) ≡ P (Ot+1 · · · OT |qt = Si , λ)
This can again be recursively computed as follows, this time going in
the backward direction:
Initialization (arbitrarily to 1):
βT (i) = 1
Recursion (see ﬁgure 15.3b):
βt (i)

≡

P (Ot+1 · · · OT |qt = Si , λ)

=

P (Ot+1 · · · OT , qt+1 = Sj |qt = Si , λ)
j

P (Ot+1 · · · OT |qt+1 = Sj , qt = Si , λ)P (qt+1 = Sj |qt = Si , λ)

=
j

=

P (Ot+1 |qt+1 = Sj , qt = Si , λ)
j

P (Ot+2 · · · OT |qt+1 = Sj , qt = Si , λ)P (qt+1 = Sj |qt = Si , λ)
=

P (Ot+1 |qt+1 = Sj , λ)
j

373

15.6 Finding the State Sequence

P (Ot+2 · · · OT |qt+1 = Sj , λ)P (qt+1 = Sj |qt = Si , λ)
N

=

(15.19)

aij bj (Ot+1 )βt+1 (j)
j=1

When in state Si , we can go to N possible next states Sj , each with
probability aij . While there, we generate the (t + 1)st observation and
βt+1 (j) explains all the observations after time t + 1, continuing from
there.
One word of caution about implementation is necessary here: Both αt
and βt values are calculated by multiplying small probabilities, and with
long sequences we risk getting underﬂow. To avoid this, at each time
step, we normalize αt (i) by multiplying it with
ct =
j

1
αt (j)

We also normalize βt (i) by multiplying it with the same ct (βt (i) do not
sum to 1). We cannot use equation 15.17 after normalization; instead, we
have (Rabiner 1989)
(15.20)

P (O|λ) =

1
t

15.6

ct

or log P (O|λ) = −

log ct
t

Finding the State Sequence
We now move on to the second problem, that of ﬁnding the state sequence Q = {q1 q2 · · · qT } having the highest probability of generating
the observation sequence O = {O1 O2 · · · OT }, given the model λ.
Let us deﬁne γt (i) as the probability of being in state Si at time t, given
O and λ, which can be computed as

(15.21)

γt (i)

≡
=
=
=

(15.22)

=

P (qt = Si |O, λ)
P (O|qt = Si , λ)P (qt = Si |λ)
P (O|λ)
P (O1 · · · Ot |qt = Si , λ)P (Ot+1 · · · OT |qt = Si , λ)P (qt = Si |λ)
N
j=1 P (O, qt

= Sj |λ)

P (O1 · · · Ot , qt = Si |λ)P (Ot+1 · · · OT |qt = Si , λ)
N
j=1 P (O|qt

αt (i)βt (i)
N
j=1 αt (j)βt (j)

= Sj , λ)P (qt = Sj |λ)

374

15 Hidden Markov Models

Here we see how nicely αt (i) and βt (i) split the sequence between
them: the forward variable αt (i) explains the starting part of the sequence until time t and ends in Si , and the backward variable βt (i) takes
it from there and explains the ending part until time T .
The numerator αt (i)βt (i) explains the whole sequence given that at
time t, the system is in state Si . We need to normalize by dividing this
over all possible intermediate states that can be traversed at time t, and
guarantee that i γt (i) = 1.
To ﬁnd the state sequence, for each time step t, we can choose the state
that has the highest probability:
(15.23)

Viterbi algorithm

(15.24)

∗
qt = arg max γt (i)
i

but this may choose Si and Sj as the most probable states at time t and
t + 1 even when aij = 0. To ﬁnd the single best state sequence (path), we
use the Viterbi algorithm, based on dynamic programming, which takes
such transition probabilities into account.
Given state sequence Q = q1 q2 · · · qT and observation sequence O =
O1 · · · OT , we deﬁne δt (i) as the probability of the highest probability
path at time t that accounts for the ﬁrst t observations and ends in Si :
δt (i) ≡

max

q1 q2 ···qt −1

p(q1 q2 · · · qt−1 , qt = Si , O1 · · · Ot |λ)

Then we can recursively calculate δt+1 (i) and the optimal path can be
read by backtracking from T , choosing the most probable at each instant.
The algorithm is as follows:
1. Initialization:
δ1 (i)

=

πi bi (O1 )

ψ1 (i)

=

0

2. Recursion:
δt (j)

=

max δt−1 (i)aij · bj (Ot )

ψt (j)

=

arg max δt−1 (i)aij

i

i

3. Termination:
p∗

=

max δT (i)

∗
qT

=

arg max δT (i)

i

i

15.7 Learning Model Parameters

375

Figure 15.4 Computation of arc probabilities, ξt (i, j).

4. Path (state sequence) backtracking:
∗
∗
qt = ψt+1 (qt+1 ), t = T − 1, T − 2, . . . , 1

Using the lattice structure of ﬁgure 15.2, ψt (j) keeps track of the state
that maximizes δt (j) at time t − 1, that is, the best previous state. The
Viterbi algorithm has the same complexity with the forward phase, where
instead of the sum, we take the maximum at each step.

15.7

Learning Model Parameters
We now move on to the third problem, learning an HMM from data.
The approach is maximum likelihood, and we would like to calculate
λ∗ that maximizes the likelihood of the sample of training sequences,
X = {O k }K , namely, P (X|λ). We start by deﬁning a new variable that
k=1
will become handy later on.
We deﬁne ξt (i, j) as the probability of being in Si at time t and in Sj at
time t + 1, given the whole observation O and λ:

(15.25)

ξt (i, j) ≡ P (qt = Si , qt+1 = Sj |O, λ)
which can be computed as (see ﬁgure 15.4)
ξt (i, j)

≡
=

P (qt = Si , qt+1 = Sj |O, λ)
P (O|qt = Si , qt+1 = Sj , λ)P (qt = Si , qt+1 = Sj |λ)
P (O|λ)

376

15 Hidden Markov Models

=
=

=

=
=

(15.26)

P (O|qt = Si , qt+1 = Sj , λ)P (qt+1 = Sj |qt = Si , λ)P (qt = Si |λ)
P (O|λ)
1
P (O1 · · · Ot |qt = Si , λ)P (Ot+1 |qt+1 = Sj , λ)
P (O|λ)
P (Ot+2 · · · OT |qt+1 = Sj , λ)aij P (qt = Si |λ)
1
P (O1 · · · Ot , qt = Si |λ)P (Ot+1 |qt+1 = Sj , λ)
P (O|λ)
P (Ot+2 · · · OT |qt+1 = Sj , λ)aij
αt (i)bj (Ot+1 )βt+1 (j)aij
k
l P (qt = Sk , qt+1 = Sl , O|λ)
αt (i)aij bj (Ot+1 )βt+1 (j)
k
l αt (k)akl bl (Ot+1 )βt+1 (l)

αt (i) explains the ﬁrst t observations and ends in state Si at time t. We
move on to state Sj with probability aij , generate the (t+1)st observation,
and continue from Sj at time t + 1 to generate the rest of the observation
sequence. We normalize by dividing for all such possible pairs that can
be visited at time t and t + 1.
If we want, we can also calculate the probability of being in state Si
at time t by marginalizing over the arc probabilities for all possible next
states:
N

(15.27)

γt (i) =

ξt (i, j)
j=1

soft counts

Baum-Welch
algorithm

Note that if the Markov model were not hidden but observable, both
γt (i) and ξt (i, j) would be 0/1. In this case when they are not, we estimate
them with posterior probabilities that give us soft counts. This is just like
the diﬀerence between supervised classiﬁcation and unsupervised clustering where we did and did not know the class labels, respectively. In
unsupervised clustering using EM (section 7.4), not knowing the class labels, we estimated them ﬁrst (in the E-step) and calculated the parameters
with these estimates (in the M-step).
Similarly here we have the Baum-Welch algorithm, which is an EM procedure. At each iteration, ﬁrst in the E-step, we compute ξt (i, j) and γt (i)
values given the current λ = (A, B, Π), and then in the M-step, we recalculate λ given ξt (i, j) and γt (i). These two steps are alternated until
convergence during which, it has been shown, P (O|λ) never decreases.

15.7 Learning Model Parameters

377

Assume indicator variables zit as
(15.28)

zit =

1
0

if qt = Si
otherwise

and
(15.29)

t
zij =

1
0

if qt = Si and qt+1 = Sj
otherwise

These are 0/1 in the case of an observable Markov model and are hidden random variables in the case of an HMM. In this latter case, we estimate them in the E-step as
E[zit ]

=

γt (i)

t
E[zij ]

(15.30)

=

ξt (i, j)

In the M-step, we calculate the parameters given these estimated values. The expected number of transitions from Si to Sj is t ξt (i, j) and
the total number of transitions from Si is t γt (i). The ratio of these two
gives us the probability of transition from Si to Sj at any time:
(15.31)

ˆ
aij =

T −1
t=1 ξt (i, j)
T −1
t=1 γt (i)

Note that this is the same as equation 15.9, except that the actual counts
are replaced by estimated soft counts.
The probability of observing vm in Sj is the expected number of times
vm is observed when the system is in Sj over the total number of times
the system is in Sj :
(15.32)

ˆ
bj (m) =

T
t=1 γt (j)1(Ot =
T
t=1 γt (j)

vm )

When there are multiple observation sequences
X = {O k }K
k=1
which we assume to be independent
K

P (O k |λ)

P (X|λ) =
k=1

378

15 Hidden Markov Models

the parameters are now averages over all observations in all sequences:
=
=

ˆ
πi

15.8

ˆ
aij
ˆ
bj (m)

(15.33)

=

K
k=1
K
k=1
K
k=1

Tk −1 k
t=1 ξt (i, j)
Tk −1 k
t=1 γt (i)
Tk
k
k
t=1 γt (j)1(Ot =
Tk
K
k
k=1
t=1 γt (j)

vm )

K
k
k=1 γ1 (i)

K

Continuous Observations
In our discussion, we assumed discrete observations modeled as a multinomial
M

(15.34)

t

bj (m)rm

P (Ot |qt = Sj , λ) =
m=1

where
(15.35)

t
rm =

1
0

if Ot = vm
otherwise

If the inputs are continuous, one possibility is to discretize them and
then use these discrete values as observations. Typically, a vector quantizer (section 7.3) is used for this purpose of converting continuous values to the discrete index of the closest reference vector. For example,
in speech recognition, a word utterance is divided into short speech segments corresponding to phonemes or part of phonemes; after preprocessing, these are discretized using a vector quantizer and an HMM is
then used to model a word utterance as a sequence of them.
We remember that k-means used for vector quantization is the hard
version of a Gaussian mixture model:
L

(15.36)

p(Ot |qt = Sj , λ) =

P (Gl )p(Ot |qt = Sj , Gl , λ)
l=1

where
(15.37)

p(Ot |qt = Sj , Gl , λ) ∼ N (μl , Σl )
and the observations are kept continuous. In this case of Gaussian mixtures, EM equations can be derived for the component parameters (with

15.9 The HMM with Input

379

suitable regularization to keep the number of parameters in check) and
the mixture proportions (Rabiner 1989).
Let us see the case of a scalar continuous observation, Ot ∈ . The
easiest is to assume a normal distribution:
(15.38)

2
p(Ot |qt = Sj , λ) ∼ N (μj , σj )

which implies that in state Sj , the observation is drawn from a normal
2
with mean μj and variance σj . The M-step equations in this case are

15.9

ˆ
μj

=

t

ˆ2
σj

(15.39)

=

t

γt (j)Ot
t γt (j)
ˆ
γt (j)(Ot − μj )2
γt (j)
t

The HMM with Input
In some applications, additional to the observation sequence Ot , we have
an input sequence, xt . We can condition the observation Ot in state Sj
on the input xt , and write P (Ot |qt = Sj , xt ). In the case when the observations are continuous scalars, we replace equation 15.38 with a generalized model

(15.40)

2
p(Ot |qt = Sj , xt , λ) ∼ N (gj (xt |θj ), σj )

where, for example, assuming a linear model, we have
(15.41)

Markov mixture of
experts

input-output HMM

gj (xt |wj , wj0 ) = wj xt + wj0
If the observations are discrete and multinomial, we have a classiﬁer
taking xt as input and generating a 1-of-M output, or we can generate
posterior class probabilities and keep the observations continuous.
Similarly, the state transition probabilities can also be conditioned on
the input, namely, P (qt+1 = Sj |qt = Si , xt ), which is implemented by a
classiﬁer choosing the state at time t + 1 as a function of the state at time
t and the input. This is a Markov mixture of experts (Meila and Jordan
1996) and is a generalization of the mixture of experts architecture (section 12.8) where the gating network keeps track of the decision it made
in the previous time step. Such an architecture is also called an inputoutput HMM (Bengio and Frasconi 1996) and has the advantage that the
model is no longer homogeneous; diﬀerent observation and transition

380

15 Hidden Markov Models

probabilities are used at diﬀerent time steps. There is still a single model
for each state, parameterized by θj , but it generates diﬀerent transition
or observation probabilities depending on the input seen. It is possible
that the input is not a single value but a window around time t making
the input a vector; this allows handling applications where the input and
observation sequences have diﬀerent lengths.
Even if there is no other explicit input sequence, an HMM with input
can be used by generating an “input” through some prespeciﬁed function
of previous observations
x t = f (Ot−τ , . . . , Ot−1 )
thereby providing a window of size τ of contextual input.

15.10

left-to-right HMMs

Model Selection in HMM
Just like any model, the complexity of an HMM should be tuned so as to
balance its complexity with the size and properties of the data at hand.
One possibility is to tune the topology of the HMM. In a fully connected
(ergodic) HMM, there is transition from a state to any other state, which
makes A a full N × N matrix. In some applications, only certain transitions are allowed, with the disallowed transitions having their aij = 0.
When there are fewer possible next states, N < N, the complexity of
forward-backward passes and the Viterbi procedure is O(NN T ) instead
of O(N 2 T ).
For example, in speech recognition, left-to-right HMMs are used, which
have their states ordered in time so that as time increases, the state index increases or stays the same. Such a constraint allows modeling sequences whose properties change over time as in speech, and when we
get to a state, we know approximately the states preceding it. There is
the property that we never move to a state with a smaller index, namely,
aij = 0, for j < i. Large changes in state indices are not allowed either,
namely, aij = 0, for j > i + τ. The example of the left-to-right HMM given
in ﬁgure 15.5 with τ = 2 has the state transition matrix
⎡
⎢
⎢
A=⎢
⎣

a11
0
0
0

a12
a22
0
0

a13
a23
a33
0

0
a24
a34
a44

⎤
⎥
⎥
⎥
⎦

381

15.10 Model Selection in HMM

a11
π1

1

a12

2

3

4

a13
Figure 15.5 Example of a left-to-right HMM.

Another factor that determines the complexity of an HMM is the number of states N. Because the states are hidden, their number is not known
and should be chosen before training. This is determined using prior information and can be ﬁne-tuned by cross-validation, namely, by checking
the likelihood of validation sequences.
When used for classiﬁcation, we have a set of HMMs, each one modeling the sequences belonging to one class. For example, in spoken word
recognition, examples of each word train a separate model, λi . Given a
new word utterance O to classify, all of the separate word models are
evaluated to calculate P (O|λi ). We then use Bayes’ rule to get the posterior probabilities
(15.42)

phones

P (λi |O) =

P (O|λi )P (λi )
j P (O|λj )P (λj )

where P (λi ) is the prior probability of word i. The utterance is assigned
to the word having the highest posterior. This is the likelihood-based
approach; there is also work on discriminative HMM trained directly to
maximize the posterior probabilities. When there are several pronunciations of the same word, these are deﬁned as parallel paths in the HMM
for the word.
In the case of a continuous input like speech, the diﬃcult task is that of
segmenting the signal into small discrete observations. Typically, phones
are used that are taken as the primitive parts, and combining them,
longer sequences (e.g., words) are formed. Each phone is recognized in
parallel (by the vector quantizer), then the HMM is used to combine them
serially. If the speech primitives are simple, then the HMM becomes complex and vice versa. In connected speech recognition where the words are
not uttered one by one with clear pauses between them, there is a hierarchy of HMMs at several levels; one combines phones to recognize words,

382

15 Hidden Markov Models

another combines words to recognize sentences by building a language
model, and so forth.
Hybrid neural network/HMM models were also used for speech recognition (Morgan and Bourlard 1995). In such a model, a multilayer perceptron (chapter 11) is used to capture temporally local but possibly complex
and nonlinear primitives, for example, phones, while the HMM is used to
learn the temporal structure. The neural network acts as a preprocessor
and translates the raw observations in a time window to a form that is
easier to model than the output of a vector quantizer.
An HMM can be visualized as a graphical model and evaluation in an
HMM is a special case of the belief propagation algorithm, as we will see in
chapter 16. The reason that we devote a special chapter is the widespread
successful use of this particular model, especially in automatic speech
recognition. When we discuss graphical models in detail, we will see
how the basic HMM architecture can be extended—for example, by having
multiple sequences, or by introducing hidden (latent) variables that can
simplify the model.

15.11

Notes
The HMM is a mature technology, and there are HMM-based commercial speech recognition systems in actual use (Rabiner and Juang 1993;
Jelinek 1997). In section 11.12, we discussed how to train multilayer
perceptrons for recognizing sequences. HMMs have the advantage over
time delay neural networks in that no time window needs to be deﬁned
a priori, and they train better than recurrent neural networks. HMMs are
applied to diverse sequence recognition tasks. Applications of HMMs to
bioinformatics is given in Baldi and Brunak 1998, and to natural language
processing in Manning and Schütze 1999. It is also applied to online
handwritten character recognition, which diﬀers from optical recognition
in that the writer writes on a touch-sensitive pad and the input is a sequence of (x, y) coordinates of the pen tip as it moves over the pad and is
not a static image. Bengio et al. (1995) explain a hybrid system for online
recognition where an MLP recognizes individual characters, and an HMM
combines them to recognize words. Various applications of the HMM
and several extensions, for example, discriminative HMMs, are discussed
in Bengio 1999. A more recent survey of what HMMs can and cannot do
is Bilmes 2006.

15.12 Exercises

383

In any such recognition system, one critical point is to decide how
much to do things in parallel and what to leave to serial processing. In
speech recognition, phonemes may be recognized by a parallel system
that corresponds to assuming that all the phoneme sound is uttered in
one time step. The word is then recognized serially by combining the
phonemes. In an alternative system, phonemes themselves may be designed as a sequence of simpler speech sounds, if the same phoneme
has many versions, for example, depending on the previous and following phonemes. Doing things in parallel is good but only to a degree; one
should ﬁnd the ideal balance of parallel and serial processing. To be able
to call anyone at the touch of a button, we would need millions of buttons
on our telephone; instead, we have ten buttons and we press them in a
sequence to dial the number.
We will discuss graphical models in chapter 16 where we will see that
HMMs can be considered a special class of graphical models and inference
and learning operations on HMMs are analogous to their counterparts in
Bayesian networks (Smyth, Heckerman, and Jordan 1997). As we will see
shortly, there are various extensions to HMMs like factorial HMMs where
at each time step, there are a number of states that collectively generate
the observation and tree-structured HMMs where there is a hierarchy of
states. The general formalism also allows us to treat continuous as well
as discrete states, known as linear dynamical systems. For some of these
models, exact inference is not possible and one needs to use approximation or sampling methods (Ghahramani 2001).

15.12

Exercises
1. Given the observable Markov model with three states, S1 , S2 , S3 , initial probabilities
Π = [0.5, 0.2, 0.3]T
and transition probabilities
⎤
⎡
0.4 0.3 0.3
⎥
⎢
A = ⎣ 0.2 0.6 0.2 ⎦
0.1 0.1 0.8
generate 100 sequences of 1,000 states.
2. Using the data generated by the previous exercise, estimate Π, A and compare
with the parameters used to generate the data.

384

15 Hidden Markov Models

3. Formalize a second-order Markov model. What are the parameters? How can
we calculate the probability of a given state sequence? How can the parameters be learned for the case of a observable model?
4. Show that any second- (or higher-order) Markov model can be converted to a
ﬁrst-order Markov model.
5. Some researchers deﬁne a Markov model as generating an observation while
traversing an arc, instead of on arrival at a state. Is this model any more
powerful than what we have discussed?
6. Generate training and validation sequences from an HMM of your choosing.
Then train diﬀerent HMMs by varying the number of hidden states on the
same training set and calculate the validation likelihoods. Observe how the
validation likelihood changes as the number of states increases.
7. If in equation 15.38 we have multivariate observations, what will be the Mstep equations?
8. Consider the urn-and-ball example where we draw without replacement. How
will it be diﬀerent?
9. Let us say at any time we have two observations from two diﬀerent alphabets;
for example, let us say we are observing the values of two currencies every
day. How can we implement this using HMM?
10. How can we have an incremental HMM where we add new hidden states when
necessary?

15.13

References
Baldi, P., and S. Brunak. 1998. Bioinformatics: The Machine Learning Approach.
Cambridge, MA: MIT Press.
Bengio, Y. 1999. “Markovian Models for Sequential Data.” Neural Computing
Surveys 2: 129–162.
Bengio, Y., and P. Frasconi. 1996. “Input-Output HMMs for Sequence Processing.” IEEE Transactions on Neural Networks 7: 1231–1249.
Bengio, Y., Y. Le Cun, C. Nohl, and C. Burges. 1995. “LeRec: A NN/HMM Hybrid
for On-line Handwriting Recognition.” Neural Computation 7: 1289–1303.
Bilmes, J. A. 2006. “What HMMs Can Do.” IEICE Transactions on Information
and Systems E89-D: 869–891.
Ghahramani, Z. 2001. “An Introduction to Hidden Markov Models and Bayesian
Networks.” International Journal of Pattern Recognition and Artiﬁcial Intelligence 15: 9–42.

15.13 References

385

Jelinek, F. 1997. Statistical Methods for Speech Recognition. Cambridge, MA:
MIT Press.
Manning, C. D., and H. Schütze. 1999. Foundations of Statistical Natural Language Processing. Cambridge, MA: MIT Press.
Meila, M., and M. I. Jordan. 1996. “Learning Fine Motion by Markov Mixtures
of Experts.” In Advances in Neural Information Processing Systems 8, ed.
D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, 1003–1009. Cambridge,
MA: MIT Press.
Morgan, N., and H. Bourlard. 1995. “Continuous Speech Recognition: An Introduction to the Hybrid HMM/Connectionist Approach.” IEEE Signal Processing
Magazine 12: 25–42.
Smyth, P., D. Heckerman, and M. I. Jordan. 1997. “Probabilistic Independence
Networks for Hidden Markov Probability Models.” Neural Computation 9:
227–269.
Rabiner, L. R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” Proceedings of the IEEE 77: 257–286.
Rabiner, L. R., and B. H. Juang. 1986. “An Introduction to Hidden Markov
Models.” IEEE Acoustics, Speech, and Signal Processing Magazine 3: 4–16.
Rabiner, L. R., and B. H. Juang. 1993. Fundamentals of Speech Recognition. New
York: Prentice Hall.

16

Graphical Models

Graphical models represent the interaction between variables visually and have the advantage that inference over a large number of
variables can be decomposed into a set of local calculations involving a small number of variables making use of conditional independencies. After some examples of inference by hand, we discuss the
concept of d-separation and the belief propagation algorithm on a
variety of graphs.

16.1
graphical models
Bayesian networks
belief networks
probabilistic
networks

directed acyclic
graph

Introduction
Graphical models, also called Bayesian networks, belief networks, or probabilistic networks, are composed of nodes and arcs between the nodes.
Each node corresponds to a random variable, X, and has a value corresponding to the probability of the random variable, P (X). If there is a
directed arc from node X to node Y , this indicates that X has a direct
inﬂuence on Y . This inﬂuence is speciﬁed by the conditional probability
P (Y |X). The network is a directed acyclic graph (DAG); namely, there are
no cycles. The nodes and the arcs between the nodes deﬁne the structure of the network, and the conditional probabilities are the parameters
given the structure.
A simple example is given in ﬁgure 16.1, which models that rain causes
the grass to get wet. It rains on 40 percent of the days and when it rains,
there is a 90 percent chance that the grass gets wet; maybe 10 percent of
the time it does not rain long enough for us to really consider the grass
wet enough. The random variables in this example are binary; they are
either true or false. There is a 20 percent probability that the grass gets
wet without its actually raining, for example, when a sprinkler is used.

388

16 Graphical Models

Figure 16.1 Bayesian network modeling that rain is the cause of wet grass.

We see that these three values completely specify the joint distribution
of P (R, W ). If P (R) = 0.4, then P (∼R) = 0.6, and similarly P (∼W |R) = 0.1
and P (∼W |∼R) = 0.8. The joint is written as
P (R, W ) = P (R)P (W |R)
We can calculate the individual (marginal) probability of wet grass by
summing up over the possible values that its parent node can take:
P (W )

P (R, W ) = P (W |R)P (R) + P (W |∼R)P (∼R)

=
R

=

causal graph

If we knew that it rained, the probability of wet grass would be 0.9; if
we knew for sure that it did not, it would be as low as 0.2; not knowing
whether it rained or not, the probability is 0.48.
Figure 16.1 shows a causal graph in that it explains that the cause
of wet grass is rain. Bayes’ rule allows us to invert the dependencies
and have a diagnosis. For example, knowing that the grass is wet, the
probability that it rained can be calculated as follows:
P (R|W ) =

independence

(16.1)

0.9 · 0.4 + 0.2 · 0.6 = 0.48

P (W |R)P (R)
= 0.75
P (W )

Knowing that the grass is wet increased the probability of rain from 0.4
to 0.75; this is because P (W |R) is high and P (W |∼R) is low.
We form graphs by adding nodes and arcs and generate dependencies.
X and Y are independent events if
p(X, Y ) = P (X)P (Y )

16.2 Canonical Cases for Conditional Independence

conditional
independence

(16.2)

389

X and Y are conditionally independent events given a third event Z if
P (X, Y |Z) = P (X|Z)P (Y |Z)
which can also be rewritten as

(16.3)

P (X|Y , Z) = P (X|Z)
In a graphical model, not all nodes are connected; actually, in general,
a node is connected to only a small number of other nodes. Certain
subgraphs imply conditional independence statements, and these allow
us to break down a complex graph into smaller subsets in which inferences can be done locally and whose results are later propagated over the
graph. There are three canonical cases and larger graphs are constructed
using these as subgraphs.

16.2

Canonical Cases for Conditional Independence
Case 1: Head-to-tail Connection
Three events may be connected serially, as seen in ﬁgure 16.2a. We see
here that X and Z are independent given Y : Knowing Y tells Z everything;
knowing the state of X does not add any extra knowledge for Z; we write
P (Z|Y , X) = P (Z|Y ). We say that Y blocks the path from X to Z, or in
other words, it separates them in the sense that if Y is removed, there is
no path between X to Z. In this case, the joint is written as

(16.4)

P (X, Y , Z) = P (X)P (Y |X)P (Z|Y )
Writing the joint this way implies independence:

(16.5)

P (Z|X, Y ) =

P (X, Y , Z)
P (X)P (Y |X)P (Z|Y )
=
= P (Z|Y )
P (X, Y )
P (X)P (Y |X)

Typically, X is the cause of Y and Y is the cause of Z. For example, as
seen in ﬁgure 16.2b, X can be cloudy sky, Y can be rain, and Z can be wet
grass. We can propagate information along the chain. If we do not know
the state of cloudy, we have
P (R)

=

P (R|C)P (C) + P (R|∼C)P (∼C) = 0.38

P (W )

=

P (W |R)P (R) + P (W |∼R)P (∼R) = 0.47

Let us say, in the morning we see that the weather is cloudy; what can
we say about the probability that the grass will be wet? To do this, we

390

16 Graphical Models

Figure 16.2 Head-to-tail connection. (a) Three nodes are connected serially. X
and Z are independent given the intermediate node Y : P (Z|Y , X) = P (Z|Y ). (b)
Example: Cloudy weather causes rain, which in turn causes wet grass.

need to propagate evidence ﬁrst to the intermediate node R, and then to
the query node W .
P (W |C) = P (W |R)P (R|C) + P (W |∼R)P (∼R|C) = 0.76
Knowing that the weather is cloudy increased the probability of wet
grass. We can also propagate evidence back using Bayes’ rule. Let us say
that we were traveling and on our return, see that our grass is wet; what
is the probability that the weather was cloudy that day? We use Bayes’
rule to invert the direction:
P (C|W ) =

P (W |C)P (C)
= 0.65
P (W )

Knowing that the grass is wet increased the probability of cloudy weather
from its default (prior) value of 0.4 to 0.65.

Case 2: Tail-to-tail Connection
X may be the parent of two nodes Y and Z, as shown in ﬁgure 16.3a. The
joint density is written as
(16.6)

P (X, Y , Z) = P (X)P (Y |X)P (Z|X)

16.2 Canonical Cases for Conditional Independence

391

Figure 16.3 Tail-to-tail connection. X is the parent of two nodes Y and Z. The
two child nodes are independent given the parent: P (Y |X, Z) = P (Y |X). In the
example, cloudy weather causes rain and also makes us less likely to turn the
sprinkler on.

Normally Y and Z are dependent through X; given X, they become
independent:
(16.7)

P (Y , Z|X) =

P (X)P (Y |X)P (Z|X)
P (X, Y , Z)
=
= P (Y |X)P (Z|X)
P (X)
P (X)

When its value is known, X blocks the path between Y and Z, or in
other words, separates them.
In ﬁgure 16.3b, we see an example where cloudy weather inﬂuences
both rain and the use of the sprinkler, one positively and the other negatively. Knowing that it rained, for example, we can invert the dependency
using Bayes’ rule and infer the cause:
P (C|R)

=
=

(16.8)

P (R|C)P (C)
P (R|C)P (C)
=
P (R)
C P (R, C)
P (R|C)P (C)
= 0.89
P (R|C)P (C) + P (R|∼C)P (∼C)

Note that this value is larger than P (C); knowing that it rained increased the probability that the weather is cloudy.
In ﬁgure 16.3a, if X is not known, knowing Y , for example, we can infer
X which we can then use to infer Z. In ﬁgure 16.3b, knowing the state of
the sprinkler has an eﬀect on the probability that it rained. If we know
that the sprinkler is on,
(16.9)

P (R|S)

=

P (R, C|S) = P (R|C)P (C|S) + P (R|∼C)P (∼C|S)
C

392

16 Graphical Models

Figure 16.4 Head-to-head connection. A node has two parents that are independent unless the child is given. For example, an event may have two independent
causes.

=

P (R|C)

=

P (S|∼C)P (∼C)
P (S|C)P (C)
+ P (R|∼C)
P (S)
P (∼S)

0.22

This is less than P (R) = 0.45; that is, knowing that the sprinkler is
on decreases the probability that it rained because sprinkler and rain
happens for diﬀerent states of cloudy weather. If the sprinkler is known
to be oﬀ, using the same approach, we ﬁnd that P (R|∼S) = 0.55; the
probability of rain increases this time.

Case 3: Head-to-head Connection
In a head-to-head node, there are two parents X and Y to a single node
Z, as shown in ﬁgure 16.4a. The joint density is written as
(16.10)

P (X, Y , Z) = P (X)P (Y )P (Z|X, Y )
X and Y are independent: P (X, Y ) = P (X) · P (Y ) (exercise 2); they become dependent when Z is known. The concept of blocking or separation
is diﬀerent for this case: The path between X and Y is blocked, or they
are separated, when Z is not observed; when Z (or any of its descendants)
is observed, they are not blocked, separated, nor are independent.

16.2 Canonical Cases for Conditional Independence

393

We see for example in ﬁgure 16.4b that node W has two parents, R
and S, and thus its probability is conditioned on the values of those two,
P (W |R, S).
Not knowing anything else, the probability that grass is wet is calculated by marginalizing over the joint:
P (W )

=

P (W , R, S)
R,S

=

P (W |R, S)P (R, S) + P (W |∼R, S)P (∼R, S)
+P (W |R, ∼S)P (R, ∼S) + P (W |∼R, ∼S)P (∼R, ∼S)

=

P (W |R, S)P (R)P (S) + P (W |∼R, S)P (∼R)P (S)
+P (W |R, ∼S)P (R)P (∼S) + P (W |∼R, ∼S)P (∼R)P (∼S)

=

0.52

Now, let us say that we know that the sprinkler is on, and we check
how this aﬀects the probability. This is a causal (predictive) inference:
P (W |S)

=

P (W , R|S)
R

=

P (W |R, S)P (R|S) + P (W |∼R, S)P (∼R|S)

=

P (W |R, S)P (R) + P (W |∼R, S)P (∼R)

=

0.92

We see that P (W |S) > P (W ); knowing that the sprinkler is on, the probability of wet grass increases.
We can also calculate the probability that the sprinkler is on, given that
the grass is wet. This is a diagnostic inference.
P (W |S)P (S)
= 0.35
P (W )
P (S|W ) > P (S), that is, knowing that the grass is wet increased the
probability of having the sprinkler on. Now let us assume that it rained.
Then we have
P (W |R, S)P (S|R)
P (W |R, S)P (S)
P (S|R, W ) =
=
P (W |R)
P (W |R)
= 0.21

P (S|W ) =

explaining away

which is less than P (S|W ). This is called explaining away; given that
we know it rained, the probability of sprinkler causing the wet grass decreases. Knowing that the grass is wet, rain and sprinkler become dependent. Similarly, P (S|∼R, W ) > P (S|W ). We see the same behavior when
we compare P (R|W ) and P (R|W , S) (exercise 3).

394

16 Graphical Models

Figure 16.5 Larger graphs are formed by combining simpler subgraphs over
which information is propagated using the implied conditional independencies.

We can construct larger graphs by combining such subgraphs. For example, in ﬁgure 16.5 where we combine the two subgraphs, we can, for
example, calculate the probability of having wet grass if it is cloudy:
P (W |C)

=

P (W , R, S|C)
R,S

=

P (W , R, S|C) + P (W , ∼R, S|C)
+P (W , R, ∼S|C) + P (W , ∼R, ∼S|C)

=

P (W |R, S, C)P (R, S|C)
+P (W |∼R, S, C)P (∼R, S|C)
+P (W |R, ∼S, C)P (R, ∼S|C)
+P (W |∼R, ∼S, C)P (∼R, ∼S|C)

=

P (W |R, S)P (R|C)P (S|C)
+P (W |∼R, S)P (∼R|C)P (S|C)
+P (W |R, ∼S)P (R|C)P (∼S|C)
+P (W |∼R, ∼S)P (∼R|C)P (∼S|C)

16.2 Canonical Cases for Conditional Independence

395

where we have used that P (W |R, S, C) = P (W |R, S); given R and S, W is
independent of C: R and S between them block the path between W and
C. Similarly, P (R, S|C) = P (R|C)P (S|C); given C, R and S are independent. We see the advantage of Bayesian networks here, which explicitly
encode independencies and allow breaking down inference into calculation over small groups of variables that are propagated from evidence
nodes to query nodes.
We can calculate P (C|W ) and have a diagnostic inference:
P (C|W ) =

P (W |C)P (C)
P (W )

The graphical representation is visual and helps understanding. The
network represents conditional independence statements and allows us
to break down the problem of representing the joint distribution of many
variables into local structures; this eases both analysis and computation.
Figure 16.5 represents a joint density of four binary variables that would
normally require ﬁfteen values (24 − 1) to be stored, whereas here there
are only nine. If each node has a small number of parents, the complexity
decreases from exponential to linear (in the number of nodes). As we
have seen earlier, inference is also easier as the joint density is broken
down into conditional densities of smaller groups of variables:
(16.11)

P (C, S, R, W ) = P (C)P (S|C)P (R|C)P (W |S, R)
In the general case, when we have variables X1 , . . . , Xd , we write
d

(16.12)

P (X1 , . . . , Xd ) =

P (Xi |parents(Xi ))
i=1

Then given any subset of Xi , namely, setting them to certain values due
to evidence, we can calculate the probability distribution of some other
subset of Xi by marginalizing over the joint. This is costly because it
requires calculating an exponential number of joint probability combinations, even though each of them can be simpliﬁed as in equation 16.11.
Note, however, that given the same evidence, for diﬀerent Xi , we may be
using the same intermediate values (products of conditional probabilities and sums for marginalization), and in section 16.5, we will discuss
the belief propagation algorithm to do inference cheaply by doing the local intermediate calculations once which we can use multiple times for
diﬀerent query nodes.

396

16 Graphical Models

hidden variables

causality

16.3
16.3.1

Though in this example we use binary variables, it is straightforward
to generalize for cases where the variables are discrete with any number
of possible values (with m possible values and k parents, a table of size
mk is needed for the conditional probabilities), or they can be continuous
(parameterized, e.g., p(Y |x) ∼ N (μ(x|θ), σ 2 ); see section 16.3.3).
One major advantage to using a Bayesian network is that we do not
need to designate explicitly certain variables as input and certain others
as output. The value of any set of variables can be established through
evidence and the probabilities of any other set of variables can be inferred, and the diﬀerence between unsupervised and supervised learning
becomes blurry. From this perspective, a graphical model can be thought
of as a “probabilistic database” (Jordan 2009), a machine that can answer
queries regarding the values of random variables.
In a problem, there may also be hidden variables whose values are
never known through evidence. The advantage of using hidden variables
is that the dependency structure can be more easily deﬁned. For example, in basket analysis when we want to ﬁnd the dependencies among
items sold, let us say we know that there is a dependency among “baby
food,” “diapers,” and “milk” in that a customer buying one of these is
very much likely to buy the other two. Instead of putting (noncausal)
arcs among these three, we may designate a hidden node “baby at home”
as the hidden cause of the consumption of these three items. When there
are hidden nodes, their values are estimated given the values of observed
nodes and ﬁlled in.
It should be stressed at this point that a link from a node X does not,
and need not, always imply a causality. It only implies a direct inﬂuence of
X over Y in the sense that the probability of Y is conditioned on the value
of X, and two nodes may have a link between them even if there is no
direct cause. It is preferable to have the causal relations in constructing
the network by providing an explanation of how the data is generated
(Pearl 2000) but such causes may not always be accessible.

Example Graphical Models
Naive Bayes’ Classiﬁer
For the case of classiﬁcation, the corresponding graphical model is shown
in ﬁgure 16.6a, with x as the input and C a multinomial variable taking

16.3 Example Graphical Models

397

Figure 16.6 (a) Graphical model for classiﬁcation. (b) Naive Bayes’ classiﬁer
assumes independent inputs.

one of K states for the class code. Bayes’ rule allows a diagnosis, as in
the rain and wet grass case we saw in ﬁgure 16.1:
P (C|x) =

naive Bayes’
classifier

P (C)p(x|C)
P (x)

If the inputs are independent, we have the graph shown in ﬁgure 16.6b,
which is called the naive Bayes’ classiﬁer, because it ignores possible dependencies, namely, correlations, among the inputs and reduces a multivariate problem to a group of univariate problems:
d

p(xj |C)

p(x|C) =
j=1

generative model

We have discussed classiﬁcation for this case in sections 5.5 and 5.7
for numeric and discrete x, respectively.
Clustering is also similar except that the multinomial class indicator
variable C is observed in classiﬁcation, but the similar variable, Z, cluster
indicator, is not observed. The E-step of the Expectation Maximization
algorithm (section 7.4) uses Bayes’ rule to invert the arc and estimates
the cluster indicator given the input.
Figure 16.6a is a generative model of the process that creates the data.
It is as if we ﬁrst pick a class C at random by sampling from P (C), and
then having ﬁxed C, we pick an x by sampling from p(x|C). Thinking of
data as sampled from a causal generative model that can be visualized
as a graph can ease understanding and also inference in many domains.

398

16 Graphical Models

Figure 16.7 Hidden Markov model can be drawn as a graphical model where q t
are the hidden states and shaded O t are observed.

phylogenetic tree

16.3.2
hidden Markov
model

For example, in text categorization, generating a text may be thought of
as the process where an author decides to write a document on a certain
topic and then chooses the set of words accordingly. In bioinformatics,
one area among many where a graphical approach used is the modeling of a phylogenetic tree; namely, a directed graph whose leaves are the
current species, nonterminal nodes are past ancestors that split into multiple species during a speciation event, and the conditional probabilities
depend on the evolutionary distance between a species and its ancestor
(Jordan 2004).

Hidden Markov Model
Hidden Markov models (HMM), which we previously discussed in chapter 15, are an example of case 1 where three successive states qt−2 , qt−1 , qt
correspond to three states on a chain in a ﬁrst-order Markov model. The
state at time t, qt , depends only on the state at time t − 1, qt−1 , and given
qt−1 , qt is independent of qt−2
P (qt |qt−1 , qt−2 ) = P (qt |qt−1 )
as given by the state transition probability matrix A (see ﬁgure 16.7).
Each hidden variable generates a discrete observation that is observed,
as given by the observation probability matrix B. The forward-backward
procedure of hidden Markov models is a special case of belief propagation that we will discuss shortly.

16.3 Example Graphical Models

399

Figure 16.8 Diﬀerent types of HMM model diﬀerent assumptions about the way
the observed data (shown shaded) is generated from Markov sequences of latent
variables.

input-output HMM

Diﬀerent HMM types can be shown as diﬀerent graphical models. In
ﬁgure 16.8a, an input-output HMM is shown (see section 15.9) where there
are two separate observed input-output sequences and there is also a
sequence of hidden states. The output observation depends both on the
state and also on the input; one can think of this as a B matrix whose
elements are not scalars but parametrized functions of the input. This
may similarly be seen as a mixture of expert architecture (section 12.8)

400

16 Graphical Models

factorial HMM

pedigree

coupled HMM

switching HMM

linear dynamical
system
Kalman filter

whose gating output (hidden state) depends also on the gating value at
the previous time step.
Another HMM type that can be easily visualized is a factorial HMM,
where there are multiple separate hidden sequences that interact to generate a single observation sequence. An example is a pedigree which
displays the parent-child relationship (Jordan 2004); ﬁgure 16.8b models
meiosis where the two sequences correspond to the chromosomes of the
father and the mother (which are independent), and at each locus (gene),
the oﬀspring receives one allele from the father or the other allele from
the mother.
A coupled HMM, shown in ﬁgure 16.8c, models two parallel but related
hidden sequences that generate two parallel observation sequences. For
example, in speech recognition, we may have one observed acoustic sequence of uttered words and one observed visual sequence of lip images,
each having its hidden states where the two are dependent.
In a switching HMM, shown in ﬁgure 16.8d, there are K parallel independent hidden state sequences and the state variable S at any one time
picks one of them and the chosen one generates the output. That is, we
switch between state sequences as we go along.
In HMM proper, though the observation may be continuous, state is
discrete; in a linear dynamical system, also known as the Kalman ﬁlter,
both the state and the observations are continuous. In the basic case,
state at time t is a linear function of state at t − 1 with additive zeromean Gaussian noise, and, at each state, the observation is another linear
function of the state with additive zero-mean Gaussian noise. The two
linear mappings and the covariances of the two noise sources make up
the parameters. All HMM variants we discussed earlier can similarly be
generalized to use continuous states.
By suitably modifying the graphical model, one can adapt the architecture to the characteristics of the process that generates the data. This
process of matching the model to the data is a model selection procedure to best trade oﬀ bias and variance. The disadvantage is that exact
inference may no longer be possible on such extended HMMs, and one
would need approximation or sampling methods (Ghahramani 2001; Jordan 2009).

16.3 Example Graphical Models

401

Figure 16.9 Bayesian network for linear regression.

16.3.3

Linear Regression
Linear regression can be visualized as a graphical model, as shown in ﬁgure 16.9. Input x t is drawn from a prior p(x) and the dependent variable
r t depend on the input x, weights w (drawn from a prior parameterized
by α, i.e., p(w) ∼ N (0, α−1 I)), and noise
(parameterized by β, i.e.,
p( ) ∼ N (0, β−1 )):

(16.13)

p(r t |x t , w) ∼ N (w T x t , β−1 )
There are N such pairs in the training set, which is shown by the rectangular plate in the ﬁgure. Given a new input x , the aim is to estimate
r , which will be E[r |x , w].
The weights w are not given but they can be estimated using the training set of [X, r]. Just as in equation 16.9, where C is the cause of R and
S, where we used
P (R|S) =

P (R, C|S) = P (R|C)P (C|S) + P (R|∼C)P (∼C|S)
C

402

16 Graphical Models

ﬁlling in C using S, which we in turn used to estimate R. Here, we write
p(r |x , r, X)

p(r |x , w)p(w|X, r)dw

=
(16.14)

=

p(r |x , w)

∝

p(r |x , w)

p(r|X, w)p(w)
dw
p(r)
p(r t |x t , w)p(w)dw
t

where the second line is due to Bayes’ rule and the third line is due to the
independence of instances in the training set.

16.4

d-separation

Bayes’ ball

d-Separation
We now generalize the concept of blocking and separation under the
name of d-separation, and we deﬁne it in a way so that for arbitrary subsets of nodes A, B, and C, we can check if A and B are independent given
C. Jordan (2009) visualizes this as a ball bouncing over the graph and
calls this the Bayes’ ball. We set the nodes in C to their values, place a
ball at each node in A, let the balls move around according to a set of
rules, and check whether a ball reaches any node in B. If this is the case,
they are dependent; otherwise, they are independent.
To check whether A and B are d-separated given C, we consider all
possible paths between any node in A and any node in B. Any such path
is blocked if
(a) the directions of the edges on the path either meet head-to-tail (case 1)
or tail-to-tail (case 2) and the node is in C, or
(b) the directions of the edges on the path meet head-to-head (case 3) and
neither that node nor any of its descendant is in C.
If all paths are blocked, we say that A and B are d-separated, that is,
independent, given C; otherwise, they are dependent. Examples are given
in ﬁgure 16.10.

16.5

Belief Propagation
Having discussed some inference examples by hand, we now are interested in an algorithm that can answer queries such as P (X|E) where X

16.5 Belief Propagation

403

Figure 16.10 Examples of d-separation. The path BCDF is blocked given C
because C is a tail-to-tail node. BEF G is blocked by F because F is a head-to-tail
node. BEF D is blocked unless F (or G) is given.

is any query node in the graph and E is any subset of evidence nodes
whose values are set to certain value. Following Pearl (1988), we start
with the simplest case of chains and gradually move on to more complex
graphs. Our aim is to ﬁnd the graph operation counterparts of probabilistic procedures such as Bayes’ rule or marginalization, so that the task of
inference can be mapped to general purpose graph algorithms.

16.5.1

Chains
A chain is a sequence of head-to-tail nodes with one root node without
any parent; all other nodes have exactly one parent node, and all nodes
except the very last, leaf, have a single child. If evidence is in the ancestors of X, we can just do a diagnostic inference and propagate evidence
down the chain; if evidence is in the descendants of X, we can do a causal
inference and propagate upward using Bayes’ rule. Let us see the general
case where we have evidence in both directions, up the chain E + and

404

16 Graphical Models

Figure 16.11 Inference along a chain.

down the chain E − (see ﬁgure 16.11). Note that any evidence node separates X from the nodes on the chain on the other side of the evidence
and their values do not aﬀect p(X); this is true in both directions.
We consider each node as a processor that receives messages from its
neighbors and pass it along after some local calculation. Each node X
locally calculates and stores two values: λ(X) ≡ P (E − |X) is the propagated E − that X receives from its child and forwards to its parent, and
π (X) ≡ P (X|E + ) is the propagated E + that X receives from its parent
and passes on to its child.
P (E|X)P (X)
P (E + , E − |X)P (X)
=
P (E)
P (E)
P (E + |X)P (E − |X)P (X)
P (E)
P (X|E + )P (E + )P (E − |X)P (X)
P (X)P (E)
+
αP (X|E )P (E − |X) = απ (X)λ(X)

=

P (X|E)

=
=
=

(16.15)

for some normalizing constant α, not dependent on the value of X. The
second line is there because E + and E − are independent given X, and the
third line is due to Bayes’ rule.
˜
If a node E is instantiated to a certain value e, λ(˜) ≡ 1 and λ(e) ≡ 0,
e
˜
for e = e. The leaf node X that is not instantiated has its λ(x) ≡ 1,
for all x values. The root node X that is not instantiated takes the prior
probabilities as π values: π (x) ≡ P (x), ∀x.
Given these initial conditions, we can devise recursive formulas to propagate evidence along the chain.
For the π -messages, we have
π (X)

≡

P (X|E + ) =

P (X|U, E + )P (U|E + )
U

(16.16)

P (X|U)P (U|E + ) =

=
U

P (X|U)π (U)
U

405

16.5 Belief Propagation

where the second line follows from the fact that U blocks the path between X and E + .
For the λ-messages, we have
λ(X)

≡

P (E − |X) =

P (E − |X, Y )P (Y |X)
Y

−

=

(16.17)

P (E |Y )P (Y |X) =
Y

P (Y |X)λ(Y )
U

where the second line follows from the fact that Y blocks the path between X and E − .
When the evidence nodes are set to a value, they initiate traﬃc and
nodes continue updating until there is convergence. Pearl (1988) views
this as a parallel machine where each node is implemented by a processor
that works in parallel with others and exchanges information through λand π -messages with its parent and child.

16.5.2

Trees
Chains are restrictive because each node can have only a single parent
and a single child, that is, a single cause and a single symptom. In a
tree, each node may have several children but all nodes, except the single
root, have exactly one parent. The same belief propagation also applies
here with the diﬀerence from chains being that a node receives diﬀerent
λ-messages from its children, λY (X) denoting the message X receives
from its child Y , and sends diﬀerent π -messages to its children, πY (X)
denoting the message X sends to its child Y .
Again, we divide possible evidence to two parts, E − are nodes that are
in the subtree rooted at the query node X, and E + are evidence nodes
elsewhere (see ﬁgure 16.12). Note that this second need not be an ancestor of X but may also be in a subtree rooted at a sibling of X. The
important point is that again X separates E + and E − so that we can write
P (E + , E − |X) = P (E + |X)P (E − |X), and hence have
P (X|E) = απ (X)λ(X)
where again α is a normalizing constant.
λ(X) is the evidence in the subtree rooted at X, and if X has two children Y and Z, as shown in ﬁgure 16.12, it can be calculated as
λ(X)

(16.18)

≡

−
−
−
P (EX |X) = P (EY , EZ |X)

=

−
−
P (EY |X)P (EZ |X) = λY (X)λZ (X)

406

16 Graphical Models

Figure 16.12 In a tree, a node may have several children but a single parent.

In the general case if X has m children, Yj , j = 1, . . . , m, then we multiply all their λ values:
m

(16.19)

λ(X) =

λYj (X)
j=1

Once X accumulates λ evidence from its children’s λ-messages, it propagates it up to its parent:
(16.20)

λX (U) =

λ(X)P (X|U)
X

Similarly and in the other direction, π (X) is the evidence elsewhere
that is accumulated in P (U |E + ) and passed on to X as a π -message:
(16.21)

+
π (X) ≡ P (X|EX ) =

+
P (X|U)P (U|EX ) =
U

P (X|U)πX (U)
U

This calculated π value is then propagated down to X’s children. Note
that what Y receives from X is what X receives from its parent U and
+
also from its other child Z; together they make up EY (see ﬁgure 16.12):
πY (X)

≡

+
+
−
P (X|EY ) = P (X|EX , EZ )

16.5 Belief Propagation

407

Figure 16.13 In a polytree, a node may have several children and several parents, but the graph is singly connected; that is, there is a single chain between
Ui and Yj passing through X.

=
(16.22)

=

−
+
+
−
+
P (EZ |X)P (X|EX )
P (EZ |X, EX )P (X|EX )
=
−
−
P (EZ )
P (EZ )
αλZ (X)π (X)

Again, if Y has not one sibling Z but multiple, we need to take a product
over all their λ values:
(16.23)

πYj (X) = α

λYs (X)π (X)
s=j

16.5.3
polytree

Polytrees
In a tree, a node has a single parent, that is, a single cause. In a polytree, a
node may have multiple parents, but we require that the graph be singly
connected, which means that there is a single chain between any two
nodes. If we remove X, the graph will split into two components. This is
+
−
necessary so that we can continue splitting EX into EX and EX , which are
independent given X (see ﬁgure 16.13).
If X has multiple parents Ui , i = 1, . . . , k, it receives π -messages from

408

16 Graphical Models

all of them, πX (Ui ), which it combines as follows:
π (X)

+
+
+
+
P (X|EX ) = P (X, EU1 X , EU2 X , . . . , EUk X )

≡
=

+
+
P (X|U1 , U2 , . . . , Uk )P (U1 |EU1 X ) · · · P (Uk |EUk X )

···
U1 U2

Uk
k

···

=

(16.24)

U1 U2

P (X|U1 , U2 , . . . , Uk )
Uk

πX (Ui )
i=1

and passes it on to its several children Yj , j = 1, . . . , m:
(16.25)

πYj (X) = α

λYs (X)π (X)
s=j

In this case when X has multiple parents, a λ-message X passes on
to one of its parents Ui combines not only the evidence X receives from
its children but also the π -messages X receives from its other parents
−
Ur , r = i; they together make up EUi X :
λX (Ui )

≡

−
P (EUi X |X)

=
X Ur =i

=
X Ur =i

=
X Ur =i

−
+
P (EX , EUr =i X , X, Ur =i |Ui )
−
+
P (EX , EUr =i X |X, Ur =i , Ui )P (X, Ur =i |Ui )
−
+
P (EX |X)P (EUr =i X |Ur =i )P (X|Ur =i , Ui )P (Ur =i |Ui )

−
P (EX |X)

=

+
+
P (Ur =i |EUr =i X )P (EUr =i X )

X Ur =i

=

β
X Ur =i

=
=

β

λ(X)
X

πX (Ur )P (X|U1 , . . . , Uk )
r =i

X Ur =i

(16.26)

P (X|Ur =i , Ui )P (Ur =i |Ui )

−
+
P (EX |X)P (Ur =i |EUr =i X )P (X|Ur =i , Ui )

λ(X)

β

P (Ur =i )

P (X|U1 , . . . , Uk )
Ur =i

πX (Ur )
r =i

As in a tree, to ﬁnd its overall λ, the parent multiplies the λ-messages
it receives from its children:
m

(16.27)

λ(X) =

λYj (X)
j=1

409

16.5 Belief Propagation

noisy OR

(16.28)

In this case of multiple parents, we need to store and manipulate the
conditional probability given all the parents, p(X|U1 , . . . , Uk ), which is
costly for large k. Approaches have been proposed to decrease the complexity from exponential in k to linear. For example, in a noisy OR gate,
any of the parents is suﬃcient to cause the event and the likelihood does
not decrease when multiple parent events occur. If the probability that X
happens when only cause Ui happens is 1 − qi
P (X|Ui , ∼Up=j ) = 1 − qi
the probability that X happens when a subset T of them occur is calculated as

(16.29)

P (X|T ) = 1 −

qi
ui ∈T

For example, let us say wet grass has two causes, rain and a sprinkler,
with qR = qS = 0.1; that is, both singly have a 90 percent probability of
causing wet grass. Then, P (W |R, ∼ S) = 0.9 and P (W |R, S) = 0.99.
Another possibility is to write the conditional probability as some function given a set of parameters, for example, as a linear model
⎞
⎛
(16.30)

P (X|U1 , . . . , Uk , w0 , w1 , . . . , wk ) = sigmoid ⎝

k

wi Ui + w0 ⎠

i=1

where sigmoid guarantees that the output is a probability between 0 and
1. During training, we can learn the parameters wi , i = 0, . . . , d, for example, to maximize the likelihood on a sample.

16.5.4

Junction Trees
If there is a loop, that is, if there is a cycle in the underlying undirected
graph—for example, if the parents of X share a common ancestor—the
algorithm we discussed earlier does not work. In such a case, there is
more than one path on which to propagate evidence and, for example,
while evaluating the probability at X, we cannot say that X separates E
+
−
into EX and EX as causal (upward) and diagnostic (downward) evidence;
removing X does not split the graph into two. Conditioning them on X
does not make them independent and the two can interact through some
other path not involving X.
We can still use the same algorithm if we can convert the graph to a
polytree. We deﬁne clique nodes that correspond to a set of original variables and connect them so that they form a tree (see ﬁgure 16.14). We

410

16 Graphical Models

Figure 16.14 (a) A multiply connected graph, and (b) its corresponding junction
tree with nodes clustered.

junction tree

16.6

Markov random
field

can then run the same belief propagation algorithm with some modiﬁcations. This is the basic idea behind the junction tree algorithm (Lauritzen
and Spiegelhalter 1988; Jensen 1996; Jordan 2009).

Undirected Graphs: Markov Random Fields
Up to now, we have discussed directed graphs where the inﬂuences are
undirectional and have used Bayes’ rule to invert the arcs. If the inﬂuences are symmetric, we represent them using an undirected graphical
model, also known as a Markov random ﬁeld. For example, neighboring
pixels in an image tend to have the same color—that is, are correlated—
and this correlation goes both ways.
Directed and undirected graphs deﬁne conditional independence differently, and, hence, there are probability distributions that are represented by a directed graph and not by an undirected graph, and vice
versa (Pearl 1988).
Because there are no directions and hence no distinction between the
head or the tail of an arc, the treatment of undirected graphs is simpler.
For example, it is much easier to check if A and B are independent given
C. We just check if after removing all nodes in C, we still have a path
between a node in A and a node in B. If so, they are dependent, otherwise,
if all paths between nodes in A and nodes in B pass through nodes in C
such that removal of C leaves nodes of A and nodes of B in separate
components, we have independence.

411

16.6 Undirected Graphs: Markov Random Fields

clique

potential function

(16.31)

In the case of an undirected graph, we do not talk about the parent
or the child but about cliques, which are sets of nodes such that there
exists a link between any two nodes in the set. A maximal clique has
the maximum number of elements. Instead of conditional probabilities
(implying a direction), in undirected graphs we have potential functions
ψC (XC ) where XC is the set of variables in clique C, and we deﬁne the
joint distribution as the product of the potential functions of the maximal
cliques of the graph
p(X) =

1
Z

ψC (XC )
C

where Z is the normalization constant to make sure that
(16.32)

Z=

p(X) = 1:

ψC (X)
X

moralization

X

C

It can be shown that a directed graph is already normalized (exercise 5).
Unlike in directed graphs, the potential functions in an undirected
graph do not need to have a probabilistic interpretation, and one has
more freedom in deﬁning them. In general, we can view potential functions as expressing local constraints, that is, favoring some local conﬁgurations over others. For example, in an image, we can deﬁne a pairwise
potential function between neighboring pixels, which takes a higher value
if their colors are similar than the case when they are diﬀerent (Bishop
2006). Then, setting some of the pixels to their values given as evidence,
we can estimate the values of other pixels that are not known, for example, due to occlusion.
If we have the directed graph, it is easy to redraw it as an undirected
graph, simply by dropping all the directions, and if a node has a single
parent, we can set the pairwise potential function simply to the conditional probability. If the node has more than one parent, however, the
“explaining away” phenomenon due to the head-to-head node makes the
parents dependent, and hence we should have the parents in the same
clique so that the clique potential includes all the parents. This is done by
connecting all the parents of a node by links so that they are completely
connected among them and form a clique. This is called “marrying” the
parents, and the process is called moralization. Incidentally, moralization
is one of the steps in generating a junction tree, which is undirected.
It is straightforward to adapt the belief propagation algorithm to work
on undirected graphs, and it is easier because the potential function is

412

16 Graphical Models

Figure 16.15 (a) A directed graph that would have a loop after moralization,
and (b) its corresponding factor graph that is a tree. The three factors are fa (R) ≡
P (R), fb (S) ≡ P (S), and fc (R, S, W ) ≡ P (W |R, S).

factor graph

(16.33)

sum-product
algorithm

symmetric and we do not need to make a diﬀerence between causal and
diagnostic evidence. Thus, we can do inference on undirected chains and
trees. But in polytrees where a node has multiple parents and moralization necessarily creates loops, this would not work. One trick is to
convert it to a factor graph that uses a second kind of factor nodes in
addition to the variable nodes, and we write the joint distribution as a
product of factors (Kschischang, Frey, and Loeliger 2001)
p(X) =

1
Z

fS (XS )
S

where Xs denotes a subset of the variable nodes used by factor S. Directed graphs are a special case where factors correspond to local conditional distributions, and undirected graphs are another special case
where factors are potential functions over maximal cliques. The advantage is that, as we can see in ﬁgure 16.15, the tree structure can be kept
even after moralization.
It is possible to generalize the belief propagation algorithm to work on
factor graphs; this is called the sum-product algorithm (Bishop 2006; Jordan 2009) where there is the same idea of doing local computations once
and propagating them through the graph as messages. The diﬀerence
now is that there are two types of messages because there are two kinds
of nodes, factors and variables, and we make a distinction between their

16.7 Learning the Structure of a Graphical Model

max-product
algorithm

16.7

413

messages. Note, however, that a factor graph is bipartite and one kind of
node can have a close encounter only with the second kind.
In belief propagation, or the sum-product algorithm, the aim is to ﬁnd
the probability of a set of nodes X given that another set of evidence
nodes E are clamped to a certain value, that is, P (X|E). In some applications, we may be interested in ﬁnding the setting of all X that maximizes the full joint probability distribution p(X). For example, in the
undirected case where potential functions code locally consistent conﬁgurations, such an approach would propagate local constraints over the
whole graph and ﬁnd a solution that maximizes global consistency. In a
graph where nodes correspond to pixels and pairwise potential functions
favor correlation, this approach would implement noise removal (Bishop
2006). The algorithm for this, named the max-product algorithm (Bishop
2006; Jordan 2009) is the same as the sum-product algorithm where we
take the maximum (choose the most likely value) rather than the sum
(marginalize). This is analogous to the diﬀerence between the forwardbackward procedure and the Viterbi algorithm in hidden Markov models
that we discussed in chapter 15.
Note that the nodes need not correspond to low-level concepts like pixels; in a vision application, for instance, we may have nodes for corners of
diﬀerent types or lines of diﬀerent orientations with potential functions
checking for compatibility, so as to see if they can be part of the same
interpretation—remember the Necker cube, for example—so that overall
consistent solutions emerge after the consolidation of local evidences.
The complexity of the inference algorithms on polytrees or junction
trees is determined by the maximum number of parents or the size of the
largest clique, and when this is large, exact inference may be infeasible. In
such a case, one needs to use an approximation or a sampling algorithm
(Jordan 1999; Bishop 2006; Jordan 2009).

Learning the Structure of a Graphical Model
As in any approach, learning a graphical model has two parts. The ﬁrst
is the learning of parameters given a structure; this is relatively easier
(Buntine 1996), and, in graphical models, conditional probability tables
or their parameterizations (as in equation 16.30) can be trained to maximize the likelihood, or by using a Bayesian approach if suitable priors
are known (chapter 14).

414

16 Graphical Models

The second, more diﬃcult, and interesting part is to learn the graph
structure (Cowell et al. 1999). This is basically a model selection problem, and just like the incremental approaches for learning the structure
of a multilayer perceptron (section 11.9), we can see this as a search in
the space of all possible graphs. One can, for example, consider operators that can add/remove arcs and/or hidden nodes and then do a search
evaluating the improvement at each step (using parameter learning at
each intermediate iteration). Note, however, that to check for overﬁtting,
one should regularize properly, corresponding to a Bayesian approach
with a prior that favors simpler graphs (Neapolitan 2004). However, because the state space is large, it is most helpful if there is a human expert
who can manually deﬁne causal relationships among variables and creates subgraphs of small groups of variables.

16.8

influence diagrams

16.9

Inﬂuence Diagrams
Just as in chapter 3, we generalized from probabilities to actions with
risks, inﬂuence diagrams are graphical models that allow the generalization of graphical models to include decisions and utilities. An inﬂuence
diagram contains chance nodes representing random variables that we
use in graphical models (see ﬁgure 16.16). It also has decision nodes and
a utility node. A decision node represents a choice of actions. A utility
node is where the utility is calculated. Decisions may be based on chance
nodes and may aﬀect other chance nodes and the utility node.
Inference on an inﬂuence diagram is an extension to belief propagation on a graphical model. Given evidence on some of the chance nodes,
this evidence is propagated, and for each possible decision, the utility is
calculated and the decision having the highest utility is chosen. The inﬂuence diagram for classiﬁcation of a given input is shown in ﬁgure 16.16.
Given the input, the decision node decides on a class, and for each choice
we incur a certain utility (risk).

Notes
Graphical models have two advantages. One is that we can visualize the
interaction of variables and have a better understanding of the process,
for example, by using a causal generative model. The second is that by
ﬁnding graph operations that correspond to basic probabilistic proce-

415

16.9 Notes

choose
class

x

U

Figure 16.16 Inﬂuence diagram corresponding to classiﬁcation. Depending on
input x, a class is chosen that incurs a certain utility (risk).

dynamic graphical
models

dures such as Bayes’ rule or marginalization, the task of inference can
be mapped to general-purpose graph algorithms that can be eﬃciently
represented and implemented.
The idea of visual representation of variables and dependencies between them as a graph, and the related factorization of a complicated
global function of many variables as a product of local functions involving a small subset of the variables for each, seems to be used in diﬀerent
domains in decision making, coding, and signal processing; Kschischang,
Frey, and Loeliger (2001) give a review.
The complexity of the inference algorithms on polytrees or junction
trees is determined by the maximum number of parents or the size of the
largest clique, and when this is large exact inference may be infeasible. In
such a case, one needs to use an approximation or a sampling algorithm.
Variational approximations and Markov chain Monte Carlo (MCMC) algorithms are discussed in Jordan et al. 1999, MacKay 2003, Andrieu et al.
2003, Bishop 2006, and Jordan 2009.
Graphical models are especially suited to represent Bayesian approaches
where in addition to nodes for variables, we also have nodes for hidden
parameters that inﬂuence the observed variables. We may also introduce a hierarchy where we have nodes for the hyperparameters—that is,
second-level parameters for the priors of the ﬁrst-level parameters, and
so on.
Hidden Markov models is one type of graphical model, and actually
any graphical model can be extended in time by unfolding it in time and
adding dependencies between successive copies. Such dynamic graphical
models ﬁnd application in areas where there is also a temporal dimension—
speech recognition, for example. In fact, a hidden Markov model is noth-

416

16 Graphical Models

Figure 16.17 A dynamic version where we have a chain of graphs to show dependency in weather in consecutive days.

ing but a sequence of clustering problems where the cluster index at time
t is dependent not only on observation at time t but also on the index at
time t − 1, and Baum-Welch algorithm is Expectation-Maximization extended to also include this dependency in time. In section 6.4, we discussed factor analysis where a small number of hidden factors generate
the observation; similarly, a linear dynamical system may be viewed as a
sequence of such factor analysis models where the current factors also
depend on the previous factors.
This dynamic dependency may be added when needed. For example,
ﬁgure 16.5 models the cause of wet grass for a particular day; if we believe that yesterday’s weather has an inﬂuence on today’s weather (and
we should—it tends to be cloudy on successive days, then sunny for a
number of days, and so on), we can have the dynamic graphical model
shown in ﬁgure 16.17 where we model this dependency.
The general graphical model formalism allows us to go beyond the
power of HMM proper and lead to improved performances, for example,
in speech recognition (Zweig 2003; Bilmes and Bartels 2005). Graphical
models are also used in computer vision—for example, in information
retrieval (Barnard et al. 2003) and scene analysis (Sudderth et al. 2008).
A review of the use of graphical models in bioinformatics (and related
software) is given in Donkers and Tuyls 2008.

16.10 Exercises

16.10

417

Exercises
1. With two independent inputs in a classiﬁcation problem, that is, p(x1 , x2 |C) =
p(x1 |C)p(x2 |C), how can we calculate p(x1 |x2 )? Derive the formula for p(xj |Ci ) ∼
2
N (μij , σij ).
2. For a head-to-head node, show that equation 16.10 implies P (X, Y ) = P (X) ·
P (Y ).
3. In ﬁgure 16.4, calculate P (R|W ), P (R|W , S), and P (R|W , ∼S).
4. In equation 16.30, X is binary. How do we need to modify it if X can take one
of K discrete values?
5. Show that in a directed graph where the joint distribution is written as equation 16.12, x p(x) = 1.
6. Draw the Necker cube as a graphical model deﬁning links to indicate mutually
reinforcing or inhibiting relations between diﬀerent corner interpretations.
7. How can we do inference on the dynamic weather graph shown in ﬁgure 16.17?
8. Write down the graphical model for linear logistic regression for two classes
in the manner of ﬁgure 16.9.
9. Propose a suitable goodness measure that can be used in learning graph
structure as a state-space search. What are suitable operators?
10. Generally, in a newspaper, a reporter writes a series of articles on successive
days related to the same topic as the story develops. How can we model this
using a graphical model?

16.11

References
Andrieu, C., N. de Freitas, A. Doucet, and M. I. Jordan. 2003. “An Introduction
to MCMC for Machine Learning.” Machine Learning 50: 5–43.
Barnard, K., P. Duygulu, D. Forsyth, N. de Freitas, D. M. Blei, and M. I. Jordan.
2003. “Matching Words and Pictures.” Journal of Machine Learning Research
3: 1107–1135.
Bilmes, J., and C. Bartels. 2005. “Graphical Model Architectures for Speech
Recognition.” IEEE Signal Processing Magazine 22: 89–100.
Bishop, C. M. 2006. Pattern Recognition and Machine Learning. New York:
Springer.
Buntine, W. 1996. “A Guide to the Literature on Learning Probabilistic Networks
from Data.” IEEE Transactions on Knowledge and Data Engineering 8: 195–
210.

418

16 Graphical Models

Cowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter. 1999. Probabilistic Networks and Expert Systems. New York: Springer.
Donkers, J., and K. Tuyls. 2008. “Belief Networks in Bioinformatics.” In Computational Intelligence in Bioinformatics, ed. A. Kelemen, A. Abraham, and Y.
Chen, 75–111. Berlin: Springer.
Ghahramani, Z. 2001. “An Introduction to Hidden Markov Models and Bayesian
Networks.” International Journal of Pattern Recognition and Artiﬁcial Intelligence 15: 9–42.
Jensen, F. 1996. An Introduction to Bayesian Networks. New York: Springer.
Jordan, M. I., ed. 1999. Learning in Graphical Models. Cambridge, MA: MIT
Press.
Jordan, M. I. 2004. “Graphical Models.” Statistical Science 19: 140–155.
Jordan, M. I. 2009. An Introduction to Probabilistic Graphical Models. Forthcoming.
Jordan, M. I., Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. 1999. “An Introduction to Variational Methods for Graphical Models.” In Learning in Graphical
Models, ed. M. I. Jordan, 105–161. Cambridge, MA: MIT Press.
Kschischang, F. R., B. J. Frey, and H.-A. Loeliger. 2001. “Factor Graphs and
the Sum-Product Algorithm.” IEEE Transactions on Information Theory 47:
498–519.
Lauritzen, S. L., and D. J. Spiegelhalter. 1988. “Local Computations with Probabilities on Graphical Structures and their Application to Expert Systems.”
Journal of Royal Statistical Society B 50: 157–224.
MacKay, D. J. C. 2003. Information Theory, Inference, and Learning Algorithms.
Cambridge, UK: Cambridge University Press.
Neapolitan, R. E. 2004. Learning Bayesian Networks. Upper Saddle River, NJ:
Pearson.
Pearl, J. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. San Francisco, CA: Morgan Kaufmann.
Pearl, J. 2000. Causality: Models, Reasoning, and Inference. Cambridge, UK:
Cambridge University Press.
Sudderth, E. B., A. Torralba, W. T. Freeman, and A. S. Willsky. 2008. “Describing
Visual Scenes Using Transformed Objects and Parts.” International Journal of
Computer Vision 77: 291–330.
Zweig, G. 2003. “Bayesian Network Structures and Inference Techniques for
Automatic Speech Recognition.” Computer Speech and Language 17: 173–
193.

17

Combining Multiple Learners

We discussed many diﬀerent learning algorithms in the previous
chapters. Though these are generally successful, no one single algorithm is always the most accurate. Now, we are going to discuss
models composed of multiple learners that complement each other
so that by combining them, we attain higher accuracy.

17.1

base-learner

Rationale
I n a n y ap p l i c a t i o n , we can use one of several learning algorithms,
and with certain algorithms, there are hyperparameters that aﬀect the
ﬁnal learner. For example, in a classiﬁcation setting, we can use a parametric classiﬁer or a multilayer perceptron, and, for example, with a multilayer perceptron, we should also decide on the number of hidden units.
The No Free Lunch Theorem states that there is no single learning algorithm that in any domain always induces the most accurate learner. The
usual approach is to try many and choose the one that performs the best
on a separate validation set.
Each learning algorithm dictates a certain model that comes with a set
of assumptions. This inductive bias leads to error if the assumptions do
not hold for the data. Learning is an ill-posed problem and with ﬁnite
data, each algorithm converges to a diﬀerent solution and fails under
diﬀerent circumstances. The performance of a learner may be ﬁne-tuned
to get the highest possible accuracy on a validation set, but this ﬁnetuning is a complex task and still there are instances on which even the
best learner is not accurate enough. The idea is that there may be another
learner that is accurate on these. By suitably combining multiple baselearners then, accuracy can be improved. Recently with computation and

420

17 Combining Multiple Learners

memory getting cheaper, such systems composed of multiple learners
have become popular (Kuncheva 2004).
There are basically two questions here:
1. How do we generate base-learners that complement each other?
2. How do we combine the outputs of base-learners for maximum accuracy?
Our discussion in this chapter will answer these two related questions.
We will see that model combination is not a trick that always increases
accuracy; model combination does always increase time and space complexity of training and testing, and unless base-learners are trained carefully and their decisions combined smartly, we will only pay for this extra
complexity without any signiﬁcant gain in accuracy.

17.2

diversity

Generating Diverse Learners
Since there is no point in combining learners that always make similar
decisions, the aim is to be able to ﬁnd a set of diverse learners who diﬀer
in their decisions so that they complement each other. At the same time,
there cannot be a gain in overall success unless the learners are accurate,
at least in their domain of expertise. We therefore have this double task
of maximizing individual accuracies and the diversity between learners.
Let us now discuss the diﬀerent ways to achieve this.
Diﬀerent Algorithms
We can use diﬀerent learning algorithms to train diﬀerent base-learners.
Diﬀerent algorithms make diﬀerent assumptions about the data and lead
to diﬀerent classiﬁers. For example, one base-learner may be parametric
and another may be nonparametric. When we decide on a single algorithm, we give emphasis to a single method and ignore all others. Combining multiple learners based on multiple algorithms, we free ourselves
from taking a decision and we no longer put all our eggs in one basket.
Diﬀerent Hyperparameters
We can use the same learning algorithm but use it with diﬀerent hyperparameters. Examples are the number of hidden units in a multilayer

17.2 Generating Diverse Learners

421

perceptron, k in k-nearest neighbor, error threshold in decision trees, the
kernel function in support vector machines, and so forth. With a Gaussian parametric classiﬁer, whether the covariance matrices are shared or
not is a hyperparameter. If the optimization algorithm uses an iterative
procedure such as gradient descent whose ﬁnal state depends on the initial state, such as in backpropagation with multilayer perceptrons, the
initial state, for example, the initial weights, is another hyperparameter.
When we train multiple base-learners with diﬀerent hyperparameter values, we average over this factor and reduce variance, and therefore error.

Diﬀerent Input Representations

sensor fusion

random subspace

Separate base-learners may be using diﬀerent representations of the same
input object or event, making it possible to integrate diﬀerent types of
sensors/measurements/modalities. Diﬀerent representations make different characteristics explicit allowing better identiﬁcation. In many applications, there are multiple sources of information, and it is desirable
to use all of these data to extract more information and achieve higher
accuracy in prediction.
For example, in speech recognition, to recognize the uttered words, in
addition to the acoustic input, we can also use the video image of the
speaker’s lips as the words are spoken. This is similar to sensor fusion
where the data from diﬀerent sensors are integrated to extract more information for a speciﬁc application.
The simplest approach is to concatenate all data vectors and treat it as
one large vector from a single source, but this does not seem theoretically
appropriate since this corresponds to modeling data as sampled from
one multivariate statistical distribution. Moreover, larger input dimensionalities make the systems more complex and require larger samples
for the estimators to be accurate. The approach we take is to make separate predictions based on diﬀerent sources using separate base-learners,
then combine their predictions.
Even if there is a single input representation, by choosing random subsets from it, we can have classiﬁers using diﬀerent input features; this is
called the random subspace method (Ho 1998). This has the eﬀect that
diﬀerent learners will look at the same problem from diﬀerent points of
view and will be robust; it will also help reduce the curse of dimensionality because inputs are fewer dimensional.

422

17 Combining Multiple Learners

Diﬀerent Training Sets
Another possibility is to train diﬀerent base-learners by diﬀerent subsets
of the training set. This can be done randomly by drawing random training sets from the given sample; this is called bagging. Or, the learners can
be trained serially so that instances on which the preceding base-learners
are not accurate are given more emphasis in training later base-learners;
examples are boosting and cascading, which actively try to generate complementary learners, instead of leaving this to chance.
The partitioning of the training sample can also be done based on locality in the input space so that each base-learner is trained on instances
in a certain local part of the input space; this is what is done by the mixture of experts that we discussed in chapter 12 but that we revisit in this
context of combining multiple learners. Similarly, it is possible to deﬁne
the main task in terms of a number of subtasks to be implemented by
the base-learners, as is done by error-correcting output codes.

Diversity vs. Accuracy
One important note is that when we generate multiple base-learners, we
want them to be reasonably accurate but do not require them to be very
accurate individually, so they are not, and need not be, optimized separately for best accuracy. The base-learners are not chosen for their
accuracy, but for their simplicity. We do require, however, that the baselearners be diverse, that is, accurate on diﬀerent instances, specializing
in subdomains of the problem. What we care for is the ﬁnal accuracy
when the base-learners are combined, rather than the accuracies of the
base-learners we started from. Let us say we have a classiﬁer that is 80
percent accurate. When we decide on a second classiﬁer, we do not care
for the overall accuracy; we care only about how accurate it is on the 20
percent that the ﬁrst classiﬁer misclassiﬁes, as long as we know when to
use which one.
This implies that the required accuracy and diversity of the learners
also depend on how their decisions are to be combined, as we will discuss next. If, as in a voting scheme, a learner is consulted for all inputs,
it should be accurate everywhere and diversity should be enforced everywhere; if we have a partioning of the input space into regions of expertise
for diﬀerent learners, diversity is already guaranteed by this partitioning
and learners need to be accurate only in their own local domains.

17.3 Model Combination Schemes

17.3

423

Model Combination Schemes
There are also diﬀerent ways the multiple base-learners are combined to
generate the ﬁnal output:

multiexpert
combination

Multiexpert combination methods have base-learners that work in parallel. These methods can in turn be divided into two:
In the global approach, also called learner fusion, given an input,
all base-learners generate an output and all these outputs are used.
Examples are voting and stacking.
In the local approach, or learner selection, for example, in mixture
of experts, there is a gating model, which looks at the input and
chooses one (or very few) of the learners as responsible for generating the output.

multistage
combination

Multistage combination methods use a serial approach where the next
base-learner is trained with or tested on only the instances where the
previous base-learners are not accurate enough. The idea is that the
base-learners (or the diﬀerent representations they use) are sorted in
increasing complexity so that a complex base-learner is not used (or its
complex representation is not extracted) unless the preceding simpler
base-learners are not conﬁdent. An example is cascading.
Let us say that we have L base-learners. We denote by dj (x) the prediction of base-learner Mj given the arbitrary dimensional input x. In the
case of multiple representations, each Mj uses a diﬀerent input representation xj . The ﬁnal prediction is calculated from the predictions of
the base-learners:

(17.1)

y = f (d1 , d2 , . . . , dL |Φ)
where f (·) is the combining function with Φ denoting its parameters.
When there are K outputs, for each learner there are dji (x), i = 1, . . . , K,
j = 1, . . . , L, and combining them, we also generate K values, yi , i =
1, . . . , K and then for example in classiﬁcation, we choose the class with
the maximum yi value:
K

Choose Ci if yi = max yk
k=1

424

17 Combining Multiple Learners

y
+

w1

f()
wL

w2
d1

dL

d2

x
Figure 17.1 Base-learners are dj and their outputs are combined using f (·).
This is for a single output; in the case of classiﬁcation, each base-learner has K
outputs that are separately used to calculate yi , and then we choose the maximum. Note that here, all learners observe the same input; it may be the case that
diﬀerent learners observe diﬀerent representations of the same input object or
event.

17.4
voting

(17.2)

Voting
The simplest way to combine multiple classiﬁers is by voting, which corresponds to taking a linear combination of the learners (see ﬁgure 17.1):
yi =

wj dji where wj ≥ 0,
j

ensembles
linear opinion
pools

wj = 1
j

This is also known as ensembles and linear opinion pools. In the simplest case, all learners are given equal weight and we have simple voting
that corresponds to taking an average. Still, taking a (weighted) sum is
only one of the possibilities and there are also other combination rules,
as shown in table 17.1 (Kittler et al. 1998). If the outputs are not posterior probabilities, these rules require that outputs be normalized to the
same scale (Jain, Nandakumar, and Ross 2005).

425

17.4 Voting

Table 17.1 Classiﬁer combination rules.

Rule
Sum
Weighted sum
Median
Minimum
Maximum
Product

Fusion function f (·)
L
1
yi = L j=1 dji
yi = j wj dji , wj ≥ 0,
yi = medianj dji
yi = minj dji
yi = maxj dji
yi = j dji

j

wj = 1

Table 17.2 Example of combination rules on three learners and three classes.

d1
d2
d3
Sum
Median
Minimum
Maximum
Product

C1
0.2
0.0
0.4
0.2
0.2
0.0
0.4
0.0

C2
0.5
0.6
0.4
0.5
0.5
0.4
0.6
0.12

C3
0.3
0.4
0.2
0.3
0.4
0.2
0.4
0.032

An example of the use of these rules is shown in table 17.2, which
demonstrates the eﬀects of diﬀerent rules. Sum rule is the most intuitive
and is the most widely used in practice. Median rule is more robust to
outliers; minimum and maximum rules are pessimistic and optimistic, respectively. With the product rule, each learner has veto power; regardless
of the other ones, if one learner has an output of 0, the overall output
goes to 0. Note that after the combination rules, yi do not necessarily
sum up to 1.
In weighted sum, dji is the vote of learner j for class Ci and wj is the
weight of its vote. Simple voting is a special case where all voters have
equal weight, namely, wj = 1/L. In classiﬁcation, this is called plurality
voting where the class having the maximum number of votes is the winner. When there are two classes, this is majority voting where the winning

426

17 Combining Multiple Learners

Bayesian model
combination

(17.3)

class gets more than half of the votes (exercise 1). If the voters can also
supply the additional information of how much they vote for each class
(e.g., by the posterior probability), then after normalization, these can be
used as weights in a weighted voting scheme. Equivalently, if dji are the
class posterior probabilities, P (Ci |x, Mj ), then we can just sum them up
(wj = 1/L) and choose the class with maximum yi .
In the case of regression, simple or weighted averaging or median can
be used to fuse the outputs of base-regressors. Median is more robust to
noise than the average.
Another possibility to ﬁnd wj is to assess the accuracies of the learners
(regressor or classiﬁer) on a separate validation set and use that information to compute the weights, so that we give more weights to more
accurate learners. These weights can also be learned from data, as we
will discuss when we discuss stacked generalization in section 17.9.
Voting schemes can be seen as approximations under a Bayesian framework with weights approximating prior model probabilities, and model
decisions approximating model-conditional likelihoods. This is Bayesian
model combination. For example, in classiﬁcation we have wj ≡ P (Mj ),
dji = P (Ci |x, Mj ), and equation 17.2 corresponds to
P (Ci |x) =

P (Ci |x, Mj )P (Mj )
all models Mj

Simple voting corresponds to a uniform prior. If we have a prior distribution preferring simpler models, this would give larger weights to them.
We cannot integrate over all models; we only choose a subset for which
we believe P (Mj ) is high, or we can have another Bayesian step and calculate P (Mj |X), the probability of a model given the sample, and sample
high probable models from this density.
Hansen and Salamon (1990) have shown that given independent twoclass classiﬁers with success probability higher than 1/2, namely, better
than random guessing, by taking a majority vote, the accuracy increases
as the number of voting classiﬁers increases.
Let us assume that dj are iid with expected value E[dj ] and variance
Var(dj ), then when we take a simple average with wj = 1/L, the expected
value and variance of the output are
⎡
E[y]

=

E⎣
j

⎤
1 ⎦ 1
dj = LE[dj ] = E[dj ]
L
L

427

17.5 Error-Correcting Output Codes

⎛
(17.4)

Var(y)

=

Var ⎝
j

(17.5)

⎞
⎛
1
1 ⎠
dj = 2 Var ⎝
L
L

⎞
dj ⎠ =
j

1
1
LVar(dj ) = Var(dj )
L2
L

We see that the expected value does not change, so the bias does not
change. But variance, and therefore mean square error, decreases as the
number of independent voters, L, increases. In the general case,
⎞
⎡
⎛
⎤
1
1 ⎣
Var(y) = 2 Var ⎝ dj ⎠ = 2
Var(dj ) + 2
Cov(dj , di )⎦
L
L
j
j
j i<j
which implies that if learners are positively correlated, variance (and error) increase. We can thus view using diﬀerent algorithms and input
features as eﬀorts to decrease, if not completely eliminate, the positive
correlation. In section 17.10, we will discuss pruning methods to remove
learners with high positive correlation fron an ensemble.
We also see here that further decrease in variance is possible if the
voters are not independent but negatively correlated. The error then decreases if the accompanying increase in bias is not higher because these
aims are contradictory; we cannot have a number of classiﬁers that are
all accurate and negatively correlated. In mixture of experts for example,
where learners are localized, the experts are negatively correlated but
biased (Jacobs 1997).
If we view each base-learner as a random noise function added to the
true discriminant/regression function and if these noise functions are
uncorrelated with 0 mean, then the averaging of the individual estimates
is like averaging over the noise. In this sense, voting has the eﬀect of
smoothing in the functional space and can be thought of as a regularizer
with a smoothness assumption on the true function (Perrone 1993). We
saw an example of this in ﬁgure 4.5d, where, averaging over models with
large variance, we get a better ﬁt than those of the individual models.
This is the idea in voting: we vote over models with high variance and
low bias so that after combination, the bias remains small and we reduce
the variance by averaging. Even if the individual models are biased, the
decrease in variance may oﬀset this bias and still a decrease in error is
possible.

17.5
error-correcting
output codes

Error-Correcting Output Codes
In error-correcting output codes (ECOC) (Dietterich and Bakiri 1995), the

428

17 Combining Multiple Learners

main classiﬁcation task is deﬁned in terms of a number of subtasks that
are implemented by the base-learners. The idea is that the original task
of separating one class from all other classes may be a diﬃcult problem. Instead, we want to deﬁne a set of simpler classiﬁcation problems,
each specializing in one aspect of the task, and combining these simpler
classiﬁers, we get the ﬁnal classiﬁer.
Base-learners are binary classiﬁers having output −1/ + 1, and there is
a code matrix W of K × L whose K rows are the binary codes of classes
in terms of the L base-learners dj . For example, if the second row of
W is [−1, +1, +1, −1], this means that for us to say an instance belongs
to C2 , the instance should be on the negative side of d1 and d4 , and on
the positive side of d2 and d3 . Similarly, the columns of the code matrix
deﬁnes the task of the base-learners. For example, if the third column
is [−1, +1, +1]T , we understand that the task of the third base-learner,
d3 , is to separate the instances of C1 from the instances of C2 and C3
combined. This is how we form the training set of the base-learners. For
+
example in this case, all instances labeled with C2 and C3 form X3 and
−
+
t
instances labeled with C1 form X3 , and d3 is trained so that x ∈ X3 give
−
output +1 and xt ∈ X3 give output −1.
The code matrix thus allows us to deﬁne a polychotomy (K > 2 classiﬁcation problem) in terms of dichotomies (K = 2 classiﬁcation problem), and it is a method that is applicable using any learning algorithm to
implement the dichotomizer base-learners—for example, linear or multilayer perceptrons (with a single output), decision trees, or SVMs whose
original deﬁnition is for two-class problems.
The typical one discriminant per class setting corresponds to the diagonal code matrix where L = K. For example, for K = 4, we have
⎡
⎢
⎢
W=⎢
⎣

+1
−1
−1
−1

−1
+1
−1
−1

−1
−1
+1
−1

−1
−1
−1
+1

⎤
⎥
⎥
⎥
⎦

The problem here is that if there is an error with one of the baselearners, there may be a misclassiﬁcation because the class code words
are so similar. So the approach in error-correcting codes is to have L > K
and increase the Hamming distance between the code words. One possibility is pairwise separation of classes where there is a separate baselearner to separate Ci from Cj , for i < j (section 10.4). In this case,

17.5 Error-Correcting Output Codes

429

L = K(K − 1)/2 and with K = 4, the code matrix is
⎡
⎤
+1 +1 +1
0
0
0
⎢ −1
0
0 +1 +1
0 ⎥
⎥
⎢
⎥
W=⎢
⎣ 0 −1
0 −1
0 +1 ⎦
0
0 −1
0 −1 −1
where a 0 entry denotes “don’t care.” That is, d1 is trained to separate C1
from C2 and does not use the training instances belonging to the other
classes. Similarly, we say that an instance belongs to C2 if d1 = −1 and
d4 = d5 = +1, and we do not consider the values of d2 , d3 , and d6 . The
problem here is that L is O(K 2 ), and for large K pairwise separation may
not be feasible.
The approach is to set L beforehand and then ﬁnd W such that the
distances between rows, and at the same time the distances between
columns, are as large as possible, in terms of Hamming distance. With
K classes, there are 2(K−1) − 1 possible columns, namely, two-class problems. This is because K bits can be written in 2K diﬀerent ways and
complements (e.g., “0101” and “1010,” from our point of view, deﬁne
the same discriminant) dividing the possible combinations by 2 and then
subtracting 1 because a column of all 0s (or 1s) is useless. For example,
when K = 4, we have
⎡
⎤
−1 −1 −1 −1 −1 −1 −1
⎢ −1 −1 −1 +1 +1 +1 +1 ⎥
⎢
⎥
⎥
W=⎢
⎣ −1 +1 +1 −1 −1 +1 +1 ⎦
+1 −1 +1 −1 +1 −1 +1
When K is large, for a given value of L, we look for L columns out of the
2(K−1) −1. We would like these columns of W to be as diﬀerent as possible
so that the tasks to be learned by the base-learners are as diﬀerent from
each other as possible. At the same time, we would like the rows of W to
be as diﬀerent as possible so that we can have maximum error correction
in case one or more base-learners fail.
ECOC can be written as a voting scheme where the entries of W, wij ,
are considered as vote weights:
L

(17.6)

yi =

wij dj
j=1

and then we choose the class with the highest yi . Taking a weighted sum
and then choosing the maximum instead of checking for an exact match

430

17 Combining Multiple Learners

allows dj to no longer need to be binary but to take a value between −1
and +1, carrying soft certainties instead of hard decisions. Note that a
value pj between 0 and 1, for example, a posterior probability, can be
converted to a value dj between −1 and +1 simply as
dj = 2pj − 1
The diﬀerence between equation 17.6 and the generic voting model of
equation 17.2 is that the weights of votes can be diﬀerent for diﬀerent
classes, namely, we no longer have wj but wij , and also that wj ≥ 0
whereas wij are −1, 0, or +1.
One problem with ECOC is that because the code matrix W is set a priori, there is no guarantee that the subtasks as deﬁned by the columns
of W will be simple. Dietterich and Bakiri (1995) report that the dichotomizer trees may be larger than the polychotomizer trees and when
multilayer perceptrons are used, there may be slower convergence by
backpropagation.

17.6
bagging

unstable algorithm

Bagging
Bagging is a voting method whereby base-learners are made diﬀerent by
training them over slightly diﬀerent training sets. Generating L slightly
diﬀerent samples from a given sample is done by bootstrap, where given
a training set X of size N, we draw N instances randomly from X with
replacement. Because sampling is done with replacement, it is possible
that some instances are drawn more than once and that certain instances
are not drawn at all. When this is done to generate L samples Xj , j =
1, . . . , L, these samples are similar because they are all drawn from the
same original sample, but they are also slightly diﬀerent due to chance.
The base-learners dj are trained with these L samples Xj .
A learning algorithm is an unstable algorithm if small changes in the
training set causes a large diﬀerence in the generated learner, namely, the
learning algorithm has high variance. Bagging, short for bootstrap aggregating, uses bootstrap to generate L training sets, trains L base-learners
using an unstable learning procedure, and then, during testing, takes an
average (Breiman 1996). Bagging can be used both for classiﬁcation and
regression. In the case of regression, to be more robust, one can take the
median instead of the average when combining predictions.
We saw before that averaging reduces variance only if the positive correlation is small; an algorithm is stable if diﬀerent runs of the same al-

17.7 Boosting

431

gorithm on resampled versions of the same dataset lead to learners with
high positive correlation. Algorithms such as decision trees and multilayer perceptrons are unstable. Nearest neighbor is stable, but condensed
nearest neighbor is unstable (Alpaydın 1997). If the original training set
is large, then we may want to generate smaller sets of size N < N from
them using bootstrap, since otherwise the bootstrap replicates Xj will be
too similar, and dj will be highly correlated.

17.7

boosting

weak learner
strong learner

AdaBoost

Boosting
In bagging, generating complementary base-learners is left to chance and
to the unstability of the learning method. In boosting, we actively try
to generate complementary base-learners by training the next learner
on the mistakes of the previous learners. The original boosting algorithm (Schapire 1990) combines three weak learners to generate a strong
learner. A weak learner has error probability less than 1/2, which makes
it better than random guessing on a two-class problem, and a strong
learner has arbitrarily small error probability.
Given a large training set, we randomly divide it into three. We use X1
and train d1 . We then take X2 and feed it to d1 . We take all instances
misclassiﬁed by d1 and also as many instances on which d1 is correct
from X2 , and these together form the training set of d2 . We then take X3
and feed it to d1 and d2 . The instances on which d1 and d2 disagree form
the training set of d3 . During testing, given an instance, we give it to d1
and d2 ; if they agree, that is the response, otherwise the response of d3 is
taken as the output. Schapire (1990) has shown that this overall system
has reduced error rate, and the error rate can arbitrarily be reduced by
using such systems recursively, that is, a boosting system of three models
used as dj in a higher system.
Though it is quite successful, the disadvantage of the original boosting method is that it requires a very large training sample. The sample
should be divided into three and furthermore, the second and third classiﬁers are only trained on a subset on which the previous ones err. So
unless one has a quite large training set, d2 and d3 will not have training
sets of reasonable size. Drucker et al. (1994) use a set of 118,000 instances in boosting multilayer perceptrons for optical handwritten digit
recognition.
Freund and Schapire (1996) proposed a variant, named AdaBoost, short

432

17 Combining Multiple Learners

Training:
t
For all {xt , r t }N ∈ X, initialize p1 = 1/N
t=1
For all base-learners j = 1, . . . , L
t
Randomly draw Xj from X with probabilities pj
Train dj using Xj
t
For each (xt , r t ), calculate yj ← dj (xt )
t
t
Calculate error rate: j ← t pj · 1(yj = r t )
If j > 1/2, then L ← j − 1; stop
βj ← j /(1 − j )
For each (xt , r t ), decrease probabilities if correct:
t
t
t
t
t
If yj = r t , then pj+1 ← βj pj Else pj+1 ← pj
Normalize probabilities:
t
t
t
Zj ← t pj+1 ; pj+1 ← pj+1 /Zj
Testing:
Given x, calculate dj (x), j = 1, . . . , L
Calculate class outputs, i = 1, . . . , K:
yi =

L
j=1

1
log βj dji (x)

Figure 17.2 AdaBoost algorithm.

for adaptive boosting, that uses the same training set over and over and
thus need not be large, but the classiﬁers should be simple so that they
do not overﬁt. AdaBoost can also combine an arbitrary number of baselearners, not three.
Many variants of AdaBoost have been proposed; here, we discuss the
original algorithm AdaBoost.M1 (see ﬁgure 17.2). The idea is to modify
the probabilities of drawing the instances as a function of the error. Let
t
us say pj denotes the probability that the instance pair (xt , r t ) is drawn
t
to train the jth base-learner. Initially, all p1 = 1/N. Then we add new
base-learners as follows, starting from j = 1: j denotes the error rate
of dj . AdaBoost requires that learners are weak, that is, j < 1/2, ∀j; if
not, we stop adding new base-learners. Note that this error rate is not
on the original problem but on the dataset used at step j. We deﬁne
t
t
βj = j /(1 − j ) < 1, and we set pj+1 = βj pj if dj correctly classiﬁes
t
t
t
xt ; otherwise, pj+1 = pj . Because pj+1 should be probabilities, there is a
t
t
normalization where we divide pj+1 by t pj+1 , so that they sum up to 1.
This has the eﬀect that the probability of a correctly classiﬁed instance

17.7 Boosting

margin

433

is decreased, and the probability of a misclassiﬁed instance increases.
Then a new sample of the same size is drawn from the original sample
t
according to these modiﬁed probabilities, pj+1 , with replacement, and is
used to train dj+1 .
This has the eﬀect that dj+1 focuses more on instances misclassiﬁed
by dj ; that is why the base-learners are chosen to be simple and not accurate, since otherwise the next training sample would contain only a few
outlier and noisy instances repeated many times over. For example, with
decision trees, decision stumps, which are trees grown only one or two
levels, are used. So it is clear that these would have bias but the decrease
in variance is larger and the overall error decreases. An algorithm like the
linear discriminant has low variance, and we cannot gain by AdaBoosting
linear discriminants.
Once training is done, AdaBoost is a voting method. Given an instance,
all dj decide and a weighted vote is taken where weights are proportional
to the base-learners’ accuracies (on the training set): wj = log(1/βj ). Freund and Schapire (1996) showed improved accuracy in twenty-two benchmark problems, equal accuracy in one problem, and worse accuracy in
four problems.
Schapire et al. (1998) explain that the success of AdaBoost is due to its
property of increasing the margin. If the margin increases, the training
instances are better separated and an error is less likely. This makes
AdaBoost’s aim similar to that of support vector machines (chapter 13).
In AdaBoost, although diﬀerent base-learners have slightly diﬀerent
training sets, this diﬀerence is not left to chance as in bagging, but is
a function of the error of the previous base-learner. The actual performance of boosting on a particular problem is clearly dependent on the
data and the base-learner. There should be enough training data and the
base-learner should be weak but not too weak, and boosting is especially
susceptible to noise and outliers.
AdaBoost has also been generalized to regression: One straightforward
way, proposed by Avnimelech and Intrator (1997), checks for whether
the prediction error is larger than a certain threshold, and if so marks
it as error, then uses AdaBoost proper. In another version (Drucker
1997), probabilities are modiﬁed based on the magnitude of error, such
that instances where the previous base-learner commits a large error,
have a higher probability of being drawn to train the next base-learner.
Weighted average, or median, is used to combine the predictions of the
base-learners.

434

17 Combining Multiple Learners

y
f()

+
wL

gating

w1
d1

d2

dL

x
Figure 17.3 Mixture of experts is a voting method where the votes, as given
by the gating system, are a function of the input. The combiner system f also
includes this gating system.

17.8
mixture of experts

Mixture of Experts Revisited
In voting, the weights wj are constant over the input space. In the mixture
of experts architecture, which we previously discussed in section 12.8) as
a local method, as an extension of radial basis functions, there is a gating
network whose outputs are weights of the experts. This architecture can
then be viewed as a voting method where the votes depend on the input,
and may be diﬀerent for diﬀerent inputs. The competitive learning algorithm used by the mixture of experts localizes the base-learners such
that each of them becomes an expert in a diﬀerent part of the input space
and have its weight, wj (x), close to 1 in its region of expertise. The ﬁnal
output is a weighted average as in voting
L

(17.7)

y=

wj (x)dj
j=1

except in this case, both the base-learners and the weights are a function
of the input (see ﬁgure 17.3).

17.9 Stacked Generalization

dynamic classifier
selection

17.9
stacked
generalization

(17.8)

435

Jacobs (1997) has shown that in the mixture of experts architecture,
experts are biased but are negatively correlated. As training proceeds,
bias decreases and expert variances increase but at the same time as
experts localize in diﬀerent parts of the input space, their covariances
get more and more negative, which, due to equation 17.5, decreases the
total variance, and thus the error. In section 12.8, we considered the
case where both experts and gating are linear functions but a nonlinear
method, for example, a multilayer perceptron with hidden units, can also
be used for both. This may decrease the expert biases but risks increasing
expert variances and overﬁtting.
In dynamic classiﬁer selection, similar to the gating network of mixture
of experts, there is ﬁrst a system which takes a test input and estimates
the competence of base-classiﬁers in the vicinity of the input. It then
picks the most competent to generate output and that output is given
as the overall output. Woods, Kegelmeyer, and Bowyer (1997) ﬁnd the k
nearest training points of the test input, look at the accuracies of the base
classiﬁers on those, and choose the one that performs the best on them.
Only the selected base-classiﬁer need be evaluated for that test input. To
decrease variance, at the expense of more computation, one can take a
vote over a few competent base-classiﬁers instead of using just a single
one.
Note that in such a scheme, one should make sure that for any region of the input space, there is a competent base-classiﬁer; this implies
that there should be some partitioning of the learning of the input space
among the base-classiﬁers. This is the nice property of mixture of experts, namely, the gating model that does the selection and the expert
base-learners that it selects from are trained in a coupled manner. It
would be straightforward to have a regression version of this dynamic
learner selection algorithm (exercise 5).

Stacked Generalization
Stacked generalization is a technique proposed by Wolpert (1992) that extends voting in that the way the output of the base-learners is combined
need not be linear but is learned through a combiner system, f (·|Φ),
which is another learner, whose parameters Φ are also trained (see ﬁgure 17.4):
y = f (d1 , d2 , . . . , dL |Φ)

436

17 Combining Multiple Learners

y
f()

d1

d2

dL

x
Figure 17.4 In stacked generalization, the combiner is another learner and is
not restricted to being a linear combination as in voting.

The combiner learns what the correct output is when the base-learners
give a certain output combination. We cannot train the combiner function
on the training data because the base-learners may be memorizing the
training set; the combiner system should actually learn how the baselearners make errors. Stacking is a means of estimating and correcting
for the biases of the base-learners. Therefore, the combiner should be
trained on data unused in training the base-learners.
If f (·|w1 , . . . , wL ) is a linear model with constraints, wi ≥ 0, j wj =
1, the optimal weights can be found by constrained regression, but of
course we do not need to enforce this; in stacking, there is no restriction
on the combiner function and unlike voting, f (·) can be nonlinear. For
example, it may be implemented as a multilayer perceptron with Φ its
connection weights. The outputs of the base-learners dj deﬁne a new Ldimensional space in which the output discriminant/regression function
is learned by the combiner function.
In stacked generalization, we would like the base-learners to be as different as possible so that they will complement each other, and, for this,
it is best if they are based on diﬀerent learning algorithms. If we are
combining classiﬁers that can generate continuous outputs, for example,
posterior probabilities, it is better that they be the combined rather than

17.10 Fine-Tuning an Ensemble

437

hard decisions.
When we compare a trained combiner as we have in stacking, with a
ﬁxed rule such as in voting, we see that both have their advantages: a
trained rule is more ﬂexible and may have less bias, but adds extra parameters, risks introducing variance, and needs extra time and data for
training. Note also that there is no need to normalize classiﬁer outputs
before stacking.

17.10

ensemble selection

Fine-Tuning an Ensemble
Model combination is not a magical formula always guaranteed to decrease error; base-learners should be diverse and accurate—that is, they
should provide useful information. If a base-learner does not add to accuracy, it can be discarded; also, of the two base-learners that are highly
correlated, one is not needed. Note that an inaccurate learner can also
worsen accuracy, for example, majority voting assumes more than half
of the classiﬁers to be accurate for an input. Therefore, given a set of
candidate base-learners, it may not be a good idea to use all and we may
do better by choosing a subset. This means that selecting a subset is
good not only for decreasing complexity but can also improve accuracy.
Choosing a subset from an ensemble of base-learners is similar to input feature selection, and the possible approaches for ensemble selection are the same. We can have a forward/incremental/growing approach
where at each iteration, from a set of candidate base-learners, we add to
the ensemble the one that most improves accuracy, we can have a backward/decremental/pruning approach where at each iteration, we remove
the base-learner from the ensemble whose absence leads to highest improvement, or we can have a ﬂoating approach where both additions and
removals are allowed. The combination scheme can be a ﬁxed rule, such
as voting, or it can be a trained stacker. Such a selection scheme would
not include inaccurate learners, ones that are not diverse enough or are
correlated (Caruana et al. 2004; Ruta and Gabrys 2005). Diﬀerent learners
may be using diﬀerent representations, and such an approach also allows
choosing the best complementary representations (Demir and Alpaydın
2005).
Actually, just as in stacking, if we consider the combination as a learner
that takes base-learner outputs as inputs, what we are aiming here is
input dimensionality reduction, which we discussed in chapter 6. Again,

438

17 Combining Multiple Learners

one possibility is feature selection where we discard the uninformative
inputs and keep the useful ones; in ensemble methods, this corresponds
to choosing a subset from an ensemble of base-learners, as we discussed
earlier. Note that if we use a decision tree as the combiner it acts both as
a selector and a combiner (Ula¸ et al. 2009).
s
The second possibility is feature extraction where from the space of
the outputs of base-learners, the aim is to go to a new, lower-dimensional
space where we remove unnecessary inputs and also remove correlations.
Merz (1999) proposes the SCANN algorithm that uses correspondence
analysis—a variant of principal components analysis (section 6.3)—on
the crisp outputs of base classiﬁers and combines them using the nearest mean classiﬁer. Actually, any linear or nonlinear feature extraction
method we discussed in chapter 6 can be used and its (preferrably continuous) output can be fed to any learner. So with L learners and K outputs
each, we map from the K · L-dimensional space to the new space of lower
dimensional, uncorrelated space of these “eigenlearners” where we train
the combiner (using a separate dataset unused to train the base-learners
and the dimensionality reducer).
Rather than drastically discarding or keeping a subset of the ensemble,
this approach uses all the base-learners, and hence all the information,
but does not decrease complexity.

17.11

cascading

(17.9)

Cascading
The idea in cascaded classiﬁers is to have a sequence of base-classiﬁers
dj sorted in terms of their space or time complexity, or the cost of the
representation they use, so that dj+1 is costlier than dj (Kaynak and Alpaydın 2000). Cascading is a multistage method, and we use dj only if all
preceding learners, dk , k < j are not conﬁdent (see ﬁgure 17.5). For this,
associated with each learner is a conﬁdence wj such that we say dj is conﬁdent of its output and can be used if wj > θj where 1/K < θj ≤ θj+1 < 1
is the conﬁdence threshold. In classiﬁcation, the conﬁdence function is
set to the highest posterior: wj ≡ maxi dji ; this is the strategy used for
rejections (section 3.3).
We use learner dj if all the preceding learners are not conﬁdent:
yi = dji if wj > θj and ∀k < j, wk < θk
Starting with j = 1, given a training set, we train dj . Then we ﬁnd all
instances from a separate validation set on which dj is not conﬁdent, and

439

17.11 Cascading

y=d L
y=d 2
yes
y=d 1

w 2>θ 2

no

...

dL

yes
w 1>θ 1

no

d2

d1
x
Figure 17.5 Cascading is a multistage method where there is a sequence of classiﬁers, and the next one is used only when the preceding ones are not conﬁdent.

these constitute the training set of dj+1 . Note that unlike in AdaBoost,
we choose not only the misclassiﬁed instances but the ones for which the
previous base-learner is not conﬁdent. This covers the misclassiﬁcations
as well as the instances for which the posterior is not high enough; these
are instances on the right side of the boundary but for which the distance
to the discriminant, namely, the margin, is not large enough.
The idea is that an early simple classiﬁer handles the majority of instances, and a more complex classiﬁer is used only for a small percentage, thereby not signiﬁcantly increasing the overall complexity. This is
contrary to the multiexpert methods like voting where all base-learners
generate their output for any instance. If the problem space is complex,
a few base-classiﬁers may be cascaded increasing the complexity at each
stage. In order not to increase the number of base-classiﬁers, the few
instances not covered by any are stored as they are and are treated by a

440

17 Combining Multiple Learners

nonparametric classiﬁer, such as k-NN.
The inductive bias of cascading is that the classes can be explained by
a small number of “rules” in increasing complexity, with an additional
small set of “exceptions” not covered by the rules. The rules are implemented by simple base-classiﬁers, for example, perceptrons of increasing
complexity, which learn general rules valid over the whole input space.
Exceptions are localized instances and are best handled by a nonparametric model.
Cascading thus stands between the two extremes of parametric and
nonparametric classiﬁcation. The former—for example, a linear model—
ﬁnds a single rule that should cover all the instances. A nonparametric
classiﬁer—for example, k-NN—stores the whole set of instances without
generating any simple rule explaining them. Cascading generates a rule
(or rules) to explain a large part of the instances as cheaply as possible
and stores the rest as exceptions. This makes sense in a lot of learning
applications. For example, most of the time the past tense of a verb in
English is found by adding a “–d” or “–ed” to the verb; there are also
irregular verbs—for example, “go”/“went”—that do not obey this rule.

17.12

Notes
The idea in combining learners is to divide a complex task into simpler
tasks that are handled by separately trained base-learners. Each baselearner has its own task. If we had a large learner containing all the
base-learners, then it would risk overﬁtting. For example, consider taking a vote over three multilayer perceptrons, each with a single hidden
layer. If we combine them all together with the linear model combining
their outputs, this is a large multilayer perceptron with two hidden layers. If we train this large model with the whole sample, it very probably
overﬁts. When we train the three multilayer perceptrons separately, for
example, using ECOC, bagging, and so forth, it is as if we deﬁne a required output for the second-layer hidden nodes of the large multilayer
perceptron. This puts a constraint on what the overall learner should
learn and simpliﬁes learning.
One disadvantage of combining is that the combined system is not interpretable. For example, even though decision trees are interpretable,
bagged or boosted trees are not interpretable. Error-correcting codes with
their weights as −1/0/ + 1 allow some form of interpretability. Mayoraz

17.12 Notes

biometrics

441

and Moreira (1997) discuss incremental methods for learning the errorcorrecting output codes where base-learners are added when needed.
Allwein, Schapire, and Singer (2000) discuss various methods for coding multiclass problems as two-class problems. Alpaydın and Mayoraz
(1999) consider the application of ECOC where linear base-learners are
combined to get nonlinear discriminants, and they also propose methods
to learn the ECOC matrix from data.
The earliest and most intuitive approach is voting. Kittler et al. (1998)
give a review of ﬁxed rules and also discuss an application where multiple representations are combined. The task is person identiﬁcation using
three representations: frontal face image, face proﬁle image, and voice.
The error rate of the voting model is lower than the error rates when a
single representation is used. Another application is given in Alimo˘lu
g
and Alpaydın 1997 where for improved handwritten digit recognition,
two sources of information are combined: one is the temporal pen movement data as the digit is written on a touch-sensitive pad, and the other
is the static two-dimensional bitmap image once the digit is written. In
that application, the two classiﬁers using either of the two representations have around 5 percent error, but combining the two reduces the
error rate to 3 percent. It is also seen that the critical stage is the design
of the complementary learners and/or representations, the way they are
combined is not as critical.
Combining diﬀerent modalities is used in biometrics, where the aim is
authentication using diﬀerent input sources, ﬁngerprint, signature, face,
and so on. In such a case, diﬀerent classiﬁers use these modalities separately and their decisions are combined. This both improves accuracy
and makes spooﬁng more diﬃcult.
Noble (2004) makes a distinction between three type of combination
strategies when we have information coming from multiple sources in
diﬀerent representations or modalities:
In early integration, all these inputs are concatenated to form a single
vector that is then fed to a single classiﬁer. Previously we discussed
why this is not a very good idea.
In late integration, which we advocated in this chapter, diﬀerent inputs
are fed to separate classiﬁers whose outputs are then combined, by
voting, stacking, or any other method we discussed.
Kernel algorithms, which we discussed in chapter 13, allow a diﬀerent

442

17 Combining Multiple Learners

multiple kernel
learning

method of integration that Noble (2004) calls intermediate integration,
as being between early and late integration. This is the multiple kernel learning approach (see section 13.8) where there is a single kernel
machine classiﬁer that uses multiple kernels for diﬀerent inputs and
the combination is not in the input space as in early integration, or in
the space of decisions as in late integration, but in the space of the
basis functions that deﬁne the kernels. For diﬀerent sources, there
are diﬀerent notions of similarity calculated by their kernels, and the
classiﬁer accumulates and uses them.
It has been shown by Jacobs (1995) that L dependent experts are worth
the same as L independent experts where L ≤ L. Under certain circumstances, voting models and Bayesian techniques will yield identical
results (Jacobs 1995). The priors of equation 17.3 are in turn modeled
as distributions with hyperparameters and in the ideal case, one should
integrate over the whole model-parameter space. This approach is not
generally feasible in practice and one resorts to approximation or sampling. With advances in Bayesian statistics, these supra-Bayesian techniques may become more important in the near future.
Combining multiple learners has been a popular topic in machine learning since the early 1990s, and research has been going on ever since.
Kuncheva (2004) discusses diﬀerent aspects of classiﬁer combination;
the book also includes a section on combination of multiple clustering
results.
AdaBoosted decision trees used to considered to be one of the best machine learning algorithms. There are also versions of AdaBoost where the
next base-learner is trained on the residual of the previous base-learner
(Hastie, Tibshirani, and Friedman 2001). Recently, it has been noticed
that ensembles do not always improve accuracy and research has started
to focus on the criteria that a good ensemble should satisfy or how to
form a good one. A survey of the role of diversity in ensembles is given
in Kuncheva 2005.

17.13

Exercises
1. If each base-learner is iid and correct with probability p > 1/2, what is the
probability that a majority vote over L classiﬁers gives the correct answer?
2. In bagging, to generate the L training sets, what would be the eﬀect of using
L-fold cross-validation instead of bootstrap?

17.14 References

443

3. Propose an incremental algorithm for learning error-correcting output codes
where new two-class problems are added as they are needed to better solve
the multiclass problem.
4. In mixture of experts, we can have diﬀerent experts use diﬀerent input representations. How can we design the gating network in such a case?
5. Propose a dynamic regressor selection algorithm.
6. What is the diﬀerence between voting and stacking using a linear perceptron
as the combiner function?
7. In cascading, why do we require θj+1 ≥ θj ?
8. To be able to use cascading for regression, during test, a regressor should be
able to say if it is conﬁdent of its output. How can we implement this?
9. How can we combine the results of multiple clustering solutions?
10. In section 17.10, we discussed that if we use a decision tree as a combiner
in stacking, it works both as a selector and a combiner. What are the other
advantages and disadvantages?

17.14

References
Alimo˘lu, F., and E. Alpaydın. 1997. “Combining Multiple Representations
g
and Classiﬁers for Pen-Based Handwritten Digit Recognition.” In Fourth International Conference on Document Analysis and Recognition, 637–640. Los
Alamitos, CA: IEEE Computer Society.
Allwein, E. L., R. E. Schapire, and Y. Singer. 2000. “Reducing Multiclass to Binary:
A Unifying Approach for Margin Classiﬁers.” Journal of Machine Learning
Research 1: 113–141.
Alpaydın, E. 1997. “Voting over Multiple Condensed Nearest Neighbors.” Artiﬁcial Intelligence Review 11: 115–132.
Alpaydın, E., and E. Mayoraz. 1999. “Learning Error-Correcting Output Codes
from Data.” In Ninth International Conference on Artiﬁcial Neural Networks,
743–748. London: IEE Press.
Avnimelech, R., and N. Intrator. 1997. “Boosting Regression Estimators.” Neural Computation 11: 499–520.
Breiman, L. 1996. “Bagging Predictors.” Machine Learning 26: 123–140.
Caruana, R., A. Niculescu-Mizil, G. Crew, and A. Ksikes. 2004. “Ensemble Selection from Libraries of Models.” In Twenty-First International Conference on
Machine Learning, ed. C. E. Brodley, 137–144. New York: ACM.
Demir, C., and E. Alpaydın. 2005. “Cost-Conscious Classiﬁer Ensembles.” Pattern Recognition Letters 26: 2206–2214.

444

17 Combining Multiple Learners

Dietterich, T. G., and G. Bakiri. 1995. “Solving Multiclass Learning Problems via
Error-Correcting Output Codes.” Journal of Artiﬁcial Intelligence Research 2:
263–286.
Drucker, H. 1997. “Improving Regressors using Boosting Techniques.” In Fourteenth International Conference on Machine Learning, ed. D. H. Fisher, 107–
115. San Mateo, CA: Morgan Kaufmann.
Drucker, H., C. Cortes, L. D. Jackel, Y. Le Cun, and V. Vapnik. 1994. “Boosting
and Other Ensemble Methods.” Neural Computation 6: 1289–1301.
Freund, Y., and R. E. Schapire. 1996. “Experiments with a New Boosting Algorithm.” In Thirteenth International Conference on Machine Learning, ed. L.
Saitta, 148–156. San Mateo, CA: Morgan Kaufmann.
Hansen, L. K., and P. Salamon. 1990. “Neural Network Ensembles.” IEEE Transactions on Pattern Analysis and Machine Intelligence 12: 993–1001.
Hastie, T., R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical
Learning: Data Mining, Inference, and Prediction. New York: Springer.
Ho, T. K. 1998. “The Random Subspace Method for Constructing Decision
Forests.” IEEE Transactions on Pattern Analysis and Machine Intelligence 20:
832–844.
Jacobs, R. A. 1995. “Methods for Combining Experts’ Probability Assessments.”
Neural Computation 7: 867–888.
Jacobs, R. A. 1997. “Bias/Variance Analyses for Mixtures-of-Experts Architectures.” Neural Computation 9: 369–383.
Jain, A., K. Nandakumar, and A. Ross. 2005. “Score Normalization in Multimodal Biometric Systems.” Pattern Recognition 38: 2270–2285.
Kaynak, C., and E. Alpaydın. 2000. “MultiStage Cascading of Multiple Classiﬁers: One Man’s Noise is Another Man’s Data.” In Seventeenth International
Conference on Machine Learning, ed. P. Langley, 455–462. San Francisco:
Morgan Kaufmann.
Kittler, J., M. Hatef, R. P. W. Duin, and J. Matas. 1998. “On Combining Classiﬁers.” IEEE Transactions on Pattern Analysis and Machine Intelligence 20:
226–239.
Kuncheva, L. I. 2004. Combining Pattern Classiﬁers: Methods and Algorithms.
Hoboken, NJ: Wiley.
Kuncheva, L. I. 2005. Special issue on Diversity in Multiple Classiﬁer Systems.
Information Fusion 6: 1–115.
Mayoraz, E., and M. Moreira. 1997. “On the Decomposition of Polychotomies
into Dichotomies.” In Fourteenth International Conference on Machine Learning, ed. D. H. Fisher, 219–226. San Mateo, CA: Morgan Kaufmann.

17.14 References

445

Merz, C. J. 1999. “Using Correspondence Analysis to Combine Classiﬁers.” Machine Learning 36: 33–58.
Noble, W. S. 2004. “Support Vector Machine Applications in Computational Biology.” In Kernel Methods in Computational Biology, ed. B. Schölkopf, K. Tsuda,
and J.-P. Vert, 71–92. Cambridge, MA: MIT Press.
Perrone, M. P. 1993. “Improving Regression Estimation: Averaging Methods
for Variance Reduction with Extensions to General Convex Measure.” Ph.D.
thesis, Brown University.
Ruta, D., and B. Gabrys. 2005. “Classiﬁer Selection for Majority Voting.” Information Fusion 6: 63–81.
Schapire, R. E. 1990. “The Strength of Weak Learnability.” Machine Learning 5:
197–227.
Schapire, R. E., Y. Freund, P. Bartlett, and W. S. Lee. 1998. “Boosting the Margin: A New Explanation for the Eﬀectiveness of Voting Methods.” Annals of
Statistics 26: 1651–1686.
Ula¸, A., M. Semerci, O. T. Yıldız, and E. Alpaydın. 2009. “Incremental Construcs
tion of Classiﬁer and Discriminant Ensembles.” Information Sciences 179:
1298–1318.
Wolpert, D. H. 1992. “Stacked Generalization.” Neural Networks 5: 241–259.
Woods, K., W. P. Kegelmeyer Jr., and K. Bowyer. 1997. “Combination of Multiple Classiﬁers Using Local Accuracy Estimates.” IEEE Transactions on Pattern
Analysis and Machine Intelligence 19: 405–410.

18

Reinforcement Learning

In reinforcement learning, the learner is a decision-making agent
that takes actions in an environment and receives reward (or penalty)
for its actions in trying to solve a problem. After a set of trial-anderror runs, it should learn the best policy, which is the sequence of
actions that maximize the total reward.

18.1

Introduction
L e t u s s a y we want to build a machine that learns to play chess. In
this case we cannot use a supervised learner for two reasons. First, it is
very costly to have a teacher that will take us through many games and
indicate us the best move for each position. Second, in many cases, there
is no such thing as the best move; the goodness of a move depends on the
moves that follow. A single move does not count; a sequence of moves is
good if after playing them we win the game. The only feedback is at the
end of the game when we win or lose the game.
Another example is a robot that is placed in a maze. The robot can
move in one of the four compass directions and should make a sequence
of movements to reach the exit. As long as the robot is in the maze, there
is no feedback and the robot tries many moves until it reaches the exit
and only then does it get a reward. In this case there is no opponent, but
we can have a preference for shorter trajectories, implying that in this
case we play against time.
These two applications have a number of points in common: there is
a decision maker, called the agent, that is placed in an environment (see
ﬁgure 18.1). In chess, the game-player is the decision maker and the environment is the board; in the second case, the maze is the environment

448

18 Reinforcement Learning

Figure 18.1 The agent interacts with an environment. At any state of the environment, the agent takes an action that changes the state and returns a reward.

critic

credit assignment

of the robot. At any time, the environment is in a certain state that is
one of a set of possible states—for example, the state of the board, the
position of the robot in the maze. The decision maker has a set of actions
possible: legal movement of pieces on the chess board, movement of the
robot in possible directions without hitting the walls, and so forth. Once
an action is chosen and taken, the state changes. The solution to the task
requires a sequence of actions, and we get feedback, in the form of a reward rarely, generally only when the complete sequence is carried out.
The reward deﬁnes the problem and is necessary if we want a learning
agent. The learning agent learns the best sequence of actions to solve a
problem where “best” is quantiﬁed as the sequence of actions that has
the maximum cumulative reward. Such is the setting of reinforcement
learning.
Reinforcement learning is diﬀerent from the learning methods we discussed before in a number of respects. It is called “learning with a critic,”
as opposed to learning with a teacher which we have in supervised learning. A critic diﬀers from a teacher in that it does not tell us what to do
but only how well we have been doing in the past; the critic never informs
in advance. The feedback from the critic is scarce and when it comes, it
comes late. This leads to the credit assignment problem. After taking
several actions and getting the reward, we would like to assess the individual actions we did in the past and ﬁnd the moves that led us to win the
reward so that we can record and recall them later on. As we see shortly,
what a reinforcement learning program does is that it learns to generate
an internal value for the intermediate states or actions in terms of how

18.2 Single State Case: K-Armed Bandit

449

good they are in leading us to the goal and getting us to the real reward.
Once such an internal reward mechanism is learned, the agent can just
take the local actions to maximize it.
The solution to the task requires a sequence of actions, and from this
perspective, we remember the Markov models we discussed in chapter 15.
Indeed, we use a Markov decision process to model the agent. The diﬀerence is that in the case of Markov models, there is an external process that
generates a sequence of signals, for example, speech, which we observe
and model. In the current case, however, it is the agent that generates
the sequence of actions. Previously, we also made a distinction between
observable and hidden Markov models where the states are observed or
hidden (and should be inferred) respectively. Similarly here, sometimes
we have a partially observable Markov decision process in cases where
the agent does not know its state exactly but should infer it with some
uncertainty through observations using sensors. For example, in the case
of a robot moving in a room, the robot may not know its exact position
in the room, nor the exact location of obstacles nor the goal, and should
make decisions through a limited image provided by a camera.

18.2
K-armed bandit

Single State Case: K-Armed Bandit
We start with a simple example. The K-armed bandit is a hypothetical
slot machine with K levers. The action is to choose and pull one of the
levers, and we win a certain amount of money that is the reward associated with the lever (action). The task is to decide which lever to pull to
maximize the reward. This is a classiﬁcation problem where we choose
one of K. If this were supervised learning, then the teacher would tell us
the correct class, namely, the lever leading to maximum earning. In this
case of reinforcement learning, we can only try diﬀerent levers and keep
track of the best. This is a simpliﬁed reinforcement learning problem
because there is only one state, or one slot machine, and we need only
decide on the action. Another reason why this is simpliﬁed is that we
immediately get a reward after a single action; the reward is not delayed,
so we immediately see the value of our action.
Let us say Q(a) is the value of action a. Initially, Q(a) = 0 for all a.
When we try action a, we get reward ra ≥ 0. If rewards are deterministic,
we always get the same ra for any pull of a and in such a case, we can
just set Q(a) = ra . If we want to exploit, once we ﬁnd an action a such

450

18 Reinforcement Learning

that Q(a) > 0, we can keep choosing it and get ra at each pull. However,
it is quite possible that there is another lever with a higher reward, so we
need to explore.
We can choose diﬀerent actions and store Q(a) for all a. Whenever we
want to exploit, we can choose the action with the maximum value, that
is,
(18.1)

choose a∗ if Q(a∗ ) = max Q(a)
a

If rewards are not deterministic but stochastic, we get a diﬀerent reward each time we choose the same action. The amount of the reward is
deﬁned by the probability distribution p(r |a). In such a case, we deﬁne
Qt (a) as the estimate of the value of action a at time t. It is an average of
all rewards received when action a was chosen before time t. An online
update can be deﬁned as
(18.2)

Qt+1 (a) ← Qt (a) + η[rt+1 (a) − Qt (a)]
where rt+1 (a) is the reward received after taking action a at time (t + 1)st
time.
Note that equation 18.2 is the delta rule that we have used on many
occasions in the previous chapters: η is the learning factor (gradually
decreased in time for convergence), rt+1 is the desired output, and Qt (a)
is the current prediction. Qt+1 (a) is the expected value of action a at time
t + 1 and converges to the mean of p(r |a) as t increases.
The full reinforcement learning problem generalizes this simple case in
a number of ways. First, we have several states. This corresponds to having several slot machines with diﬀerent reward probabilities, p(r |si , aj ),
and we need to learn Q(si , aj ), which is the value of taking action aj when
in state si . Second, the actions aﬀect not only the reward but also the next
state, and we move from one state to another. Third, the rewards are delayed and we need to be able to estimate immediate values from delayed
rewards.

18.3

Elements of Reinforcement Learning
The learning decision maker is called the agent. The agent interacts with
the environment that includes everything outside the agent. The agent
has sensors to decide on its state in the environment and takes an action
that modiﬁes its state. When the agent takes an action, the environment

451

18.3 Elements of Reinforcement Learning

Markov decision
process

episode
policy

finite-horizon

(18.3)

provides a reward. Time is discrete as t = 0, 1, 2, . . ., and st ∈ S denotes
the state of the agent at time t where S is the set of all possible states.
at ∈ A(st ) denotes the action that the agent takes at time t where A(st )
is the set of possible actions in state st . When the agent in state st takes
the action at , the clock ticks, reward rt+1 ∈
is received, and the agent
moves to the next state, st+1 . The problem is modeled using a Markov
decision process (MDP). The reward and next state are sampled from their
respective probability distributions, p(rt+1 |st , at ) and P (st+1 |st , at ). Note
that what we have is a Markov system where the state and reward in
the next time step depend only on the current state and action. In some
applications, reward and next state are deterministic, and for a certain
state and action taken, there is one possible reward value and next state.
Depending on the application, a certain state may be designated as the
initial state and in some applications, there is also an absorbing terminal
(goal) state where the search ends; all actions in this terminal state transition to itself with probability 1 and without any reward. The sequence
of actions from the start to the terminal state is an episode, or a trial.
The policy, π , deﬁnes the agent’s behavior and is a mapping from the
states of the environment to actions: π : S → A. The policy deﬁnes the
action to be taken in any state st : at = π (st ). The value of a policy π ,
V π (st ), is the expected cumulative reward that will be received while the
agent follows the policy, starting from state st .
In the ﬁnite-horizon or episodic model, the agent tries to maximize the
expected reward for the next T steps:
⎡
⎤
V π (st ) = E[rt+1 + rt+2 + · · · + rt+T ] = E ⎣

T

rt+i ⎦

i=1

infinite-horizon

(18.4)

Certain tasks are continuing, and there is no prior ﬁxed limit to the
episode. In the inﬁnite-horizon model, there is no sequence limit, but
future rewards are discounted:
⎡
⎤
V π (st ) = E[rt+1 + γrt+2 + γ 2 rt+3 + · · ·] = E ⎣

∞

γ i−1 rt+i ⎦

i=1

discount rate

where 0 ≤ γ < 1 is the discount rate to keep the return ﬁnite. If γ = 0,
then only the immediate reward counts. As γ approaches 1, rewards
further in the future count more, and we say that the agent becomes
more farsighted. γ is less than 1 because there generally is a time limit
to the sequence of actions needed to solve the task. The agent may be a

452

18 Reinforcement Learning

optimal policy

(18.5)

robot that runs on a battery. We prefer rewards sooner rather than later
because we are not certain how long we will survive.
For each policy π , there is a V π (st ), and we want to ﬁnd the optimal
policy π ∗ such that
V ∗ (st ) = max V π (st ), ∀st
π

In some applications, for example, in control, instead of working with
the values of states, V (st ), we prefer to work with the values of stateaction pairs, Q(st , at ). V (st ) denotes how good it is for the agent to be
in state st , whereas Q(st , at ) denotes how good it is to perform action at
when in state st . We deﬁne Q∗ (st , at ) as the value, that is, the expected
cumulative reward, of action at taken in state st and then obeying the
optimal policy afterward. The value of a state is equal to the value of the
best possible action:
V ∗ (st )

=
=

max Q∗ (st , at )
at
⎡
max E ⎣
at

∞

⎤

γ i−1 rt+i ⎦

i=1

⎡
=

max E ⎣rt+1 + γ
at

⎤

∞

γ

i−1

rt+i+1 ⎦

i=1
∗

=
(18.6)

Bellman’s equation

(18.7)

∗

V (st )

max E rt+1 + γV (st+1 )
at
⎛

=

max ⎝E[rt+1 ] + γ
at

⎞

P (st+1 |st , at )V (st+1 )⎠
∗

st +1

To each possible next state st+1 , we move with probability P (st+1 |st , at ),
and continuing from there using the optimal policy, the expected cumulative reward is V ∗ (st+1 ). We sum over all such possible next states, and
we discount it because it is one time step later. Adding our immediate
expected reward, we get the total expected cumulative reward for action
at . We then choose the best of possible actions. Equation 18.6 is known
as Bellman’s equation (Bellman 1957). Similarly, we can also write
Q∗ (st , at ) = E[rt+1 ] + γ

P (st+1 |st , at ) max Q∗ (st+1 , at+1 )
st +1

at +1

Once we have Q∗ (st , at ) values, we can then deﬁne our policy π as
∗
taking the action at , which has the highest value among all Q∗ (st , at ):
(18.8)

∗
∗
π ∗ (st ) : Choose at where Q∗ (st , at ) = max Q∗ (st , at )
at

453

18.4 Model-Based Learning

Initialize V (s) to arbitrary values
Repeat
For all s ∈ S
For all a ∈ A
Q(s, a) ← E[r |s, a] + γ
V (s) ← maxa Q(s, a)
Until V (s) converge

s ∈S P (s

|s, a)V (s )

Figure 18.2 Value iteration algorithm for model-based learning.

This means that if we have the Q∗ (st , at ) values, then by using a greedy
search at each local step we get the optimal sequence of steps that maximizes the cumulative reward.

18.4

Model-Based Learning
We start with model-based learning where we completely know the environment model parameters, p(rt+1 |st , at ) and P (st+1 |st , at ). In such a
case, we do not need any exploration and can directly solve for the optimal value function and policy using dynamic programming. The optimal
value function is unique and is the solution to the simultaneous equations given in equation 18.6. Once we have the optimal value function,
the optimal policy is to choose the action that maximizes the value in the
next state:
⎛
⎞

(18.9)

18.4.1

value iteration

π ∗ (st ) = arg max ⎝E[rt+1 |st , at ] + γ
at

P (st+1 |st , at )V ∗ (st + 1)⎠
st +1 ∈S

Value Iteration
To ﬁnd the optimal policy, we can use the optimal value function, and
there is an iterative algorithm called value iteration that has been shown
to converge to the correct V ∗ values. Its pseudocode is given in ﬁgure 18.2.
We say that the values converged if the maximum value diﬀerence between two iterations is less than a certain threshold δ:
max |V (l+1) (s) − V (l) (s)| < δ
s∈S

454

18 Reinforcement Learning

Initialize a policy π arbitrarily
Repeat
π ←π
Compute the values using π by
solving the linear equations
V π (s) = E[r |s, π (s)] + γ s ∈S P (s |s, π (s))V π (s )
Improve the policy at each state
π (s) ← arg maxa (E[r |s, a] + γ s ∈S P (s |s, a)V π (s ))
Until π = π
Figure 18.3 Policy iteration algorithm for model-based learning.

where l is the iteration counter. Because we care only about the actions
with the maximum value, it is possible that the policy converges to the
optimal one even before the values converge to their optimal values. Each
iteration is O(|S|2 |A|), but frequently there is only a small number k <
|S| of next possible states, so complexity decreases to O(k|S||A|).

18.4.2

Policy Iteration
In policy iteration, we store and update the policy rather than doing this
indirectly over the values. The pseudocode is given in ﬁgure 18.3. The
idea is to start with a policy and improve it repeatedly until there is no
change. The value function can be calculated by solving for the linear
equations. We then check whether we can improve the policy by taking
these into account. This step is guaranteed to improve the policy, and
when no improvement is possible, the policy is guaranteed to be optimal.
Each iteration of this algorithm takes O(|A||S|2 + |S|3 ) time that is more
than that of value iteration, but policy iteration needs fewer iterations
than value iteration.

18.5

Temporal Diﬀerence Learning
Model is deﬁned by the reward and next state probability distributions,
and as we saw in section 18.4, when we know these, we can solve for the
optimal policy using dynamic programming. However, these methods are
costly, and we seldom have such perfect knowledge of the environment.

18.5 Temporal Diﬀerence Learning

temporal
difference

18.5.1

455

The more interesting and realistic application of reinforcement learning
is when we do not have the model. This requires exploration of the environment to query the model. We ﬁrst discuss how this exploration
is done and later see model-free learning algorithms for deterministic
and nondeterministic cases. Though we are not going to assume a full
knowledge of the environment model, we will however require that it be
stationary.
As we will see shortly, when we explore and get to see the value of the
next state and reward, we use this information to update the value of the
current state. These algorithms are called temporal diﬀerence algorithms
because what we do is look at the diﬀerence between our current estimate
of the value of a state (or a state-action pair) and the discounted value of
the next state and the reward received.

Exploration Strategies
To explore, one possibility is to use -greedy search where with probability , we choose one action uniformly randomly among all possible
actions, namely, explore, and with probability 1 − , we choose the best
action, namely, exploit. We do not want to continue exploring indeﬁnitely
but start exploiting once we do enough exploration; for this, we start with
a high value and gradually decrease it. We need to make sure that our
policy is soft, that is, the probability of choosing any action a ∈ A in
state s ∈ S is greater than 0.
We can choose probabilistically, using the softmax function to convert
values to probabilities

(18.10)

P (a|s) =

exp Q(s, a)
b∈A exp Q(s, b)

and then sample according to these probabilities. To gradually move
from exploration to exploitation, we can use a “temperature” variable T
and deﬁne the probability of choosing action a as
(18.11)

P (a|s) =

exp[Q(s, a)/T ]
b∈A exp[Q(s, b)/T ]

When T is large, all probabilities are equal and we have exploration.
When T is small, better actions are favored. So the strategy is to start
with a large T and decrease it gradually, a procedure named annealing,
which in this case moves from exploration to exploitation smoothly in
time.

456

18 Reinforcement Learning

18.5.2

Deterministic Rewards and Actions
In model-free learning, we ﬁrst discuss the simpler deterministic case,
where at any state-action pair, there is a single reward and next state
possible. In this case, equation 18.7 reduces to

(18.12)

Q(st , at ) = rt+1 + γ max Q(st+1 , at+1 )
at +1

and we simply use this as an assignment to update Q(st , at ). When in
state st , we choose action at by one of the stochastic strategies we saw
earlier, which returns a reward rt+1 and takes us to state st+1 . We then
update the value of previous action as
(18.13)

backup

ˆ
ˆ
Q(st , at ) ← rt+1 + γ max Q(st+1 , at+1 )
at +1

ˆ
where the hat denotes that the value is an estimate. Q(st+1 , at+1 ) is a later
value and has a higher chance of being correct. We discount this by γ and
add the immediate reward (if any) and take this as the new estimate for
ˆ
the previous Q(st , at ). This is called a backup because it can be viewed as
taking the estimated value of an action in the next time step and “backing
it up” to revise the estimate for the value of a current action.
ˆ
For now we assume that all Q(s, a) values are stored in a table; we will
see later on how we can store this information more succinctly when |S|
and |A| are large.
ˆ
Initially all Q(st , at ) are 0, and they are updated in time as a result
of trial episodes. Let us say we have a sequence of moves and at each
move, we use equation 18.13 to update the estimate of the Q value of the
previous state-action pair using the Q value of the current state-action
pair. In the intermediate states, all rewards and therefore values are 0,
so no update is done. When we get to the goal state, we get the reward
r and then we can update the Q value of the previous state-action pair
as γr . As for the preceding state-action pair, its immediate reward is 0
and the contribution from the next state-action pair is discounted by γ
because it is one step later. Then in another episode, if we reach this
state, we can update the one preceding that as γ 2 r , and so on. This way,
after many episodes, this information is backed up to earlier state-action
pairs. Q values increase until they reach their optimal values as we ﬁnd
paths with higher cumulative reward, for example, shorter paths, but they
never decrease (see ﬁgure 18.4).
Note that we do not know the reward or next state functions here.
They are part of the environment, and it is as if we query them when

457

18.5 Temporal Diﬀerence Learning

90

A

81
100

*

0
G
B

Figure 18.4 Example to show that Q values increase but never decrease. This
is a deterministic grid-world where G is the goal state with reward 100, all other
immediate rewards are 0, and γ = 0.9. Let us consider the Q value of the transition marked by asterisk, and let us just consider only the two paths A and B. Let
us say that path A is seen before path B, then we have γ max(0, 81) = 72.9;
if afterward B is seen, a shorter path is found and the Q value becomes
γ max(100, 81) = 90. If B is seen before A, the Q value is γ max(100, 0) = 90;
then when A is seen, it does not change because γ max(100, 81) = 90.

we explore. We are not modeling them either, though that is another
possibility. We just accept them as given and learn directly the optimal
policy through the estimated value function.

18.5.3

Nondeterministic Rewards and Actions
If the rewards and the result of actions are not deterministic, then we
have a probability distribution for the reward p(rt+1 |st , at ) from which
rewards are sampled, and there is a probability distribution for the next
state P (st+1 |st , at ). These help us model the uncertainty in the system
that may be due to forces we cannot control in the environment: for
instance, our opponent in chess, the dice in backgammon, or our lack of
knowledge of the system. For example, we may have an imperfect robot
which sometimes fails to go in the intended direction and deviates, or
advances shorter or longer than expected.
In such a case, we have

(18.14)

Q(st , at ) = E[rt+1 ] + γ

P (st+1 |st , at ) max Q(st+1 , at+1 )
st +1

at +1

We cannot do a direct assignment in this case because for the same

458

18 Reinforcement Learning

Initialize all Q(s, a) arbitrarily
For all episodes
Initalize s
Repeat
Choose a using policy derived from Q, e.g., -greedy
Take action a, observe r and s
Update Q(s, a):
Q(s, a) ← Q(s, a) + η(r + γ maxa Q(s , a ) − Q(s, a))
s←s
Until s is terminal state
Figure 18.5 Q learning, which is an oﬀ-policy temporal diﬀerence algorithm.

Q learning

(18.15)

temporal
difference
off-policy
on-policy

Sarsa

state and action, we may receive diﬀerent rewards or move to diﬀerent
next states. What we do is keep a running average. This is known as the
Q learning algorithm:
ˆ
ˆ
ˆ
Q(st , at ) ← Q(st , at ) + η(rt+1 + γ max Q(st+1 , at+1 ) − Q(st , at ))
at +1

ˆ
We think of rt+1 +γ maxat +1 Q(st+1 , at+1 ) values as a sample of instances
ˆ
for each (st , at ) pair and we would like Q(st , at ) to converge to its mean.
As usual η is gradually decreased in time for convergence, and it has been
shown that this algorithm converges to the optimal Q∗ values (Watkins
and Dayan 1992). The pseudocode of the Q learning algorithm is given
in ﬁgure 18.5.
We can also think of equation 18.15 as reducing the diﬀerence between
the current Q value and the backed-up estimate, from one time step later.
Such algorithms are called temporal diﬀerence (TD) algorithms (Sutton
1988).
This is an oﬀ-policy method as the value of the best next action is used
without using the policy. In an on-policy method, the policy is used to
determine also the next action. The on-policy version of Q learning is the
Sarsa algorithm whose pseudocode is given in ﬁgure 18.6. We see that
instead of looking for all possible next actions a and choosing the best,
the on-policy Sarsa uses the policy derived from Q values to choose one
next action a and uses its Q value to calculate the temporal diﬀerence.
On-policy methods estimate the value of a policy while using it to take
actions. In oﬀ-policy methods, these are separated, and the policy used
to generate behavior, called the behavior policy, may in fact be diﬀer-

18.5 Temporal Diﬀerence Learning

459

Initialize all Q(s, a) arbitrarily
For all episodes
Initalize s
Choose a using policy derived from Q, e.g., -greedy
Repeat
Take action a, observe r and s
Choose a using policy derived from Q, e.g., -greedy
Update Q(s, a):
Q(s, a) ← Q(s, a) + η(r + γQ(s , a ) − Q(s, a))
s←s , a←a
Until s is terminal state
Figure 18.6 Sarsa algorithm, which is an on-policy version of Q learning.

TD learning

(18.16)

ent from the policy that is evaluated and improved, called the estimation
policy.
Sarsa converges with probability 1 to the optimal policy and stateaction values if a GLIE policy is employed to choose actions. A GLIE
(greedy in the limit with inﬁnite exploration) policy is where (1) all stateaction pairs are visited an inﬁnite number of times, and (2) the policy
converges in the limit to the greedy policy (which can be arranged, e.g.,
with -greedy policies by setting = 1/t).
The same idea of temporal diﬀerence can also be used to learn V (s)
values, instead of Q(s, a). TD learning (Sutton 1988) uses the following
update rule to update a state value:
V (st ) ← V (st ) + η[rt+1 + γV (st+1 ) − V (st )]
This again is the delta rule where rt+1 + γV (st+1 ) is the better, later
prediction and V (st ) is the current estimate. Their diﬀerence is the temporal diﬀerence, and the update is done to decrease this diﬀerence. The
update factor η is gradually decreased, and TD is guaranteed to converge
to the optimal value function V ∗ (s).

18.5.4

eligibility trace

Eligibility Traces
The previous algorithms are one-step—that is, the temporal diﬀerence is
used to update only the previous value (of the state or state-action pair).
An eligibility trace is a record of the occurrence of past visits that en-

460

18 Reinforcement Learning

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

10

20

30

40

50

60

70

80

90

100

Figure 18.7 Example of an eligibility trace for a value. Visits are marked by an
asterisk.

ables us to implement temporal credit assignment, allowing us to update
the values of previously occurring visits as well. We discuss how this
is done with Sarsa to learn Q values; adapting this to learn V values is
straightforward.
To store the eligibility trace, we require an additional memory variable
associated with each state-action pair, e(s, a), initialized to 0. When the
state-action pair (s, a) is visited, namely, when we take action a in state
s, its eligibility is set to 1; the eligibilities of all other state-action pairs
are multiplied by γλ. 0 ≤ λ ≤ 1 is the trace decay parameter.
(18.17)

et (s, a) =

1
γλet−1 (s, a)

if s = st and a = at ,
otherwise

If a state-action pair has never been visited, its eligibility remains 0; if it
has been, as time passes and other state-actions are visited, its eligibility
decays depending on the value of γ and λ (see ﬁgure 18.7).
We remember that in Sarsa, the temporal error at time t is
(18.18)

δt = rt+1 + γQ(st+1 , at+1 ) − Q(st , at )
In Sarsa with an eligibility trace, named Sarsa(λ), all state-action pairs

461

18.6 Generalization

Initialize all Q(s, a) arbitrarily, e(s, a) ← 0, ∀s, a
For all episodes
Initalize s
Choose a using policy derived from Q, e.g., -greedy
Repeat
Take action a, observe r and s
Choose a using policy derived from Q, e.g., -greedy
δ ← r + γQ(s , a ) − Q(s, a)
e(s, a) ← 1
For all s, a:
Q(s, a) ← Q(s, a) + ηδe(s, a)
e(s, a) ← γλe(s, a)
s←s , a←a
Until s is terminal state

Figure 18.8 Sarsa(λ) algorithm.

are updated as
(18.19)

Sarsa(λ)

18.6

Q(s, a) ← Q(s, a) + ηδt et (s, a), ∀s, a
This updates all eligible state-action pairs, where the update depends
on how far they have occurred in the past. The value of λ deﬁnes the
temporal credit: if λ = 0, only a one-step update is done. The algorithms we discussed in section 18.5.3 are such, and for this reason they
are named Q(0), Sarsa(0), or TD(0). As λ gets closer to 1, more of the previous steps are considered. When λ = 1, all previous steps are updated
and the credit given to them falls only by γ per step. In online updating, all eligible values are updated immediately after each step; in oﬄine
updating, the updates are accumulated and a single update is done at
the end of the episode. Online updating takes more time but converges
faster. The pseudocode for Sarsa(λ) is given in ﬁgure 18.8. Q(λ) and
TD(λ) algorithms can similarly be derived (Sutton and Barto 1998).

Generalization
Until now, we assumed that the Q(s, a) values (or V (s), if we are estimating values of states) are stored in a lookup table, and the algorithms

462

18 Reinforcement Learning

we considered earlier are called tabular algorithms. There are a number of problems with this approach: (1) when the number of states and
the number of actions is large, the size of the table may become quite
large; (2) states and actions may be continuous, for example, turning the
steering wheel by a certain angle, and to use a table, they should be discretized which may cause error; and (3) when the search space is large,
too many episodes may be needed to ﬁll in all the entries of the table
with acceptable accuracy.
Instead of storing the Q values as they are, we can consider this a regression problem. This is a supervised learning problem where we deﬁne
a regressor Q(s, a|θ), taking s and a as inputs and parameterized by a
vector of parameters, θ, to learn Q values. For example, this can be an
artiﬁcial neural network with s and a as its inputs, one output, and θ its
connection weights.
A good function approximator has the usual advantages and solves the
problems discussed previously. A good approximation may be achieved
with a simple model without explicitly storing the training instances; it
can use continuous inputs; and it allows generalization. If we know that
similar (s, a) pairs have similar Q values, we can generalize from past
cases and come up with good Q(s, a) values even if that state-action pair
has never been encountered before.
To be able to train the regressor, we need a training set. In the case
of Sarsa(0), we saw before that we would like Q(st , at ) to get close to
rt+1 + γQ(st+1 , at+1 ). So, we can form a set of training samples where
the input is the state-action pair (st , at ) and the required output is rt+1 +
γQ(st+1 , at+1 ). We can write the squared error as
(18.20)

E t (θ) = [rt+1 + γQ(st+1 , at+1 ) − Q(st , at )]2
Training sets can similarly be deﬁned for Q(0) and TD(0), where in
the latter case we learn V (s), and the required output is rt+1 − γV (st+1 ).
Once such a set is ready, we can use any supervised learning algorithm
for learning the training set.
If we are using a gradient-descent method, as in training neural networks, the parameter vector is updated as

(18.21)

Δθ = η[rt+1 + γQ(st+1 , at+1 ) − Q(st , at )]∇θ t Q(st , at )
This is a one-step update. In the case of Sarsa(λ), the eligibility trace is
also taken into account:

(18.22)

Δθ = ηδt e t

18.6 Generalization

463

where the temporal diﬀerence error is

δt = rt+1 + γQ(st+1 , at+1 ) − Q(st , at )

and the vector of eligibilities of parameters are updated as

(18.23)

e t = γλe t−1 + ∇θ t Q(st , at )
with e 0 all zeros. In the case of a tabular algorithm, the eligibilities are
stored for the state-action pairs because they are the parameters (stored
as a table). In the case of an estimator, eligibility is associated with the
parameters of the estimator. We also note that this is very similar to the
momentum method for stabilizing backpropagation (section 11.8.1). The
diﬀerence is that in the case of momentum previous weight changes are
remembered, whereas here previous gradient vectors are remembered.
Depending on the model used for Q(st , at ), for example, a neural network, we plug its gradient vector in equation 18.23.
In theory, any regression method can be used to train the Q function,
but the particular task has a number of requirements. First, it should allow generalization; that is, we really need to guarantee that similar states
and actions have similar Q values. This also requires a good coding of s
and a, as in any application, to make the similarities apparent. Second,
reinforcement learning updates provide instances one by one and not as
a whole training set, and the learning algorithm should be able to do individual updates to learn the new instance without forgetting what has
been learned before. For example, a multilayer perceptron using backpropagation can be trained with a single instance only if a small learning
rate is used. Or, such instances may be collected to form a training set
and learned altogether but this slows down learning as no learning happens while a suﬃciently large sample is being collected.
Because of these reasons, it seems a good idea to use local learners to
learn the Q values. In such methods, for example, radial basis functions,
information is localized and when a new instance is learned, only a local
part of the learner is updated without possibly corrupting the information in another part. The same requirements apply if we are estimating
the state values as V (st |θ).

464

18 Reinforcement Learning

18.7
18.7.1

partially
observable MDP

value of
information

Partially Observable States
The Setting
In certain applications, the agent does not know the state exactly. It is
equipped with sensors that return an observation, which the agent then
uses to estimate the state. Let us say we have a robot that navigates
in a room. The robot may not know its exact location in the room, or
what else is there in the room. The robot may have a camera with which
sensory observations are recorded. This does not tell the robot its state
exactly but gives some indication as to its likely state. For example, the
robot may only know that there is an obstacle to its right.
The setting is like a Markov decision process, except that after taking an
action at , the new state st+1 is not known, but we have an observation ot+1
that is a stochastic function of st and at : p(ot+1 |st , at ). This is called a
partially observable MDP (POMDP). If ot+1 = st+1 , then POMDP reduces to
the MDP. This is just like the distinction between observable and hidden
Markov models and the solution is similar; that is, from the observation,
we need to infer the state (or rather a probability distribution for the
states) and then act based on this. If the agent believes that it is in state
s1 with probability 0.4 and in state s2 with probability 0.6, then the value
of any action is 0.4 times the value of the action in s1 plus 0.6 times the
value of the action in s2 .
The Markov property does not hold for observations. The next state
observation does not only depend on the current action and observation.
When there is limited observation, two states may appear the same but
are diﬀerent and if these two states require diﬀerent actions, this can
lead to a loss of performance, as measured by the cumulative reward.
The agent should somehow compress the past trajectory into a current
unique state estimate. These past observations can also be taken into
account by taking a past window of observations as input to the policy,
or one can use a recurrent neural network (section 11.12.2) to maintain
the state without forgetting past observations.
At any time, the agent may calculate the most likely state and take an
action accordingly. Or it may take an action to gather information and
reduce uncertainty, for example, search for a landmark, or stop to ask
for direction. This implies the importance of the value of information,
and indeed POMDPs can be modeled as dynamic inﬂuence diagrams (section 16.8). The agent chooses between actions based on the amount of

18.7 Partially Observable States

465

Figure 18.9 In the case of a partially observable environment, the agent has a
state estimator (SE) that keeps an internal belief state b and the policy π generates actions based on the belief states.

belief state

(18.24)

information they provide, the amount of reward they produce, and how
they change the state of the environment.
To keep the process Markov, the agent keeps an internal belief state bt
that summarizes its experience (see ﬁgure 18.9). The agent has a state
estimator that updates the belief state bt+1 based on the last action at ,
current observation ot+1 , and its previous belief state bt . There is a policy π that generates the next action at+1 based on this belief state, as
opposed to the actual state that we had in a completely observable environment. The belief state is a probability distribution over states of the
environment given the initial belief state (before we did any actions) and
the past observation-action history of the agent (without leaving out any
information that could improve agent’s performance). Q learning in such
a case involves the belief state-action pair values, instead of the actual
state-action pairs:
Q(bt , at ) = E[rt+1 ] + γ

P (bt+1 |bt , at )V (bt+1 )
bt +1

18.7.2

Example: The Tiger Problem
We now discuss an example that is a slightly diﬀerent version of the Tiger
problem discussed in Kaelbling, Littman, and Cassandra 1998, modiﬁed
as in the example in Thrun, Burgard, and Fox 2005. Let us say we are

466

18 Reinforcement Learning

standing in front of two doors, one to our left and the other to other
right, leading to two rooms. Behind one of the two doors, we do not
know which, there is a crouching tiger, and behind the other, there is
a treasure. If we open the door of the room where the tiger is, we get
a large negative reward, and if we open the door of the treasure room,
we get some positive reward. The hidden state, zL , is the location of the
tiger. Let us say p denotes the probability that tiger is in the room to the
left and therefore, the tiger is in the room to the right with probability
1 − p:
p ≡ P (zL = 1)
The two actions are aL and aR , which respectively correspond to opening the left or the right door. The rewards are
r (A, Z)
Open left
Open right

Tiger left
−100
+90

Tiger right
+80
−100

We can calculate the expected reward for the two actions. There are no
future rewards because the episode ends once we open one of the doors.
R(aL )

=

r (aL , zL )P (zL ) + r (aL , zR )P (zR ) = −100p + 80(1 − p)

R(aR )

=

r (aR , zL )P (zL ) + r (aR , zR )P (zR ) = 90p − 100(1 − p)

Given these rewards, if p is close to 1, if we believe that there is a high
chance that the tiger is on the left, the right action will be to choose the
right door, and, similarly, for p close to 0, it is better to choose the left
door.
The two intersect for p around 0.5, and there the expected reward is
approximately −10. The fact that the expected reward is negative when
p is around 0.5 (when we have uncertainty) indicates the importance of
collecting information. If we can add sensors to to decrease uncertainty—
that is, move p away from 0.5 to either close to 0 or close to 1—we can
take actions with high positive rewards. That sensing action, aS , may
have a small negative reward: R(aS ) = −1; this may be considered as
the cost of sensing or equivalent to discounting future reward by γ < 1
because we are postponing taking the real action (of opening one of the
doors).

18.7 Partially Observable States

467

In such a case, the expected rewards and value of the best action are
shown in ﬁgure 18.10a:
V = max(aL , aR , aS )
Let us say as sensory input, we use microphones to check whether the
tiger is behind the left or the right door. But we have unreliable sensors
(so that we still stay in the realm of partial observability). Let us say we
can only detect tiger’s presence with 0.7 probability:
P (oL |zL ) = 0.7

P (oL |zR ) = 0.3

P (oR |zL ) = 0.3

P (oR |zR ) = 0.7

If we sense oL , our belief in the tiger’s position changes:
p = P (zL |oL ) =

P (oL |zL )P (zL )
0.7p
=
p(oL )
0.7p + 0.3(1 − p)

The eﬀect of this is shown in ﬁgure 18.10b where we plot R(aL |oL ).
Sensing oL turns opening the right door into a better action for a wider
range. The better sensors we have (if the probability of correct sensing moves from 0.7 closer to 1), the larger this range gets (exercise 9).
Similarly, as we see in ﬁgure 18.10c, if we sense oR , this increases the
chances of opening the left door. Note that sensing also decreases the
range where there is a need to sense (once more).
The expected rewards for the actions in this case are
R(aL |oL )

=

r (aL , zL )P (zL |oL ) + r (aL , zR )P (zR |oL )

=

−100p + 80(1 − p )
0.3 · (1 − p)
0.7 · p
+ 80 ·
−100 ·
p(oL )
p(oL )
r (aR , zL )P (zL |oL ) + r (aR , zR )P (zR |oL )

=
R(aR |oL )

=
=
=

R(aS |oL )

=

90p − 100(1 − p )
0.7 · p
0.3 · (1 − p)
90 ·
− 100 ·
p(oL )
p(oL )
−1

The best action is this case is the maximum of these three. Similarly, if
we sense oR , the expected rewards become
R(aL |oR )

=
=

r (aL , zL )P (zL |oR ) + r (aL , zR )P (zR |oR )
0.3 · p
0.7 · (1 − p)
−100 ·
+ 80 ·
p(oR )
p(oR )

468

Expected reward

Expected reward

Expected reward

18 Reinforcement Learning

(a) Initially
100
0
−100
0

0.5

1

(b) After sensing oL

(c) After sensing oR

100

100

0

0

−100
0

0.5

1

−100
0

0.5
p

1

(d) Optimal after sensing
100
0
−100
0

0.5
p

1

Figure 18.10 Expected rewards and the eﬀect of sensing in the Tiger problem.

R(aR |oR )

=

r (aR , zL )P (zL |oR ) + r (aR , zR )P (zR |oR )
0.3 · p
0.7 · (1 − p)
90 ·
− 100 ·
p(oR )
p(oR )
−1

=
R(aS |oR )

=

To calculate the expected reward, we need to take average over both
sensor readings weighted by their probabilities:
V

=

max R(ai |oj ) P (Oj )
j

=

i

max(R(aL |oL ), R(aR |oL ), R(aS |oL ))P (oL ) +
max(R(aL |oR ), R(aR |oR ), R(aS |oR ))P (oR )

=

max(−70p + 24(1 − p), 63p − 30(1 − p), −0.7p − 0.3(1 − p)) +
max(−30p + 56(1 − p), 27p − 70(1 − p), −0.3p − 0.7(1 − p))

469

18.7 Partially Observable States

⎛
(18.25)

value of
information

=

⎜
⎜
max ⎜
⎝

−100p
−43p
33p
90p

+80(1 − p)
−46(1 − p)
+26(1 − p)
−100(1 − p)

⎞
⎟
⎟
⎟
⎠

Note that when we multiply by P (oL ), it cancels out and we get functions linear in p. These ﬁve lines and the piecewise function that corresponds to their maximum are shown in ﬁgure 18.10d. Note that the line,
−40p − 5(1 − p), as well as the ones involving aS , are beneath others for
all values of p and can safely be pruned. The fact that ﬁgure 18.10d is
better than ﬁgure 18.10a indicates the value of information.
What we calculate here is the value of the best action had we chosen aS .
For example, the ﬁrst line corresponds to choosing aL after aS . So to ﬁnd
the best decision with an episode of length two, we need to back this up
by subtracting −1, which is the reward of aS , and get the expected reward
for the action of sense. Equivalently, we can consider this as waiting that
has an immediate reward of 0 but discounts the future reward by some
γ < 1. We also have the two usual actions of aL and aR and we choose the
best of three; the two immediate actions and the one discounted future
action.
Let us now make the problem more interesting, as in the example of
Thrun, Burgard, and Fox 2005. Let us assume that there is a door between
the two rooms and without us seeing, the tiger can move from one room
to the other. Let us say that this is a restless tiger and it stays in the same
room with probability 0.2 and moves to the other room with probability
0.8. This means that p should also be updated as
p = 0.2p + 0.8(1 − p)
and this updated p should be used in equation 18.25 while choosing the
best action after having chosen aS :
⎛

−100p
⎜
33p
V = max ⎝
90p

⎞
+80(1 − p )
⎟
+26(1 − p ) ⎠
−100(1 − p )

Figure 18.11b corresponds to ﬁgure 18.10d with the updated p . Now,
when planning for episodes of length two, we have the two immediate

470

18 Reinforcement Learning

(a) Tiger can move

(b) Value in two steps

60

100

20

Expected reward

Expected reward

40

0
−20
−40

50

0

−50

−60
−80
0

0.5
p

1

−100
0

0.5
p

1

Figure 18.11 Expected rewards change (a) if the hidden state can change, and
(b) when we consider episodes of length two.

actions of aL and aR , or we wait and sense when p changes and then we
take the action and get its discounted reward (ﬁgure 18.11b):
⎛
⎞
−100p
+80(1 − p)
⎜
⎟
90p −100(1 − p) ⎠
V2 = max ⎝
max V − 1
We see that ﬁgure 18.11b is better than ﬁgure 18.10a; when wrong
actions may lead to large penalty, it is better to defer judgment, look for
extra information, and plan ahead. We can consider longer episodes by
continuing the iterative updating of p and discounting by subtracting 1
and including the two immediate actions to calculate Vt , t > 2.
The algorithm we have just discussed where the value is represented by
piecewise linear functions works only when the number of states, actions,
observations, and the episode length are all ﬁnite. Even in applications
where any of these is not small, or when any is continuous-valued, the
complexity becomes high and we need to resort to approximate algorithms having reasonable complexity. Reviews of such algorithms are
given in Hauskrecht 2000 and Thrun, Burgard, and Fox 2005.

18.8

Notes
More information on reinforcement learning can be found in the textbook
by Sutton and Barto (1998) that discusses all the aspects, learning algorithms, and several applications. A comprehensive tutorial is Kaelbling,

18.8 Notes

learning automata

TD-Gammon

471

Littman, and Moore 1996. Recent work on reinforcement learning applied
to robotics with some impressive applications is given in Thrun, Burgard,
and Fox 2005.
Dynamic programming methods are discussed in Bertsekas 1987 and
in Bertsekas and Tsitsiklis 1996, and TD(λ) and Q-learning can be seen as
stochastic approximations to dynamic programming (Jaakkola, Jordan,
and Singh 1994). Reinforcement learning has two advantages over classical dynamic programming: ﬁrst, as they learn, they can focus on the parts
of the space that are important and ignore the rest; and second, they can
employ function approximation methods to represent knowledge that allows them to generalize and learn faster.
A related ﬁeld is that of learning automata (Narendra and Thathachar
1974), which are ﬁnite state machines that learn by trial and error for
solving problems like the K-armed bandit. The setting we have here is
also the topic of optimal control where there is a controller (agent) taking
actions in a plant (environment) that minimize cost (maximize reward).
The earliest use of temporal diﬀerence method was in Samuel’s checkers program written in 1959 (Sutton and Barto 1998). For every two successive positions in a game, the two board states are evaluated by the
board evaluation function that then causes an update to decrease the difference. There has been much work on games because games are both
easily deﬁned and challenging. A game like chess can easily be simulated:
the allowed moves are formal, and the goal is well deﬁned. Despite the
simplicity of deﬁning the game, expert play is quite diﬃcult.
One of the most impressive application of reinforcement learning is
the TD-Gammon program that learns to play backgammon by playing
against itself (Tesauro 1995). This program is superior to the previous
neurogammon program also developed by Tesauro, which was trained
in a supervised manner based on plays by experts. Backgammon is a
complex task with approximately 1020 states, and there is randomness
due to the roll of dice. Using the TD(λ) algorithm, the program achieves
master level play after playing 1,500,000 games against a copy of itself.
Another interesting application is in job shop scheduling, or ﬁnding
a schedule of tasks satisfying temporal and resource constraints (Zhang
and Dietterich 1996). Some tasks have to be ﬁnished before others can be
started, and two tasks requiring the same resource cannot be done simultaneously. Zhang and Dietterich used reinforcement learning to quickly
ﬁnd schedules that satisfy the constraints and are short. Each state is one
schedule, actions are schedule modiﬁcations, and the program ﬁnds not

472

18 Reinforcement Learning

G

S
Figure 18.12 The grid world. The agent can move in the four compass directions starting from S. The goal state is G.

only one good schedule but a schedule for a class of related scheduling
problems.
Recently hierarchical methods have also been proposed where the problem is decomposed into a set of subproblems. This has the advantage
that policies learned for the subproblems can be shared for multiple
problems, which accelerates learning a new problem (Dietterich 2000).
Each subproblem is simpler and learning them separately is faster. The
disadvantage is that when they are combined, the policy may be suboptimal.
Though reinforcement learning algorithms are slower than supervised
learning algorithms, it is clear that they have a wider variety of application and have the potential to construct better learning machines (Ballard
1997). They do not need any supervision, and this may actually be better
since then they are not biased by the teacher. For example, Tesauro’s
TD-Gammon program in certain circumstances came up with moves that
turned out to be superior to those made by the best players. The ﬁeld of
reinforcement learning is developing rapidly, and we may expect to see
other impressive results in the near future.

18.9

Exercises
1. Given the grid world in ﬁgure 18.12, if the reward on reaching on the goal
is 100 and γ = 0.9, calculate manually Q∗ (s, a), V ∗ (S), and the actions of
optimal policy.
2. With the same conﬁguration given in exercise 1, use Q learning to learn the

473

18.10 References

optimal policy.
3. In exercise 1, how does the optimal policy change if another goal state is
added to the lower-right corner? What happens if a state of reward −100 (a
very bad state) is deﬁned in the lower-right corner?
4. Instead of having γ < 1, we can have γ = 1 but with a negative reward of −c
for all intermediate (nongoal) states. What is the diﬀerence?
5. In exercise 1, assume that the reward on arrival to the goal state is normal
distributed with mean 100 and variance 40. Assume also that the actions are
also stochastic in that when the robot advances in a direction, it moves in the
intended direction with probability 0.5 and there is a 0.25 probability that it
moves in one of the lateral directions. Learn Q(s, a) in this case.
6. Assume we are estimating the value function for states V (s) and that we want
to use TD(λ) algorithm. Derive the tabular value iteration update.
7. Using equation 18.22, derive the weight update equations when a multilayer
perceptron is used to estimate Q.
8. Give an example of a reinforcement learning application that can be modeled
by a POMDP. Deﬁne the states, actions, observations, and reward.
9. In the tiger example, show that as we get a more reliable sensor, the range
where we need to sense once again decreases.
10. Rework the tiger example using the following reward matrix
r (A, Z)
Open left
Open right

18.10

Tiger left
−100
20

Tiger right
+10
−100

References
Ballard, D. H. 1997. An Introduction to Natural Computation. Cambridge, MA:
MIT Press.
Bellman, R. E. 1957. Dynamic Programming. Princeton: Princeton University
Press.
Bertsekas, D. P. 1987. Dynamic Programming: Deterministic and Stochastic
Models. New York: Prentice Hall.
Bertsekas, D. P., and J. N. Tsitsiklis. 1996. Neuro-Dynamic Programming. Belmont, MA: Athena Scientiﬁc.
Dietterich, T. G. 2000. “Hierarchical Reinforcement Learning with the MAXQ
Value Decomposition.” Journal of Artiﬁcial Intelligence Research 13: 227–303.

474

18 Reinforcement Learning

Hauskrecht, M. 2000. “Value-Function Approximations for Partially Observable
Markov Decision Processes.” Journal of Artiﬁcial Intelligence Research 13:
33–94.
Jaakkola, T., M. I. Jordan, and S. P. Singh. 1994. “On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.” Neural Computation 6:
1185–1201.
Kaelbling, L. P., M. L. Littman, and A. R. Cassandra. 1998. “Planning and Acting
in Partially Observable Stochastic Domains.” Artiﬁcial Intelligence 101: 99–
134.
Kaelbling, L. P., M. L. Littman, and A. W. Moore. 1996. “Reinforcement Learning:
A Survey.” Journal of Artiﬁcial Intelligence Research 4: 237–285.
Narendra, K. S., and M. A. L. Thathachar. 1974. “Learning Automata—A Survey.”
IEEE Transactions on Systems, Man, and Cybernetics 4: 323–334.
Sutton, R. S. 1988. “Learning to Predict by the Method of Temporal Diﬀerences.”
Machine Learning 3: 9–44.
Sutton, R. S., and A. G. Barto. 1998. Reinforcement Learning: An Introduction.
Cambridge, MA: MIT Press.
Tesauro, G. 1995. “Temporal Diﬀerence Learning and TD-Gammon.” Communications of the ACM 38(3): 58–68.
Thrun, S., W. Burgard, and D. Fox. 2005. Probabilistic Robotics. Cambridge, MA:
MIT Press.
Watkins, C. J. C. H., and P. Dayan. 1992. “Q-learning.” Machine Learning 8:
279–292.
Zhang, W., and T. G. Dietterich. 1996. “High-Performance Job-Shop Scheduling with a Time-Delay TD(λ) Network.” In Advances in Neural Information
Processing Systems 8, ed. D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo,
1024–1030. Cambridge, MA: The MIT Press.

19

Design and Analysis of Machine
Learning Experiments

We discuss the design of machine learning experiments to assess and
compare the performances of learning algorithms in practice and
the statistical tests to analyze the results of these experiments.

19.1

Introduction
I n p r e v i o us chapters, we discussed several learning algorithms and
saw that, given a certain application, more than one is applicable. Now,
we are concerned with two questions:
1. How can we assess the expected error of a learning algorithm on a
problem? That is, for example, having used a classiﬁcation algorithm
to train a classiﬁer on a dataset drawn from some application, can we
say with enough conﬁdence that later on when it is used in real life, its
expected error rate will be less than, for example, 2 percent?
2. Given two learning algorithms, how can we say one has less error than
the other one, for a given application? The algorithms compared can
be diﬀerent, for example, parametric versus nonparametric, or they
can use diﬀerent hyperparameter settings. For example, given a multilayer perceptron (chapter 11) with four hidden units and another one
with eight hidden units, we would like to be able to say which one has
less expected error. Or with the k-nearest neighbor classiﬁer (chapter 8), we would like to ﬁnd the best value of k.
We cannot look at the training set errors and decide based on those.
The error rate on the training set, by deﬁnition, is always smaller than
the error rate on a test set containing instances unseen during training.

476

19 Design and Analysis of Machine Learning Experiments

expected error

Similarly, training errors cannot be used to compare two algorithms. This
is because over the training set, the more complex model having more
parameters will almost always give fewer errors than the simple one.
So as we have repeatedly discussed, we need a validation set that is different from the training set. Even over a validation set though, just one
run may not be enough. There are two reasons for this: First, the training
and validation sets may be small and may contain exceptional instances,
like noise and outliers, which may mislead us. Second, the learning
method may depend on other random factors aﬀecting generalization.
For example, with a multilayer perceptron trained using backpropagation, because gradient descent converges to the nearest local minimum,
the initial weights aﬀect the ﬁnal weights, and given the exact same architecture and training set, starting from diﬀerent initial weights, there
may be multiple possible ﬁnal classiﬁers having diﬀerent error rates on
the same validation set. We thus would like to have several runs to average over such sources of randomness. If we train and validate only once,
we cannot test for the eﬀect of such factors; this is only admissible if
the learning method is so costly that it can be trained and validated only
once.
We use a learning algorithm on a dataset and generate a learner. If we
do the training once, we have one learner and one validation error. To average over randomness (in training data, initial weights, etc.), we use the
same algorithm and generate multiple learners. We test them on multiple
validation sets and record a sample of validation errors. (Of course, all
the training and validation sets should be drawn from the same application.) We base our evaluation of the learning algorithm on the distribution
of these validation errors. We can use this distribution for assessing the
expected error of the learning algorithm for that problem, or compare it
with the error rate distribution of some other learning algorithm.
Before proceeding to how this is done, it is important to stress a number of points:
1. We should keep in mind that whatever conclusion we draw from our
analysis is conditioned on the dataset we are given. We are not comparing learning algorithms in a domain independent way but on some
particular application. We are not saying anything about the expected
error of a learning algorithm, or comparing one learning algorithm
with another algorithm, in general. Any result we have is only true for
the particular application, and only insofar as that application is rep-

19.1 Introduction

No Free Lunch
Theorem

477

resented in the sample we have. And anyway, as stated by the No Free
Lunch Theorem (Wolpert 1995), there is no such thing as the “best”
learning algorithm. For any learning algorithm, there is a dataset
where it is very accurate and another dataset where it is very poor.
When we say that a learning algorithm is good, we only quantify how
well its inductive bias matches the properties of the data.
2. The division of a given dataset into a number of training and validation
set pairs is only for testing purposes. Once all the tests are complete
and we have made our decision as to the ﬁnal method or hyperparameters, to train the ﬁnal learner, we can use all the labeled data that we
have previously used for training or validation.
3. Because we also use the validation set(s) for testing purposes, for example, for choosing the better of two learning algorithms, or to decide
where to stop learning, it eﬀectively becomes part of the data we use.
When after all such tests, we decide on a particular algorithm and want
to report its expected error, we should use a separate test set for this
purpose, unused during training this ﬁnal system. This data should
have never been used before for training or validation and should be
large for the error estimate to be meaningful. So, given a dataset, we
should ﬁrst leave some part of it aside as the test set and use the rest
for training and validation. Typically, we can leave one-third of the
sample as the test set, then use two-thirds for cross-validation to generate multiple training/validation set pairs, as we will see shortly. So,
the training set is used to optimize the parameters, given a particular
learning algorithm and model structure; the validation set is used to
optimize the hyperparameters of the learning algorithm or the model
structure; and the test set is used at the end, once both these have
been optimized. For example, with an MLP, the training set is used to
optimize the weights, the validation set is used to decide on the number of hidden units, how long to train, the learning rate, and so forth.
Once the best MLP conﬁguration is chosen, its ﬁnal error is calculated
on the test set. With k-NN, the training set is stored as the lookup table; we optimize the distance measure and k on the validation set and
test ﬁnally on the test set.
4. In general, we compare learning algorithms by their error rates, but it
should be kept in mind that in real life, error is only one of the criteria
that aﬀect our decision. Some other criteria are (Turney 2000):

478

19 Design and Analysis of Machine Learning Experiments

risks when errors are generalized using loss functions, instead of
0/1 loss (section 3.3),
training time and space complexity,
testing time and space complexity,
interpretability, namely, whether the method allows knowledge extraction which can be checked and validated by experts, and
easy programmability.

cost-sensitive
learning

The relative importances of these factors change depending on the application. For example, if the training is to be done once in the factory,
then training time and space complexity are not important; if adaptability during use is required, then they do become important. Most
of the learning algorithms use 0/1 loss and take error as the single
criterion to be minimized; recently, cost-sensitive learning variants of
these algorithms have also been proposed to take other cost criteria
into account.
When we train a learner on a dataset using a training set and test its
accuracy on some validation set and try to draw conclusions, what we
are doing is experimentation. Statistics deﬁnes a methodology to design
experiments correctly and analyze the collected data in a manner so as
to be able to extract signiﬁcant conclusions (Montgomery 2005). In this
chapter, we will see how this methodology can be used in the context of
machine learning.

19.2

experiment

Factors, Response, and Strategy of Experimentation
As in other branches of science and engineering, in machine learning too,
we do experiments to get information about the process under scrutiny.
In our case, this is a learner, which, having been trained on a dataset,
generates an output for a given input. An experiment is a test or a series
of tests where we play with the factors that aﬀect the output. These
factors may be the algorithm used, the training set, input features, and
so on, and we observe the changes in the response to be able to extract
information. The aim may be to identify the most important factors,
screen the unimportant ones, or ﬁnd the conﬁguration of the factors that
optimizes the response—for example, classiﬁcation accuracy on a given
test set.

19.2 Factors, Response, and Strategy of Experimentation

479

Figure 19.1 The process generates an output given an input and is aﬀected by
controllable and uncontrollable factors.

Our aim is to plan and conduct machine learning experiments and analyze the data resulting from the experiments, to be able to eliminate
the eﬀect of chance and obtain conclusions which we can consider statistically signiﬁcant. In machine learning, we target a learner having the
highest generalization accuracy and the minimal complexity (so that its
implementation is cheap in time and space) and is robust, that is, minimally aﬀected by external sources of variability.
A trained learner can be shown as in ﬁgure 19.1; it gives an output,
for example, a class code for a test input, and this depends on two type
of factors: The controllable factors, as the name suggests, are those we
have control on. The most basic is the learning algorithm used. There
are also the hyperparameters of the algorithm, for example, the number
of hidden units for a multilayer perceptron, k for k-nearest neighbor,
C for support vector machines, and so on. The dataset used and the
input representation, that is, how the input is coded as a vector, are other
controllable factors.
There are also uncontrollable factors over which we have no control,
adding undesired variability to the process, which we do not want to
aﬀect our decisions. Among these are the noise in the data, the particular
training subset if we are resampling from a large set, randomness in the
optimization process, for example, the initial state in gradient descent
with multilayer perceptrons, and so on.
We use the output to generate the response variable—for example, av-

480

19 Design and Analysis of Machine Learning Experiments

 
¡
¢
¤£
¥
¦
Figure 19.2 Diﬀerent strategies of experimentation with two factors and ﬁve
levels each.

strategies of
experimentation

erage classiﬁcation error on a test set, or the expected risk using a loss
function, or some other measure, such as precision and recall, as we will
discuss shortly.
Given several factors, we need to ﬁnd the best setting for best response,
or in the general case, determine their eﬀect on the response variable. For
example, we may be using principal components analyzer (PCA) to reduce
dimensionality to d before a k-nearest neighbor (k-NN) classiﬁer. The two
factors are d and k, and the question is to decide which combination of d
and k leads to highest performance. Or, we may be using a support vector
machine classiﬁer with Gaussian kernel, and we have the regularization
parameter C and the spread of the Gaussian s 2 to ﬁne-tune together.
There are several strategies of experimentation, as shown in ﬁgure 19.2.
In the best guess approach, we start at some setting of the factors that we
believe is a good conﬁguration. We test the response there and we ﬁddle
with the factors one (or very few) at a time, testing each combination until
we get to a state that we consider is good enough. If the experimenter has
a good intuition of the process, this may work well; but note that there is
no systematic approach to modify the factors and when we stop, we have
no guarantee of ﬁnding the best conﬁguration.
Another strategy is to modify one factor at a time where we decide
on a baseline (default) value for all factors, and then we try diﬀerent
levels for one factor while keeping all other factors at their baseline. The
major disadvantage of this is that it assumes that there is no interaction
between the factors, which may not always be true. In the PCA/k-NN
cascade we discussed earlier, each choice for d deﬁnes a diﬀerent input

19.3 Response Surface Design

factorial design

19.3

response surface
design

481

space for k-NN where a diﬀerent k value may be appropriate.
The correct approach is to use a factorial design where factors are varied together, instead of one at a time; this is colloquially called grid
search. With F factors at L levels each, searching one factor at a time
takes O(L · F ) time, whereas a factorial experiment takes O(LF ) time.

Response Surface Design
To decrease the number of runs necessary, one possibility is to run a fractional factorial design where we run only a subset, another is to try to use
knowledge gathered from previous runs to estimate conﬁgurations that
seem likely to have high response. In searching one factor at a time, if we
can assume that the response is typically quadratic (with a single maximum, assuming we are maximizing a response value, such as the test
accuracy), then instead of trying all values, we can have an iterative procedure where starting from some initial runs, we ﬁt a quadratic, ﬁnd its
maximum analytically, take that as the next estimate, run an experiment
there, add the resulting data to the sample, and then continue ﬁtting and
sampling, until we get no further improvement.
With many factors, this is generalized as the response surface design
method where we try to ﬁt a parametric response function to the factors
as
r = g(f1 , f2 , . . . , fF |φ)
where r is the response and fi , i = 1, . . . , F are the factors. This ﬁtted parametric function deﬁned given the parameters φ is our empirical
model estimating the response for a particular conﬁguration of the (controllable) factors; the eﬀect of uncontrollable factors is modeled as noise.
g(·) is a (typically quadratic) regression model and after a small number
of runs around some baseline (as deﬁned by a so-called design matrix),
one can have enough data to ﬁt g(·) on. Then, we can analytically calculate the values of fi where the ﬁtted g is maximum, which we take as
our next guess, run an experiment there, get a data instance, add it to the
sample, ﬁt g once more, and so on, until there is convergence. Whether
this approach will work well or not depends on whether the response
can indeed be written as a quadratic function of the factors with a single
maximum.

482

19 Design and Analysis of Machine Learning Experiments

19.4

Randomization, Replication, and Blocking
Let us now talk about the three basic principles of experimental design.

randomization

Randomization requires that the order in which the runs are carried
out should be randomly determined so that the results are independent. This is typically a problem in real-world experiments involving
physical objects; for example, machines require some time to warm
up until they operate in their normal range so tests should be done in
random order for time not to bias the results. Ordering generally is
not a problem in software experiments.

replication

Replication implies that for the same conﬁguration of (controllable)
factors, the experiment should be run a number of times to average
over the eﬀect of uncontrollable factors. In machine learning, this is
typically done by running the same algorithm on a number of resampled versions of the same dataset; this is known as cross-validation,
which we will discuss in section 19.6. How the response varies on
these diﬀerent replications of the same experiment allows us to obtain an estimate of the experimental error (the eﬀect of uncontrollable
factors), which we can in turn use to determine how large diﬀerences
should be to be deemed statistically signiﬁcant.

blocking

Blocking is used to reduce or eliminate the variability due to nuisance
factors that inﬂuence the response but in which we are not interested.
For example, defects produced in a factory may also depend on the different batches of raw material, and this eﬀect should be isolated from
the controllable factors in the factory, such as the equipment, personnel, and so on. In machine learning experimentation, when we use resampling and use diﬀerent subsets of the data for diﬀerent replicates,
we need to make sure that for example if we are comparing learning
algorithms, they should all use the same set of resampled subsets,
otherwise the diﬀerences in accuracies would depend not only on the
algorithms but also on the diﬀerent subsets—to be able to measure
the diﬀerence due to algorithms only, the diﬀerent training sets in
replicated runs should be identical; this is what we mean by blocking.
In statistics, if there are two populations, this is called pairing and is
used in paired testing.

pairing

19.5 Guidelines for Machine Learning Experiments

19.5

483

Guidelines for Machine Learning Experiments
Before we start experimentation, we need to have a good idea about what
it is we are studying, how the data is to be collected, and how we are planning to analyze it. The steps in machine learning are the same as for any
type of experimentation (Montgomery 2005). Note that at this point, it is
not important whether the task is classiﬁcation or regression, or whether
it is an unsupervised or a reinforcement learning application. The same
overall discussion applies; the diﬀerence is only in the sampling distribution of the response data that is collected.

A. Aim of the Study
We need to start by stating the problem clearly, deﬁning what the objectives are. In machine learning, there may be several possibilities. As we
discussed before, we may be interested in assessing the expected error
(or some other response measure) of a learning algorithm on a particular
problem and check that, for example, the error is lower than a certain
acceptable level.
Given two learning algorithms and a particular problem as deﬁned by
a dataset, we may want to determine which one has less generalization
error. These can be two diﬀerent algorithms, or one can be a proposed
improvement of the other, for example, by using a better feature extractor.
In the general case, we may have more than two learning algorithms,
and we may want to choose the one with the least error, or order them in
terms of error, for a given dataset.
In an even more general setting, instead of on a single dataset, we may
want to compare two or more algorithms on two or more datasets.

B. Selection of the Response Variable
We need to decide on what we should use as the quality measure. Most
frequently, error is used that is the misclassiﬁcation error for classiﬁcation and mean square error for regression. We may also use some variant;
for example, generalizing from 0/1 to an arbitrary loss, we may use a risk
measure. In information retrieval, we use measures such as precision and
recall; we will discuss such measures in section 19.7. In a cost-sensitive

484

19 Design and Analysis of Machine Learning Experiments

setting, not only the output but also system parameters, for example, its
complexity, are taken into account.
C. Choice of Factors and Levels
What the factors are depend on the aim of the study. If we ﬁx an algorithm and want to ﬁnd the best hyperparameters, then those are the
factors. If we are comparing algorithms, the learning algorithm is a factor. If we have diﬀerent datasets, they also become a factor.
The levels of a factor should be carefully chosen so as not to miss a
good conﬁguration and avoid doing unnecessary experimentation. It is
always good to try to normalize factor levels. For example, in optimizing
k of k-nearest neighbor, one can try values such as 1, 3, 5, and so on,
but in optimizing the spread h of Parzen windows, we should not try
absolute values such as 1.0, 2.0, and so on, because that depends on the
scale of the input; it is better to ﬁnd some statistic that is an indicator
of scale—for example, the average distance between an instance and its
nearest neighbor—and try h as diﬀerent multiples of that statistic.
Though previous expertise is a plus in general, it is also important to
investigate all factors and factor levels that may be of importance and
not be overly inﬂuenced by past experience.
D. Choice of Experimental Design
It is always better to do a factorial design unless we are sure that the
factors do not interact, because mostly they do. Replication number depends on the dataset size; it can be kept small when the dataset is large;
we will discuss this in the next section when we talk about resampling.
However, too few replicates generate few data and this will make comparing distributions diﬃcult; in the particular case of parametric tests,
the assumptions of Gaussianity may not be tenable.
Generally, given some dataset, we leave some part as the test set and
use the rest for training and validation, probably many times by resampling. How this division is done is important. In practice, using small
datasets leads to responses with high variance, and the diﬀerences will
not be signiﬁcant and results will not be conclusive.
It is also important to avoid as much as possible toy, synthetic data
and use datasets that are collected from real-world under real-life circumstances. Didactic one- or two-dimensional datasets may help provide

19.5 Guidelines for Machine Learning Experiments

485

intuition, but the behavior of the algorithms may be completely diﬀerent
in high-dimensional spaces.

E. Performing the Experiment
Before running a large factorial experiment with many factors and levels,
it is best if one does a few trial runs for some random settings to check
that all is as expected. In a large experiment, it is always a good idea to
save intermediate results (or seeds of the random number generator), so
that a part of the whole experiment can be rerun when desired. All the
results should be reproducable. In running a large experiment with many
factors and factor levels, one should be aware of the possible negative
eﬀects of software aging.
It is important that an experimenter be unbiased during experimentation. In comparing one’s favorite algorithm with a competitor, both
should be investigated equally diligently. In large-scale studies, it may
even be envisaged that testers be diﬀerent from developers.
One should avoid the temptation to write one’s own “library” and instead, as much as possible, use code from reliable sources; such code
would have been better tested and optimized.
As in any software development study, the advantages of good documentation cannot be underestimated, especially when working in groups.
All the methods developed for high-quality software engineering should
also be used in machine learning experiments.

F. Statistical Analysis of the Data
This corresponds to analyzing data in a way so that whatever conclusion
we get is not subjective or due to chance. We cast the questions that
we want to answer in a hypothesis testing framework and check whether
the sample supports the hypothesis. For example, the question "Is A a
more accurate algorithm than B?" becomes the hypothesis "Can we say
that the average error of learners trained by A is signiﬁcantly lower than
the average error of learners trained by B?"
As always, visual analysis is helpful, and we can use histograms of error
distributions, whisker-and-box plots, range plots, and so on.

486

19 Design and Analysis of Machine Learning Experiments

G. Conclusions and Recommendations
Once all data is collected and analyzed, we can draw objective conclusions. One frequently encountered conclusion is the need for further
experimentation. Most statistical, and hence machine learning or data
mining, studies are iterative. It is for this reason that we never start with
all the experimentation. It is suggested that no more than 25 percent of
the available resources should be invested in the ﬁrst experiment (Montgomery 2005). The ﬁrst runs are for investigation only. That is also why
it is a good idea not to start with high expectations, or promises to one’s
boss or thesis advisor.
We should always remember that statistical testing never tells us if
the hypothesis is correct or false, but how much the sample seems to
concur with the hypothesis. There is always a risk that we do not have a
conclusive result or that our conclusions be wrong, especially if the data
is small and noisy.
When our expectations are not met, it is most helpful to investigate why
they are not. For example, in checking why our favorite algorithm A has
worked awfully bad on some cases, we can get a splendid idea for some
improved version of A. All improvements are due to the deﬁciencies of
the previous version; ﬁnding a deﬁciency is but a helpful hint that there
is an improvement we can make!
But we should not go to the next step of testing the improved version
before we are sure that we have completely analyzed the current data and
learned all we could learn from it. Ideas are cheap, and useless unless
tested, which is costly.

19.6

cross-validation

Cross-Validation and Resampling Methods
For replication purposes, our ﬁrst need is to get a number of training
and validation set pairs from a dataset X (after having left out some
part as the test set). To get them, if the sample X is large enough, we
can randomly divide it into K parts, then randomly divide each part into
two and use one half for training and the other half for validation. K
is typically 10 or 30. Unfortunately, datasets are never large enough to
do this. So we should do our best with small datasets. This is done
by repeated use of the same data split diﬀerently; this is called crossvalidation. The catch is that this makes the error percentages dependent
as these diﬀerent sets share data.

19.6 Cross-Validation and Resampling Methods

stratification

19.6.1
K-fold
cross-validation

So, given a dataset X, we would like to generate K training/validation
set pairs, {Ti , Vi }K , from this dataset. We would like to keep the traini=1
ing and validation sets as large as possible so that the error estimates
are robust, and at the same time, we would like to keep the overlap between diﬀerent sets as small as possible. We also need to make sure that
classes are represented in the right proportions when subsets of data are
held out, not to disturb the class prior probabilities; this is called stratiﬁcation. If a class has 20 percent examples in the whole dataset, in all
samples drawn from the dataset, it should also have approximately 20
percent examples.

K-Fold Cross-Validation
In K-fold cross-validation, the dataset X is divided randomly into K equalsized parts, Xi , i = 1, . . . , K. To generate each pair, we keep one of the K
parts out as the validation set and combine the remaining K − 1 parts to
form the training set. Doing this K times, each time leaving out another
one of the K parts out, we get K pairs:
V1 = X1
V2 = X2
.
.
.
VK = XK

leave-one-out

487

T1 = X 2 ∪ X 3 ∪ · · · ∪ X K
T2 = X 1 ∪ X 3 ∪ · · · ∪ X K
TK = X1 ∪ X2 ∪ · · · ∪ XK−1

There are two problems with this. First, to keep the training set large,
we allow validation sets that are small. Second, the training sets overlap
considerably, namely, any two training sets share K − 2 parts.
K is typically 10 or 30. As K increases, the percentage of training instances increases and we get more robust estimators, but the validation
set becomes smaller. Furthermore, there is the cost of training the classiﬁer K times, which increases as K is increased. As N increases, K can
be smaller; if N is small, K should be large to allow large enough training
sets. One extreme case of K-fold cross-validation is leave-one-out where
given a dataset of N instances, only one instance is left out as the validation set (instance) and training uses the N − 1 instances. We then get N
separate pairs by leaving out a diﬀerent instance at each iteration. This
is typically used in applications such as medical diagnosis, where labeled
data is hard to ﬁnd. Leave-one-out does not permit stratiﬁcation.
Recently, with computation getting cheaper, it has also become possible to have multiple runs of K-fold cross-validation, for example, 10 × 10-

488

19 Design and Analysis of Machine Learning Experiments

fold, and use average over averages to get more reliable error estimates
(Bouckaert 2003).

19.6.2
5×2
cross-validation

5×2 Cross-Validation
Dietterich (1998) proposed the 5 × 2 cross-validation, which uses training
and validation sets of equal size. We divide the dataset X randomly into
(1)
(2)
two parts, X1 and X1 , which gives our ﬁrst pair of training and vali(1)
(2)
dation sets, T1 = X1 and V1 = X1 . Then we swap the role of the two
(2)
(1)
halves and get the second pair: T2 = X1 and V2 = X1 . This is the ﬁrst
(j)

fold; Xi denotes half j of fold i.
To get the second fold, we shuﬄe X randomly and divide this new fold
(1)
(2)
into two, X2 and X2 . This can be implemented by drawing these from
(1)
(2)
(1)
(2)
X randomly without replacement, namely, X1 ∪ X1 = X2 ∪ X2 = X.
We then swap these two halves to get another pair. We do this for three
more folds and because from each fold, we get two pairs, doing ﬁve folds,
we get ten training and validation sets:
(1)

(2)

T1 = X 1

V1 = X1

T2 =
T3 =

V2 = X1
(2)
V3 = X2

T4 =
.
.
.

(2)
X1
(1)
X2
(2)
X2

(1)

T9 = X 5
(2)
T10 = X5

(1)

(1)

V4 = X2

(2)

V9 = X5
(1)
V10 = X5

Of course, we can do this for more than ﬁve folds and get more training/validation sets, but Dietterich (1998) points out that after ﬁve folds,
the sets share many instances and overlap so much that the statistics
calculated from these sets, namely, validation error rates, become too dependent and do not add new information. Even with ﬁve folds, the sets
overlap and the statistics are dependent, but we can get away with this
until ﬁve folds. On the other hand, if we do have fewer than ﬁve folds,
we get less data (fewer than ten sets) and will not have a large enough
sample to ﬁt a distribution to and test our hypothesis on.

489

19.7 Measuring Classiﬁer Performance

Table 19.1 Confusion matrix for two classes.

True Class
Positive
Negative
Total

19.6.3

bootstrap

Predicted class
Positive
Negative
tp : true positive f n : false negative
f p : false positive tn : true negative
p
n

Total
p
n
N

Bootstrapping
To generate multiple samples from a single sample, an alternative to
cross-validation is the bootstrap that generates new samples by drawing instances from the original sample with replacement. We saw the
use of bootstrapping in section 17.6 to generate training sets for diﬀerent learners in bagging. The bootstrap samples may overlap more than
cross-validation samples and hence their estimates are more dependent;
but is considered the best way to do resampling for very small datasets.
In the bootstrap, we sample N instances from a dataset of size N with
replacement. The original dataset is used as the validation set. The probability that we pick an instance is 1/N; the probability that we do not pick
it is 1 − 1/N. The probability that we do not pick it after N draws is
1−

1
N

N

≈ e−1 = 0.368

This means that the training data contains approximately 63.2 percent
of the instances; that is, the system will not have been trained on 36.8
percent of the data, and the error estimate will be pessimistic. The solution is replication, that is, to repeat the process many times and look at
the average behavior.

19.7

Measuring Classiﬁer Performance
For classiﬁcation, especially for two-class problems, a variety of measures
has been proposed. There are four possible cases, as shown in table 19.1.
For a positive example, if the prediction is also positive, this is a true
positive; if our prediction is negative for a positive example, this is a false
negative. For a negative example, if the prediction is also negative, we

490

19 Design and Analysis of Machine Learning Experiments

Table 19.2 Performance measures used in two-class problems.

Name
error
accuracy
tp-rate
fp-rate
precision
recall
sensitivity
speciﬁcity

receiver operating
characteristics

Formula
(f p + f n)/N
(tp + tn)/N = 1−error
tp/p
f p/n
tp/p
tp/p = tp-rate
tp/p = tp-rate
tn/n = 1− fp-rate

have a true negative, and we have a false positive if we predict a negative
example as positive.
In some two-class problems, we make a distinction between the two
classes and hence the two type of errors, false positives and false negatives. Diﬀerent measures appropriate in diﬀerent settings are given in
table 19.2. Let us envisage an authentication application where, for example, users log on to their accounts by voice. A false positive is wrongly
logging on an impostor and a false negative is refusing a valid user. It is
clear that the two type of errors are not equally bad; the former is much
worse. True positive rate, tp-rate, also known as hit rate, measures what
proportion of valid users we authenticate and false positive rate, fp-rate,
also known as false alarm rate, is the proportion of impostors we wrongly
accept.
ˆ
Let us say the system returns P (C1 |x), the probability of the positive
ˆ
ˆ
class, and for the negative class, we have P (C2 |x) = 1 − P (C1 |x), and we
ˆ
choose “positive” if P (C1 |x) > θ. If θ is close to 1, we hardly choose the
positive class; that is, we will have no false positives but also few true
positives. As we decrease θ to increase the number of true positives, we
risk introducing false positives.
For diﬀerent values of θ, we can get a number of pairs of (tp-rate,
fp-rate) values and by connecting them we get the receiver operating
characteristics (ROC) curve, as shown in ﬁgure 19.3a. Note that diﬀerent values of θ correspond to diﬀerent loss matrices for the two types of
error and the ROC curve can also be seen as the behavior of a classiﬁer

491

19.7 Measuring Classiﬁer Performance
 
¡
¢
£
¤
 
¥

 
¡
¢
£
¤
 
¥

Figure 19.3 (a) Typical ROC curve. Each classiﬁer has a threshold that allows
us to move over this curve, and we decide on a point, based on the relative
importance of hits versus false alarms, namely, true positives and false positives.
The area below the ROC curve is called AUC. (b) A classiﬁer is preferred if its ROC
curve is closer to the upper-left corner (larger AUC). B and C are preferred over
A; B and C are preferred under diﬀerent loss matrices.

Area under the
curve

information
retrieval

under diﬀerent loss matrices (see exercise 1).
Ideally, a classiﬁer has a tp-rate of 1 and a fp-rate of 0, and hence a
classiﬁer is better the more it gets closer to the upper-left corner. On
the diagonal, we make as many true decisions as false ones, and this is
the worst one can do (any classiﬁer that is below the diagonal can be
improved by ﬂipping its decision). Given two classiﬁers, we can say one
is better than the other one if it is above the other one; if two ROC curves
intersect, we can say that the two classiﬁers are better under diﬀerent
loss conditions, as seen in ﬁgure 19.3b.
ROC allows a visual analysis; if we want to reduce the curve to a single
number we can do this by calculating the area under the curve (AUC) . A
classiﬁer ideally has an AUC of 1 and AUC values of diﬀerent classiﬁers
can be compared to give us a general performance averaged over diﬀerent
loss conditions.
In information retrieval, there is a database of records; we make a

492

19 Design and Analysis of Machine Learning Experiments

a
a + b
a
a + c

Figure 19.4 (a) Deﬁnition of precision and recall using Venn diagrams. (b) Precision is 1; all the retrieved records are relevant but there may be relevant ones
not retrieved. (c) Recall is 1; all the relevant records are retrieved but there may
also be irrelevant records that are retrieved.

precision

recall

query, for example, by using some keywords, and a system (basically
a two-class classiﬁer) returns a number of records. In the database, there
are relevant records and for a query, the system may retrieve some of
them (true positives) but probably not all (false negatives); it may also
wrongly retrieve records that are not relevant (false positives). The set of
relevant and retrieved records can be visualized using a Venn diagram, as
shown in ﬁgure 19.4a. Precision is the number of retrieved and relevant
records divided by the total number of retrieved records; if precision is
1, all the retrieved records may be relevant but there may still be records
that are relevant but not retrieved. Recall is the number of retrieved relevant records divided by the total number of relevant records; even if
recall is 1, all the relevant records may be retrieved but there may also
be irrelevant records that are retrieved, as shown in ﬁgure19.4c. As in
the ROC curve, for diﬀerent threshold values, one can draw a curve for
precision vs. recall.

493

19.8 Interval Estimation

sensitivity
specificity

class confusion
matrix

19.8
interval estimation

unit normal
distribution

(19.1)

From another perspective but with the same aim, there are the two
measures of sensitivity and speciﬁcity. Sensitivity is the same as tp-rate
and recall. Speciﬁcity is how well we detect the negatives, which is the
number of true negatives divided by the total number of negatives; this
is equal to 1 minus the false alarm rate. One can also draw a sensitivity
vs. speciﬁcity curve using diﬀerent thresholds.
In the case of K > 2 classes, if we are using 0/1 error, the class confusion matrix is a K × K matrix whose entry (i, j) contains the number of instances that belong to Ci but are assigned to Cj . Ideally, all oﬀ-diagonals
should be 0, for no misclassiﬁcation. The class confusion matrix allows
us to pinpoint what types of misclassiﬁcation occur, namely, if there are
two classes that are frequently confused. Or, one can deﬁne K separate
two-class problems, each one separating one class from the other K − 1.

Interval Estimation
Let us now do a quick review of interval estimation that we will use in hypothesis testing. A point estimator, for example, the maximum likelihood
estimator, speciﬁes a value for a parameter θ. In interval estimation, we
specify an interval within which θ lies with a certain degree of conﬁdence.
To obtain such an interval estimator, we make use of the probability distribution of the point estimator.
For example, let us say we are trying to estimate the mean μ of a normal
density from a sample X = {xt }N . m = t xt /N is the sample average
t=1
and is the point estimator to the mean. m is the sum of normals and
therefore is also normal, m ∼ N (μ, σ 2 /N). We deﬁne the statistic with a
unit normal distribution:
(m − μ)
√
∼Z
σ/ N
We know that 95 percent of Z lies in (−1.96, 1.96), namely, P {−1.96 <
Z < 1.96} = 0.95, and we can write (see ﬁgure 19.5)
P −1.96 <

√

N

(m − μ)
< 1.96 = 0.95
σ

or equivalently
σ
σ
P m − 1.96 √ < μ < m + 1.96 √
N
N

= 0.95

494

19 Design and Analysis of Machine Learning Experiments

Unit Normal Z=N(0,1)
0.4
0.35
0.3

p(x)

0.25
0.2
0.15
0.1
2.5%

2.5%

0.05
0
−5

0
x

5

Figure 19.5 95 percent of the unit normal distribution lies between −1.96 and
1.96.

two-sided
confidence
interval

√
That is “with 95 percent conﬁdence,” μ will lie within 1.96σ / N units
of the sample average. This is a two-sided conﬁdence interval. With 99
√
√
percent conﬁdence, μ will lie in (m − 2.58σ / N, m + 2.58σ / N); that
is, if we want more conﬁdence, the interval gets larger. The interval gets
smaller as N, the sample size, increases.
This can be generalized for any required conﬁdence as follows. Let us
denote zα such that
P {Z > zα } = α, 0 < α < 1
Because Z is symmetric around the mean, z1−α/2 = −zα/2 , and P {X <
−zα/2 } = P {X > zα/2 } = α/2. Hence for any speciﬁed level of conﬁdence
1 − α, we have
P −zα/2 < Z < zα/2 = 1 − α
and
P −zα/2 <

√

N

(m − μ)
< zα/2
σ

= 1−α

495

19.8 Interval Estimation

or
(19.2)

σ
σ
P m − zα/2 √ < μ < m + zα/2 √
N
N

= 1−α

Hence a 100(1 − α) percent two-sided conﬁdence interval for μ can be
computed for any α.
Similarly, knowing that P {Z < 1.64} = 0.95, we have (see ﬁgure 19.6)
P

√

N

(m − μ)
< 1.64 = 0.95
σ

or

one-sided
confidence
interval

(19.3)

σ
P m − 1.64 √ < μ = 0.95
N
√
and (m − 1.64σ / N, ∞) is a 95 percent one-sided upper conﬁdence interval for μ, which deﬁnes a lower bound. Generalizing, a 100(1 − α)
percent one-sided conﬁdence interval for μ can be computed from
σ
P m − zα √ < μ
N

=1−α

Similarly, the one-sided lower conﬁdence interval that deﬁnes an upper
bound can also be calculated.
In the previous intervals, we used σ ; that is, we assumed that the variance is known. If it is not, one can plug the sample variance
S2 =

(xt − m)2 /(N − 1)
t

(19.4)

t distribution

instead of σ 2 . We know that when xt ∼ N (μ, σ 2 ), (N − 1)S 2 /σ 2 is chisquare with N − 1 degrees of freedom. We also know that m and S 2 are
√
independent. Then, N(m − μ)/S is t-distributed with N − 1 degrees of
freedom (section A.3.7), denoted as
√
N(m − μ)
∼ tN−1
S
Hence for any α ∈ (0, 1/2), we can deﬁne an interval, using the values
speciﬁed by the t distribution, instead of the unit normal Z
P t1−α/2,N−1 <

√ (m − μ)
< tα/2,N−1
N
S

=1−α

or using t1−α/2,N−1 = −tα/2,N−1 , we can write
S
S
P m − tα/2,N−1 √ < μ < m + tα/2,N−1 √
N
N

= 1−α

496

19 Design and Analysis of Machine Learning Experiments

Unit Normal Z=N(0,1)
0.4
0.35
0.3

p(x)

0.25
0.2
0.15
0.1
5%
0.05
0
−5

0
x

5

Figure 19.6 95 percent of the unit normal distribution lies before 1.64.

Similarly, one-sided conﬁdence intervals can be deﬁned. The t distribution has larger spread (longer tails) than the unit normal distribution,
and generally the interval given by the t is larger; this should be expected
since additional uncertainty exists due to the unknown variance.

19.9

hypothesis testing

Hypothesis Testing
Instead of explicitly estimating some parameters, in certain applications
we may want to use the sample to test some particular hypothesis concerning the parameters. For example, instead of estimating the mean,
we may want to test whether the mean is less than 0.02. If the random
sample is consistent with the hypothesis under consideration, we “fail to
reject” the hypothesis; otherwise, we say that it is “rejected.” But when
we make such a decision, we are not really saying that it is true or false
but rather that the sample data appears to be consistent with it to a given
degree of conﬁdence or not.
In hypothesis testing, the approach is as follows. We deﬁne a statistic

497

19.9 Hypothesis Testing

Table 19.3 Type I error, type II error, and power of a test.

Truth
True
False

null hypothesis

Decision
Fail to reject
Reject
Correct
Type I error
Type II error Correct (power)

that obeys a certain distribution if the hypothesis is correct. If the statistic calculated from the sample has very low probability of being drawn
from this distribution, then we reject the hypothesis; otherwise, we fail
to reject it.
Let us say we have a sample from a normal distribution with unknown
mean μ and known variance σ 2 , and we want to test a speciﬁc hypothesis
about μ, for example, whether it is equal to a speciﬁed constant μ0 . It is
denoted as H0 and is called the null hypothesis
H0 : μ = μ0
against the alternative hypothesis
H1 : μ = μ0

level of
significance

(19.5)

two-sided test
type I error

type II error

(19.6)

m is the point estimate of μ, and it is reasonable to reject H0 if m is too
far from μ0 . This is where the interval estimate is used. We fail to reject
the hypothesis with level of signiﬁcance α if μ0 lies in the 100(1 − α)
percent conﬁdence interval, namely, if
√
N(m − μ0 )
∈ (−zα/2 , zα/2 )
σ
We reject the null hypothesis if it falls outside, on either side. This is a
two-sided test.
If we reject when the hypothesis is correct, this is a type I error and
thus α, set before the test, deﬁnes how much type I error we can tolerate,
typical values being α = 0.1, 0.05, 0.01 (see table 19.3). A type II error is
if we fail to reject the null hypothesis when the true mean μ is unequal
to μ0 . The probability that H0 is not rejected when the true mean is μ is
a function of μ and is given as
β(μ) = Pμ −zα/2 ≤

m − μ0
√ ≤ zα/2
σ/ N

498

19 Design and Analysis of Machine Learning Experiments

power function

one-sided test

1 − β(μ) is called the power function of the test and is equal to the
probability of rejection when μ is the true value. Type II error probability
increases as μ and μ0 gets closer, and we can calculate how large a sample
we need for us to be able to detect a diﬀerence δ = |μ −μ0 | with suﬃcient
power.
One can also have a one-sided test of the form
H0 : μ ≤ μ0 vs H1 : μ > μ0

(19.7)

(19.8)

(19.9)
t test

19.10

as opposed to the two-sided test when the alternative hypothesis is μ =
μ0 . The one-sided test with α level of signiﬁcance deﬁnes the 100(1 − α)
conﬁdence interval bounded on one side in which m should lie for the
hypothesis not to be rejected. We fail to reject if
√
N
(m − μ0 ) ∈ (−∞, zα )
σ
and reject outside. Note that the null hypothesis H0 also allows equality,
which means that we get ordering information only if the test rejects.
This tells us which of the two one-sided tests we should use. Whatever
claim we have should be in H1 so that rejection of the test would support
our claim.
If the variance is unknown, just as we did in the interval estimates, we
use the sample variance instead of the population variance and the fact
that
√
N(m − μ0 )
∼ tN−1
S
For example, for H0 : μ = μ0 vs H1 : μ = μ0 , we fail to reject at signiﬁcance level α if
√
N(m − μ0 )
∈ (−tα/2,N−1 , tα/2,N−1 )
S
which is known as the two-sided t test. A one-sided t test can be deﬁned
similarly.

Assessing a Classiﬁcation Algorithm’s Performance
Now that we have reviewed hypothesis testing, we are ready to see how
it is used in testing error rates. We will discuss the case of classiﬁcation error, but the same methodology applies for squared error in regression, log likelihoods in unsupervised learning, expected reward in

19.10 Assessing a Classiﬁcation Algorithm’s Performance

499

reinforcement learning, and so on, as long as we can write the appropriate parametric form for the sampling distribution. We will also discuss
nonparametric tests when no such parametric form can be found.
We now start with error rate assessment, and, in the next section, we
discuss error rate comparison.

19.10.1

Binomial Test
Let us start with the case where we have a single training set T and a
single validation set V . We train our classiﬁer on T and test it on V . We
denote by p the probability that the classiﬁer makes a misclassiﬁcation
error. We do not know p; it is what we would like to estimate or test a
hypothesis about. On the instance with index t from the validation set
V , let us say xt denotes the correctness of the classiﬁer’s decision: xt is
a 0/1 Bernoulli random variable that takes the value 1 when the classiﬁer commits an error and 0 when the classiﬁer is correct. The binomial
random variable X denotes the total number of errors:
N

xt

X=
t=1

We would like to test whether the error probability p is less than or
equal to some value p0 we specify:
H0 : p ≤ p0 vs. H1 : p > p0
If the probability of error is p, the probability that the classiﬁer commits j errors out of N is
N
j

P {X = j} =

binomial test

It is reasonable to reject p ≤ p0 if in such a case, the probability that
we see X = e errors or more is very unlikely. That is, the binomial test
rejects the hypothesis if
N

(19.10)

pj (1 − p)N−j

P {X ≥ e} =
x=e

N
x

p0 x (1 − p0 )N−x < α

where α is the signiﬁcance, for example, 0.05.

500

19 Design and Analysis of Machine Learning Experiments

19.10.2

Approximate Normal Test
ˆ
If p is the probability of error, our point estimate is p = X/N. Then, it is
ˆ
reasonable to reject the null hypothesis if p is much larger than p0 . How
ˆ
large is large enough is given by the sampling distribution of p and the
signiﬁcance α.
Because X is the sum of independent random variables from the same
distribution, the central limit theorem states that for large N, X/N is
approximately normal with mean p0 and variance p0 (1 − p0 ). Then

(19.11)

approximate
normal test

19.10.3

X/N − p0
∼Z
˙
p0 (1 − p0 )
where ∼ denotes “approximately distributed.” Then, using equation 19.7,
˙
the approximate normal test rejects the null hypothesis if this value for
X = e is greater than zα . z0.05 is 1.64. This approximation will work well
as long as N is not too small and p is not very close to 0 or 1; as a rule of
thumb, we require Np ≥ 5 and N(1 − p) ≥ 5.

t Test
The two tests we discussed earlier use a single validation set. If we run
the algorithm K times, on K training/validation set pairs, we get K error
percentages, pi , i = 1, . . . , K on the K validation sets. Let xt be 1 if the
i
classiﬁer trained on Ti makes a misclassiﬁcation error on instance t of
Vi ; xt is 0 otherwise. Then
i
pi =

N
t
t=1 xi

N

Given that
m=

(19.12)

K
i=1 pi

K

, S2 =

K
i=1 (pi

− m)2
K−1

from equation 19.8, we know that we have
√
K(m − p0 )
∼ tK−1
S
and the t test rejects the null hypothesis that the classiﬁcation algorithm
has p0 or less error percentage at signiﬁcance level α if this value is
greater than tα,K−1 . Typically, K is taken as 10 or 30. t0.05,9 = 1.83 and
t0.05,29 = 1.70.

19.11 Comparing Two Classiﬁcation Algorithms

19.11

501

Comparing Two Classiﬁcation Algorithms
Given two learning algorithms, we want to compare and test whether they
construct classiﬁers that have the same expected error rate.

19.11.1

contingency table

McNemar’s Test
Given a training set and a validation set, we use two algorithms to train
two classiﬁers on the training set and test them on the validation set
and compute their errors. A contingency table, like the one shown here,
is an array of natural numbers in matrix form representing counts, or
frequencies:
e00 : Number of examples
misclassiﬁed by both
e10 : Number of examples
misclassiﬁed by 2 but not 1

e01 : Number of examples
misclassiﬁed by 1 but not 2
e11 : Number of examples
correctly classiﬁed by both

Under the null hypothesis that the classiﬁcation algorithms have the
same error rate, we expect e01 = e10 and these to be equal to (e01 +e10 )/2.
We have the chi-square statistic with one degree of freedom
(19.13)
McNemar’s test

19.11.2

paired test

(|e01 − e10 | − 1)2
2
∼ X1
e01 + e10
and McNemar’s test rejects the hypothesis that the two classiﬁcation algorithms have the same error rate at signiﬁcance level α if this value is
2
2
greater than Xα,1 . For α = 0.05, X0.05,1 = 3.84.

K-Fold Cross-Validated Paired t Test
This set uses K-fold cross-validation to get K training/validation set pairs.
We use the two classiﬁcation algorithms to train on the training sets
Ti , i = 1, . . . , K, and test on the validation sets Vi . The error percentages
1
2
of the classiﬁers on the validation sets are recorded as pi and pi .
If the two classiﬁcation algorithms have the same error rate, then we
expect them to have the same mean, or equivalently, that the diﬀerence of
1
2
their means is 0. The diﬀerence in error rates on fold i is pi = pi −pi . This
is a paired test; that is, for each i, both algorithms see the same training
and validation sets. When this is done K times, we have a distribution
1
2
of pi containing K points. Given that pi and pi are both (approximately)

502

19 Design and Analysis of Machine Learning Experiments

normal, their diﬀerence pi is also normal. The null hypothesis is that this
distribution has 0 mean:
H0 : μ = 0 vs. H1 : μ = 0
We deﬁne
m=

(19.14)
K-fold cv paired t
test

K
i=1 pi

K

, S2 =

K
i=1 (pi

− m)2
K−1

Under the null hypothesis that μ = 0, we have a statistic that is tdistributed with K − 1 degrees of freedom:
√
√
K(m − 0)
K·m
=
∼ tK−1
S
S
Thus the K-fold cv paired t test rejects the hypothesis that two classiﬁcation algorithms have the same error rate at signiﬁcance level α if
this value is outside the interval (−tα/2,K−1 , tα/2,K−1 ). t0.025,9 = 2.26 and
t0.025,29 = 2.05.
If we want to test whether the ﬁrst algorithm has less error than the
second, we need a one-sided hypothesis and use a one-tailed test:
H0 : μ ≥ 0 vs. H1 : μ < 0
If the test rejects, our claim that the ﬁrst one has signiﬁcantly less error
is supported.

19.11.3

5 × 2 cv Paired t Test
In the 5 × 2 cv t test, proposed by Dietterich (1998), we perform ﬁve
replications of twofold cross-validation. In each replication, the dataset is
(j)
divided into two equal-sized sets. pi is the diﬀerence between the error
rates of the two classiﬁers on fold j = 1, 2 of replication i = 1, . . . , 5. The
(1)
(2)
average on replication i is pi = (pi + pi )/2, and the estimated variance
(1)
(2)
is si2 = (pi − pi )2 + (pi − pi )2 .
Under the null hypothesis that the two classiﬁcation algorithms have
(j)
the same error rate, pi is the diﬀerence of two identically distributed
proportions, and ignoring the fact that these proportions are not inde(j)
pendent, pi can be treated as approximately normal distributed with
(j)

0 mean and unknown variance σ 2 . Then pi /σ is approximately unit
(1)
(2)
normal. If we assume pi and pi are independent normals (which is
not strictly true because their training and test sets are not drawn independently of each other), then si2 /σ 2 has a chi-square distribution with

503

19.11 Comparing Two Classiﬁcation Algorithms

one degree of freedom. If each of the si2 are assumed to be independent
(which is not true because they are all computed from the same set of
available data), then their sum is chi-square with ﬁve degrees of freedom:
5
2
i=1 si
2
σ

M=

2
∼ X5

and
(1)

(19.15)
5 × 2 cv paired t test

19.11.4

t=

(1)

p1 /σ
=
M/5

p1

5
2
i=1 si /5

∼ t5

giving us a t statistic with ﬁve degrees of freedom. The 5 × 2 cv paired t
test rejects the hypothesis that the two classiﬁcation algorithms have the
same error rate at signiﬁcance level α if this value is outside the interval
(−tα/2,5 , tα/2,5 ). t0.025,5 = 2.57.

5 × 2 cv Paired F Test
(1)

We note that the numerator in equation 19.15, p1 , is arbitrary; actually,
(j)

ten diﬀerent values can be placed in the numerator, namely, pi , j =
1, 2, i = 1, . . . , 5, leading to ten possible statistics:
(19.16)

(j)

ti

(j)

pi

=

5
2
i=1 si /5

Alpaydın (1999) proposed an extension to the 5 × 2 cv t test that
(j)
combines the results of the ten possible statistics. If pi /σ ∼ Z, then
(j) 2

pi
dom:

2
/σ 2 ∼ X1 and their sum is chi-square with ten degrees of free-

5
i=1

2
j=1

(j) 2

pi

2
∼ X10
σ2
Placing this in the numerator of equation 19.15, we get a statistic that
is the ratio of two chi-square distributed random variables. Two such
variables divided by their respective degrees of freedom is F -distributed
with ten and ﬁve degrees of freedom (section A.3.8):

N=

(19.17)
5 × 2 cv paired F
test

N/10
=
f =
M/5

5
i=1

2

(j) 2
2
j=1 pi
5
2
i=1 si

∼ F10,5

5 × 2 cv paired F test rejects the hypothesis that the classiﬁcation algorithms have the same error rate at signiﬁcance level α if this value is
greater than Fα,10,5 . F0.05,10,5 = 4.74.

504

19 Design and Analysis of Machine Learning Experiments

19.12

analysis of
variance

Comparing Multiple Algorithms: Analysis of Variance
In many cases, we have more than two algorithms, and we would like
to compare their expected error. Given L algorithms, we train them on K
training sets, induce K classiﬁers with each algorithm, and then test them
on K validation sets and record their error rates. This gives us L groups
of K values. The problem then is the comparison of these L samples
for statistically signiﬁcant diﬀerence. This is an experiment with a single
factor with L levels, the learning algorithms, and there are K replications
for each level.
In analysis of variance (ANOVA), we consider L independent samples,
each of size K, composed of normal random variables of unknown mean
μj and unknown common variance σ 2 :
Xij ∼ N (μj , σ 2 ), j = 1, . . . , L, i = 1, . . . , K,
We are interested in testing the hypothesis H0 that all means are equal:
H0 : μ1 = μ2 = · · · = μL vs. H1 : μr = μs , for at least one pair (r , s)
The comparison of error rates of multiple classiﬁcation algorithms ﬁts
this scheme. We have L classiﬁcation algorithms, and we have their error
rates on K validation folds. Xij is the number of validation errors made
by the classiﬁer, which is trained by classiﬁcation algorithm j on fold i.
Each Xij is binomial and approximately normal. If H0 is not rejected, we
fail to ﬁnd a signiﬁcant error diﬀerence among the error rates of the L
classiﬁcation algorithms. This is therefore a generalization of the tests
we saw in section 19.11 that compared the error rates of two classiﬁcation algorithms. The L classiﬁcation algorithms may be diﬀerent or may
use diﬀerent hyperparameters, for example, number of hidden units in a
multilayer perceptron, number of neighbors in k-nn, and so forth.
The approach in ANOVA is to derive two estimators of σ 2 . One estimator is designed such that it is true only when H0 is true, and the second is
always a valid estimator, regardless of whether H0 is true or not. ANOVA
then rejects H0 , namely, that the L samples are drawn from the same
population, if the two estimators diﬀer signiﬁcantly.
Our ﬁrst estimator to σ 2 is valid only if the hypothesis is true, namely,
μj = μ, j = 1, . . . , L. If Xij ∼ N (μ, σ 2 ), then the group average
K

mj =

Xij
K
i=1

19.12 Comparing Multiple Algorithms: Analysis of Variance

505

is also normal with mean μ and variance σ 2 /K. If the hypothesis is true,
then mj , j = 1, . . . , L are L instances drawn from N (μ, σ 2 /K). Then their
mean and variance are
L
j=1 mj

m=

L

S2 =

,

j (mj

− m)2

L−1

Thus an estimator of σ is K · S 2 , namely,
2

L

(19.18)

(mj − m)2
L−1
j=1

ˆ2
σb = K

Each of mj is normal and (L − 1)S 2 /(σ 2 /K) is chi-square with (L − 1)
degrees of freedom. Then, we have
(19.19)
j

(mj − m)2
2
∼ XL−1
σ 2 /K
We deﬁne SSb , the between-group sum of squares, as
(mj − m)2

SSb ≡ K
j

So, when H0 is true, we have
(19.20)

SSb
2
∼ XL−1
σ2
2
Our second estimator of σ 2 is the average of group variances, Sj , deﬁned as
K
i=1 (Xij

− mj )2
K−1

2
Sj =

and their average is
2
Sj

L

(19.21)

ˆ2
σw =
j=1

L

=
j

i

(Xij − mj )2
L(K − 1)

We deﬁne SSw , the within-group sum of squares:
(Xij − mj )2

SSw ≡
j

i

Remembering that for a normal sample, we have
(K − 1)

2
Sj

σ2

2
∼ XK−1

506

19 Design and Analysis of Machine Learning Experiments

and that the sum of chi-squares is also a chi-square, we have
2
Sj

L

(K − 1)

σ2
j=1

2
∼ XL(K−1)

So
(19.22)

SSw
2
∼ XL(K−1)
σ2
Then we have the task of comparing two variances for equality, which
we can do by checking whether their ratio is close to 1. The ratio of
two independent chi-square random variables divided by their respective
degrees of freedom is a random variable that is F -distributed, and hence
when H0 is true, we have

(19.23)

F0 =

SSb /σ 2
L−1

SSw /σ 2
L(K − 1)

=

ˆ
σ2
SSb /(L − 1)
= b ∼ FL−1,L(K−1)
SSw /(L(K − 1))
ˆ2
σw

For any given signiﬁcance value α, the hypothesis that the L classiﬁcation algorithms have the same expected error rate is rejected if this
statistic is greater than Fα,L−1,L(K−1) .
Note that we are rejecting if the two estimators disagree signiﬁcantly.
If H0 is not true, then the variance of mj around m will be larger than
what we would normally have if H0 were true, and hence if H0 is not true,
ˆ2
the ﬁrst estimator σb will overestimate σ 2 , and the ratio will be greater
than 1. For α = 0.05, L = 5 and K = 10, F0.05,4,45 = 2.6. If Xij vary around
m with a variance of σ 2 , then if H0 is true, mj vary around m by σ 2 /K.
If it seems as if they vary more, then H0 should be rejected because the
displacement of mj around m is more than what can be explained by
some constant added noise.
The name analysis of variance is derived from a partitioning of the total
variability in the data into its components.
(19.24)

(Xij − m)2

SST ≡
j

i

SST divided by its degree of freedom, namely, K · L − 1 (there are K ·
L data points, and we lose one degree of freedom because m is ﬁxed),
gives us the sample variance of Xij . It can be shown that (exercise 5) the
total sum of squares can be split into between-group sum of squares and
within-group sum of squares
(19.25)

SST = SSb + SSw

507

19.12 Comparing Multiple Algorithms: Analysis of Variance

Table 19.4 The analysis of variance (ANOVA) table for a single factor model.

Source of
variation
Between
groups

Sum of
squares
SSb ≡
K j (mj − m)2

Within
groups

SSw ≡
2
j
i (Xij − mj )

Total

SST ≡
j

posthoc testing

least square
difference test

multiple
comparisons

i (Xij

− m)2

Degrees of
freedom
L−1

L(K − 1)

Mean
square
MSb =

MSw =

SSb
L−1

F0
MSb
MSw

SSw
L(K−1)

L·K −1

Results of ANOVA are reported in an ANOVA table as shown in table 19.4. This is the basic one-way analysis of variance where there is a
single factor, for example, learning algorithm. We may consider experiments with multiple factors, for example, we can have one factor for classiﬁcation algorithms and another factor for feature extraction algorithms
used before, and this will be a two-factor experiment with interaction.
If the hypothesis is rejected, we only know that there is some diﬀerence
between the L groups but we do not know where. For this, we do posthoc
testing, that is, an additional set of tests involving subsets of groups, for
example, pairs.
Fisher’s least square diﬀerence test (LSD) compares groups in a pairwise
2
manner. For each group, we have mi ∼ N (μi , σw = MSw /K) and mi −
2
mj ∼ N (μi − μj , 2σw ). Then, under the null hypothesis that H0 : μi = μj ,
we have
mi − mj
t= √
∼ tL(K−1)
2σw
We reject H0 in favor of the alternative hypothesis H1 : μ1 = μ2 if
|t| > tα/2,L(K−1) . Similarly, one-sided tests can be deﬁned to ﬁnd pairwise
orderings.
When we do a number of tests to draw one conclusion, this is called
multiple comparisons, and we need to keep in mind that if T hypotheses
are to be tested, each at signiﬁcance level α, then the probability that at
least one hypothesis is incorrectly rejected is at most T α. For example,

508

19 Design and Analysis of Machine Learning Experiments

Bonferroni
correction

19.13

nonparametric
tests

the probability that six conﬁdence intervals, each calculated at 95 percent
individual conﬁdence intervals, will simultaneously be correct is at least
70 percent. Thus to ensure that the overall conﬁdence interval is at least
100(1 − α), each conﬁdence interval should be set at 100(1 − α/T ). This
is called a Bonferroni correction.
Sometimes it may be the case that ANOVA rejects and none of the
posthoc pairwise tests ﬁnd a signiﬁcant diﬀerence. In such a case, our
conclusion is that there is a diﬀerence between the means but that we
need more data to be able to pinpoint the source of the diﬀerence.
Note that the main cost is the training and testing of L classiﬁcation
algorithms on K training/validation sets. Once this is done and the values
are stored in a K ×L table, calculating the ANOVA or pairwise comparison
test statistics from those is very cheap in comparison.

Comparison over Multiple Datasets
Let us say we want to compare two or more algorithms on several datasets
and not one. What makes this diﬀerent is that an algorithm depending on
how well its inductive bias matches the problem will behave diﬀerently
on diﬀerent datasets, and these error values on diﬀerent datasets cannot
be said to be normally distributed around some mean accuracy. This implies that the parametric tests that we discussed in the previous sections
based on binomials being approximately normal are no longer applicable
and we need to resort to nonparametric tests. The advantage of having
such tests is that we can also use them for comparing other statistics that
are not normal, for example, training times, number of free parameters,
and so on.
Parametric tests are generally robust to slight departures from normality, especially if the sample is large. Nonparametric tests are distribution
free but are less eﬃcient; that is, if both are applicable, a parametric test
should be preferred. The corresponding nonparametric test will require a
larger sample to achieve the same power. Nonparametric tests assume no
knowledge about the distribution of the underlying population but only
that the values can be compared or ordered, and, as we will see, such
tests make use of this order information.
When we have an algorithm trained on a number of diﬀerent datasets,
the average of its errors on these datasets is not a meaningful value, and,
for example, we cannot use such averages to compare two algorithms A

19.13 Comparison over Multiple Datasets

509

and B. To compare two algorithms, the only piece of information we can
use is if on any dataset, A is more accurate than B; we can then count the
number of times A is more accurate than B and check whether this could
have been by chance if they indeed were equally accurate. With more than
two algorithms, we will look at the average ranks of the learners trained
by diﬀerent algorithms. Nonparametric tests basically use this rank data
and not the absolute values.
Before proceeding with the details of these tests, it should be stressed
that it does not make sense to compare error rates of algorithms on a
whole variety of applications. Because there is no such thing as the “best
learning algorithm,” such tests would not be conclusive. However, we can
compare algorithms on a number of datasets, or versions, of the same application. For example, we may have a number of diﬀerent datasets for
face recognition but with diﬀerent properties (resolution, lighting, number of subjects, and so on), and we may use a nonparametric test to
compare algorithms on those; diﬀerent properties of the datasets would
make it impossible for us to lump images from diﬀerent datasets together in a single set, but we can train algorithms separately on diﬀerent
datasets, obtain ranks separately, and then combine these to get an overall decision.

19.13.1

sign test

Comparing Two Algorithms
Let us say we want to compare two algorithms. We both train and validate
them on i = 1, . . . , N diﬀerent datasets in a paired manner—that is, all the
conditions except the diﬀerent algorithms should be identical. We get
1
2
results ei and ei and if we use K-fold cross-validation on each dataset,
these are averages or medians of the K values. The sign test is based on
the idea that if the two algorithms have equal error, on each dataset, there
should be 1/2 probability that the ﬁrst has less error than the second, and
thus we expect the ﬁrst to win on N/2 datasets. Let us deﬁne
Xi =

1
0

1
2
if ei < ei
otherwise

N

and X =

Let us say we want to test
H0 : μ1 ≥ μ2 vs. H1 : μ1 < μ2

Xi
i=1

510

19 Design and Analysis of Machine Learning Experiments

If the null hypothesis is correct, X is binomial in N trials with p = 1/2.
Let us say that we saw that the ﬁrst one wins on X = e datasets. Then,
the probability that we have e or less wins when indeed p = 1/2 is
e

P {X ≤ e} =
x=0

N
x

1
2

x

1
2

N−x

and we reject if this probability is too small, that is, less than α. If there
are ties, we divide them equally to both sides; that is, if there are t ties,
we add t/2 to e (if t is odd, we ignore the odd one and decrease N by 1).
In testing
H0 : μ1 ≤ μ2 vs. H1 : μ1 > μ2
we reject if P {X ≥ e} < α.
For the two-sided test
H0 : μ1 = μ2 vs. H1 : μ1 = μ2
we reject if e is too small or too large. If e < N/2, we reject if 2P {X ≤
e} < α; if e > N/2, we reject if 2P {X ≥ e} < α—we need to ﬁnd the
corresponding tail, and we multiply it by 2 because it is a two-tailed test.
As we discussed before, nonparametric tests can be used to compare
any measurements, for example, training times. In such a case, we see
the advantage of a nonparametric test that uses order rather than averages of absolute values. Let us say we compare two algorithms on ten
datasets, nine of which are small and have training times for both algorithms on the order of minutes, and one that is very large and whose
training time is on the order of a day. If we use a parametric test and
take the average of training times, the single large dataset will dominate
the decision, but when we use the nonparameric test and compare values
separately on each dataset, using the order will have the eﬀect of normalizing separately for each dataset and hence will help us make a robust
decision.
We can also use the sign test as a one sample test, for example, to
check if the average error on all datasets is less than two percent, by
comparing μ1 not by the mean of a second population but by a constant
μ0 . We can do this simply by plugging the constant μ0 in place of all
observations from a second sample and using the procedure used earlier;
that is, we will count how many times we get more or less than 0.02 and
check if this is too unlikely under the null hypothesis. For large N, normal

19.13 Comparison over Multiple Datasets

Wilcoxon signed
rank test

19.13.2
Kruskal-Wallis test

511

approximation to the binomial can be used (exercise 6), but in practice,
the number of datasets may be smaller than 20. Note that the sign test
is a test on the median of a population, which is equal to the mean if the
distribution is symmetric.
The sign test only uses the sign of the diﬀerence and not its magnitude,
but we may envisage a case where the ﬁrst algorithm, when it wins, always wins by a large margin whereas the second algorithm, when it wins,
always wins barely. The Wilcoxon signed rank test uses both the sign and
the magniture of diﬀerences, as follows:
Let us say, additional to the sign of diﬀerences, we also calculate mi =
1
2
|ei −ei | and then we order them so that the smallest, mini mi , is assigned
rank 1, the next smallest is assigned rank 2, and so on. If there are ties,
their ranks are given the average value that they would receive if they
diﬀered slightly. For example, if the magnitudes are 2, 1, 2, 4, the ranks
are 2.5, 1, 2.5, 4. We then calculate w+ as the sum of all ranks whose signs
are positive and w− as the sum of all ranks whose signs are negative.
The null hypothesis μ1 ≤ μ2 can be rejected in favor of the alternative
μ1 > μ2 only if w+ is much smaller than w− . Similarly, the two-sided
hypothesis μ1 = μ2 can be rejected in favor of the alternative μ1 = μ2
only if either w+ or w− , that is, w = min(w+ , w− ), is very small. The
critical values for the Wilcoxon signed rank test are tabulated and for
N > 20, normal approximations can be used.

Multiple Algorithms
The Kruskal-Wallis test is the nonparametric version of ANOVA and is
a multiple sample generalization of a rank test. Given the M = L · N
observations, for example, error rates, of L algorithms on N datasets,
Xij , i = 1, . . . , L, j = 1, . . . , N, we rank them from the smallest to the
largest and assign them ranks, Rij , between 1 and M, again taking averages in case of ties. If the null hypothesis
H0 : μ1 = μ2 = · · · = μL
is true, then the average of ranks of algorithm i should be approximately
halfway between 1 and M, that is, (M + 1)/2. We denote the sample
average rank of algorithm i by R i• and we reject the hypothesis if the
average ranks seem to diﬀer from halfway. The test statistic
L

H=

12
M +1
R i• −
(M + 1)L i=1
2

512

19 Design and Analysis of Machine Learning Experiments

Tukey’s test

is approximately chi-square distributed with L − 1 degrees of freedom
and we reject the null hypothesis if the statistic exceeds Xα,L−1 .
Just like the parametric ANOVA, if the null hypothesis is rejected, we
can do posthoc testing to check for pairwise comparison of ranks. One
method for this is Tukey’s test, which makes use of the studentized range
statistic
q=

Rmax − R min
σw

where Rmax and R min are the largest and smallest means (of ranks), re2
spectively, out of the L means, and σw is the average variance of ranks
around group rank averages. We reject that groups i and j have the same
ranks in favor of the alternative hypothesis that they are diﬀerent if
|Ri• − R j• | > qα (L, L(K − 1))σw
where qα (L, L(K − 1)) are tabulated. One-sided tests can also be deﬁned
to order algorithms in terms of average rank.
Demsar (2006) proposes to use CD (critical diﬀerence) diagrams for
visualization. On a scale of 1 to L, we mark the averages, Ri• , and draw
lines of length given by the critical diﬀerence, qα (L, L(K −1))σw , between
groups, so that lines connect groups that are not statistically signiﬁcantly
diﬀerent.

19.14

Notes
The material related to experiment design follows the discussion from
(Montgomery 2005), which here is adapted for machine learning. A more
detailed discussion of interval estimation, hypothesis testing, and analysis of variance can be found in any introductory statistics book, for example, Ross 1987.
Dietterich (1998) discusses statistical tests and compares them on a
number of applications using diﬀerent classiﬁcation algorithms. A review
of ROC use and AUC calculation is given in Fawcett 2006. Demsar (2006)
reviews statistical tests for comparing classiﬁers over multiple datasets.
When we compare two or more algorithms, if the null hypothesis that
they have the same error rate is not rejected, we choose the simpler one,
namely, the one with less space or time complexity. That is, we use our
prior preference if the data does not prefer one in terms of error rate.
For example, if we compare a linear model and a nonlinear model and

19.15 Exercises

513

if the test does not reject that they have the same expected error rate,
we should go for the simpler linear model. Even if the test rejects, in
choosing one algorithm over another, error rate is only one of the criteria.
Other criteria like training (space/time) complexity, testing complexity,
and interpretability may override in practical applications.
This is how the posthoc test results are used in the MultiTest algorithm
(Yıldız and Alpaydın 2006) to generate a full ordering. We do L(L − 1)/2
one-sided pairwise tests to order the L algorithms, but it is very likely
that the tests will not give a full ordering but only a partial order. The
missing links are ﬁlled in using the prior complexity information to get a
full order. A topological sort gives an ordering of algorithms using both
types of information, error and complexity.
There are also tests to allow checking for contrasts. Let us say 1 and
2 are neural network methods and 3 and 4 are fuzzy logic methods. We
can then test whether the average of 1 and 2 diﬀers from the average of
3 and 4, thereby allowing us to compare methods in general.
Another important point to note is that we are only assessing or comparing misclassiﬁcations. This implies that from our point of view, all
misclassiﬁcations have the same cost. When this is not the case, our
tests should be based on risks taking a suitable loss function into account. Not much work has been done in this area. Similarly, these tests
should be generalized from classiﬁcation to regression, so as to be able
to assess the mean square errors of regression algorithms, or to be able
to compare the errors of two regression algorithms.
In comparing two classiﬁcation algorithms, note that we are testing
only whether they have the same expected error rate. If they do, this
does not mean that they make the same errors. This is an idea that we
used in chapter 17; we can combine multiple models to improve accuracy
if diﬀerent classiﬁers make diﬀerent errors.

19.15

Exercises
1. In a two-class problem, let us say we have the loss matrix where λ11 = λ22 = 0,
λ21 = 1 and λ12 = α. Determine the threshold of decision as a function of α.
2. We can simulate a classiﬁer with error probability p by drawing samples from
a Bernoulli distribution. Doing this, implement the binomial, approximate,
and t tests for p0 ∈ (0, 1). Repeat these tests at least 1,000 times for several values of p and calculate the probability of rejecting the null hypothesis.
What do you expect the probability of reject to be when p0 = p?

514

19 Design and Analysis of Machine Learning Experiments

3. Assume xt ∼ N (μ, σ 2 ) where σ 2 is known. How can we test for H0 : μ ≥ μ0
vs. H1 : μ < μ0 ?
4. The K-fold cross-validated t test only tests for the equality of error rates. If
the test rejects, we do not know which classiﬁcation algorithm has the lower
error rate. How can we test whether the ﬁrst classiﬁcation algorithm does not
have higher error rate than the second one? Hint: We have to test H0 : μ ≤ 0
vs. H1 : μ > 0.
5. Show that the total sum of squares can be split into between-group sum of
squares and within-group sum of squares as SST = SSb + SSw .
6. Use the normal approximation to the binomial for the sign test.
7. Let us say we have three classiﬁcation algorithms. How can we order these
three from best to worst?
8. If we have two variants of algorithm A and three variants of algorithm B, how
can we compare the overall accuracies of A and B taking all their variants into
account?
9. Propose a suitable test to compare the errors of two regression algorithms.
10. Propose a suitable test to compare the expected rewards of two reinforcement
learning algorithms.

19.16

References
Alpaydın, E. 1999. “Combined 5 × 2 cv F Test for Comparing Supervised Classiﬁcation Learning Algorithms.” Neural Computation 11: 1885–1892.
Bouckaert, R. R. 2003. “Choosing between Two Learning Algorithms based on
Calibrated Tests.” In Twentieth International Conference on Machine Learning, ed. T. Fawcett and N. Mishra, 51–58. Menlo Park, CA: AAAI Press.
Demsar, J. 2006. “Statistical Comparison of Classiﬁers over Multiple Data Sets.”
Journal of Machine Learning Research 7: 1–30.
Dietterich, T. G. 1998. “Approximate Statistical Tests for Comparing Supervised
Classiﬁcation Learning Algorithms.” Neural Computation 10: 1895–1923.
Fawcett, T. 2006. “An Introduction to ROC Analysis.” Pattern Recognition Letters 27: 861–874.
Montgomery, D. C. 2005. Design and Analysis of Experiments. 6th ed., New
York: Wiley.
Ross, S. M. 1987. Introduction to Probability and Statistics for Engineers and
Scientists. New York: Wiley.

19.16 References

515

Turney, P. 2000. “Types of Cost in Inductive Concept Learning.” Paper presented at Workshop on Cost-Sensitive Learning at the Seventeenth International Conference on Machine Learning, Stanford University, Stanford, CA,
July 2.
Wolpert, D. H. 1995. “The Relationship between PAC, the Statistical Physics
Framework, the Bayesian Framework, and the VC Framework.” In The Mathematics of Generalization, ed. D. H. Wolpert, 117–214. Reading, MA: AddisonWesley.
Yıldız, O. T., and E. Alpaydın. 2006. “Ordering and Finding the Best of K > 2
Supervised Learning Algorithms.” IEEE Transactions on Pattern Analysis and
Machine Intelligence 28: 392–402.

A

Probability

We review brieﬂy the elements of probability, the concept of a random variable, and example distributions.

A.1

Elements of Probability
A r an d om experiment is one whose outcome is not predictable with
certainty in advance (Ross 1987; Casella and Berger 1990). The set of all
possible outcomes is known as the sample space S. A sample space is
discrete if it consists of a ﬁnite (or countably inﬁnite) set of outcomes;
otherwise it is continuous. Any subset E of S is an event. Events are
sets, and we can talk about their complement, intersection, union, and so
forth.
One interpretation of probability is as a frequency. When an experiment is continually repeated under the exact same conditions, for any
event E, the proportion of time that the outcome is in E approaches some
constant value. This constant limiting frequency is the probability of the
event, and we denote it as P (E).
Probability sometimes is interpreted as a degree of belief. For example,
when we speak of Turkey’s probability of winning the World Soccer Cup
in 2010, we do not mean a frequency of occurrence, since the championship will happen only once and it has not yet occurred (at the time of
the writing of this book). What we mean in such a case is a subjective
degree of belief in the occurrence of the event. Because it is subjective,
diﬀerent individuals may assign diﬀerent probabilities to the same event.

518

A Probability

A.1.1

Axioms of Probability
Axioms ensure that the probabilities assigned in a random experiment
can be interpreted as relative frequencies and that the assignments are
consistent with our intuitive understanding of relationships among relative frequencies:
1. 0 ≤ P (E) ≤ 1. If E1 is an event that cannot possibly occur, then P (E1 ) =
0. If E2 is sure to occur, P (E2 ) = 1.
2. S is the sample space containing all possible outcomes, P (S) = 1.
3. If Ei , i = 1, . . . , n are mutually exclusive (i.e., if they cannot occur at the
same time, as in Ei ∩ Ej = ∅, j = i, where ∅ is the null event that does
not contain any possible outcomes), we have
⎛

(A.1)

P⎝

n
i=1

⎞
Ei ⎠ =

n

P (Ei )
i=1

For example, letting E c denote the complement of E, consisting of all
possible outcomes in S that are not in E, we have E ∩ E C = ∅ and
P (E ∪ E c ) = P (E) + P (E c ) = 1
P (E c ) = 1 − P (E)
If the intersection of E and F is not empty, we have
(A.2)

A.1.2

P (E ∪ F ) = P (E) + P (F ) − P (E ∩ F )

Conditional Probability
P (E|F ) is the probability of the occurrence of event E given that F occurred and is given as

(A.3)

P (E|F ) =

P (E ∩ F )
P (F )

Knowing that F occurred reduces the sample space to F , and the part
of it where E also occurred is E ∩ F . Note that equation A.3 is well-deﬁned
only if P (F ) > 0. Because ∩ is commutative, we have
P (E ∩ F ) = P (E|F )P (F ) = P (F |E)P (E)

519

A.2 Random Variables

which gives us Bayes’ formula:
(A.4)

P (F |E) =

P (E|F )P (F )
P (E)

When Fi are mutually exclusive and exhaustive, namely,

n
i=1 Fi

=S

n

E

E ∩ Fi

=
i=1
n

(A.5)

P (E)

=

n

P (E ∩ Fi ) =
i=1

P (E|Fi )P (Fi )
i=1

Bayes’ formula allows us to write
(A.6)

P (Fi |E) =

P (E ∩ Fi )
=
P (E)

P (E|Fi )P (Fi )
j P (E|Fj )P (Fj )

If E and F are independent, we have P (E|F ) = P (E) and thus
(A.7)

P (E ∩ F ) = P (E)P (F )
That is, knowledge of whether F has occurred does not change the probability that E occurs.

A.2

Random Variables
A random variable is a function that assigns a number to each outcome
in the sample space of a random experiment.

A.2.1

Probability Distribution and Density Functions
The probability distribution function F (·) of a random variable X for any
real number a is

(A.8)

F (a) = P {X ≤ a}
and we have

(A.9)

P {a < X ≤ b} = F (b) − F (a)
If X is a discrete random variable

(A.10)

F (a) =

P (x)
∀x≤a

520

A Probability

where P (·) is the probability mass function deﬁned as P (a) = P {X = a}. If
X is a continuous random variable, p(·) is the probability density function
such that
a

(A.11)

F (a) =

A.2.2

Joint Distribution and Density Functions

−∞

p(x)dx

In certain experiments, we may be interested in the relationship between
two or more random variables, and we use the joint probability distribution and density functions of X and Y satisfying
(A.12)

F (x, y) = P {X ≤ x, Y ≤ y}
Individual marginal distributions and densities can be computed by
marginalizing, namely, summing over the free variable:

(A.13)

FX (x) = P {X ≤ x} = P {X ≤ x, Y ≤ ∞} = F (x, ∞)
In the discrete case, we write

(A.14)

P (X = x) =

P (x, yj )
j

and in the continuous case, we have
(A.15)

pX (x) =

∞
−∞

p(x, y)dy

If X and Y are independent, we have
(A.16)

p(x, y) = pX (x)pY (y)
These can be generalized in a straightforward manner to more than two
random variables.

A.2.3

Conditional Distributions
When X and Y are random variables

(A.17)

PX|Y (x|y) = P {X = x|Y = y} =

P (x, y)
P {X = x, Y = y}
=
P {Y = y}
PY (y)

521

A.2 Random Variables

A.2.4

Bayes’ Rule
When two random variables are jointly distributed with the value of one
known, the probability that the other takes a given value can be computed
using Bayes’ rule:

(A.18)

P (y|x) =

P (x|y)PY (y)
=
PX (x)

P (x|y)PY (y)
y P (x|y)PY (y)

Or, in words
(A.19)

posterior =

likelihood × prior
evidence

Note that the denominator is obtained by summing (or integrating if y
is continuous) the numerator over all possible y values. The “shape” of
p(y|x) depends on the numerator with denominator as a normalizing
factor to guarantee that p(y|x) sum to 1. Bayes’ rule allows us to modify a prior probability into a posterior probability by taking information
provided by x into account.
Bayes’ rule inverts dependencies, allowing us to compute p(y|x) if
p(x|y) is known. Suppose that y is the “cause” of x, like y going on summer vacation and x having a suntan. Then p(x|y) is the probability that
someone who is known to have gone on summer vacation has a suntan.
This is the causal (or predictive) way. Bayes’ rule allows us a diagnostic
approach by allowing us to compute p(y|x): namely, the probability that
someone who is known to have a suntan, has gone on summer vacation.
Then p(y) is the general probability of anyone’s going on summer vacation and p(x) is the probability that anyone has a suntan, including both
those who have gone on summer vacation and those who have not.

A.2.5

Expectation
Expectation, expected value, or mean of a random variable X, denoted by
E[X], is the average value of X in a large number of experiments:

(A.20)

E[X] =

xi P (xi )
xp(x)dx
i

if X is discrete
if X is continuous

It is a weighted average where each value is weighted by the probability
that X takes that value. It has the following properties (a, b ∈ ):
(A.21)

E[aX + b]

=

aE[X] + b

E[X + Y ]

=

E[X] + E[Y ]

522

A Probability

For any real-valued function g(·), the expected value is
(A.22)

i g(xi )P (xi )
g(x)p(x)dx

E[g(X)] =

if X is discrete
if X is continuous

A special g(x) = xn , called the nth moment of X, is deﬁned as
(A.23)

xn P (xi )
i
x p(x)dx

if X is discrete
if X is continuous

i

E[X n ] =

n

Mean is the ﬁrst moment and is denoted by μ.

A.2.6

Variance
Variance measures how much X varies around the expected value. If
μ ≡ E[X], the variance is deﬁned as

(A.24)

Var(X) = E[(X − μ)2 ] = E[X 2 ] − μ 2
Variance is the second moment minus the square of the ﬁrst moment.
Variance, denoted by σ 2 , satisﬁes the following property (a, b ∈ ):

(A.25)

Var(aX + b) = a2 Var(X)
Var(X) is called the standard deviation and is denoted by σ . Standard
deviation has the same unit as X and is easier to interpret than variance.
Covariance indicates the relationship between two random variables.
If the occurrence of X makes Y more likely to occur, then the covariance
is positive; it is negative if X’s occurrence makes Y less likely to happen
and is 0 if there is no dependence.

(A.26)

Cov(X, Y ) = E [(X − μX )(Y − μY )] = E[XY ] − μX μY
where μX ≡ E[X] and μY ≡ E[Y ]. Some other properties are
Cov(X, Y )

Cov(Y , X)

Cov(X, X)

=

Var(X)

Cov(X + Z, Y )
⎛
⎞
(A.27)

=
=

Cov(X, Y ) + Cov(Z, Y )

Cov ⎝

Xi , Y ⎠

=

Cov(Xi , Y )

i

(A.28)
(A.29)

i

Var(X + Y )
⎞
⎛

=

Xi ⎠

=

Var ⎝
i

Var(X) + Var(Y ) + 2Cov(X, Y )
Var(Xi ) +
i

Cov(Xi , Xj )
i j=i

A.3 Special Random Variables

523

If X and Y are independent, E[XY ] = E[X]E[Y ] = μX μY and Cov(X, Y ) =
0. Thus if Xi are independent
⎞
⎛
(A.30)

Xi ⎠ =

Var ⎝
i

Var(Xi )
i

Correlation is a normalized, dimensionless quantity that is always between −1 and 1:
Cov(X, Y )
Var(X)Var(Y )

(A.31)

Corr(X, Y ) =

A.2.7

Weak Law of Large Numbers
Let X = {X t }N be a set of independent and identically distributed (iid)
t=1
random variables each having mean μ and a ﬁnite variance σ 2 . Then for
any > 0

(A.32)

P

Xt
−μ >
N
t

→ 0 as N → ∞

That is, the average of N trials converges to the mean as N increases.

A.3

Special Random Variables
There are certain types of random variables that occur so frequently that
names are given to them.

A.3.1

Bernoulli Distribution
A trial is performed whose outcome is either a “success” or a “failure.”
The random variable X is a 0/1 indicator variable and takes the value 1
for a success outcome and is 0 otherwise. p is the probability that the
result of trial is a success. Then

(A.33)

P {X = 1} = p and P {X = 0} = 1 − p
which can equivalently be written as

(A.34)

P {X = i} = pi (1 − p)1−i , i = 0, 1
If X is Bernoulli, its expected value and variance are

(A.35)

E[X] = p, Var(X) = p(1 − p)

524

A Probability

A.3.2

Binomial Distribution
If N identical independent Bernoulli trials are made, the random variable X that represents the number of successes that occurs in N trials is
binomial distributed. The probability that there are i successes is

(A.36)

P {X = i} =

N
i

pi (1 − p)N−i , i = 0 . . . N

If X is binomial, its expected value and variance are
(A.37)

E[X] = Np, Var(X) = Np(1 − p)

A.3.3

Multinomial Distribution
Consider a generalization of Bernoulli where instead of two states, the
outcome of a random event is one of K mutually exclusive and exhaustive
K
states, each of which has a probability of occurring pi where i=1 pi =
1. Suppose that N such trials are made where outcome i occurred Ni
k
times with i=1 Ni = N. Then the joint distribution of N1 , N2 , . . . , NK is
multinomial:
K

(A.38)

P (N1 , N2 , . . . , NK ) = N!

N

pi i
Ni !
i=1

A special case is when N = 1; only one trial is made. Then Ni are 0/1
indicator variables of which only one of them is 1 and all others are 0.
Then equation A.38 reduces to
K

(A.39)

Ni

P (N1 , N2 , . . . , NK ) =

pi
i=1

A.3.4

Uniform Distribution
X is uniformly distributed over the interval [a, b] if its density function
is given by

(A.40)

p(x) =

1
b−a

0

if a ≤ x ≤ b
otherwise

If X is uniform, its expected value and variance are
(A.41)

E[X] =

(b − a)2
a+b
, Var(X) =
2
12

525

A.3 Special Random Variables

Unit Normal Z = N(0,1)
0.4
0.35
0.3

p(x)

0.25
0.2
0.15
0.1
0.05
0
−5

−4

−3

−2

−1

0
x

1

2

3

4

5

Figure A.1 Probability density function of Z, the unit normal distribution.

A.3.5

Normal (Gaussian) Distribution
X is normal or Gaussian distributed with mean μ and variance σ 2 , denoted as N (μ, σ 2 ), if its density function is

(A.42)

p(x) = √

(x − μ)2
1
exp −
2σ 2
2π σ

, −∞ < x < ∞

Many random phenomena obey the bell-shaped normal distribution, at
least approximately, and many observations from nature can be seen as a
continuous, slightly diﬀerent versions of a typical value—that is probably
why it is called the normal distribution. In such a case, μ represents the
typical value and σ deﬁnes how much instances vary around the prototypical value.
68.27 percent lie in (μ − σ , μ + σ ), 95.45 percent in (μ − 2σ , μ + 2σ ),
and 99.73 percent in (μ − 3σ , μ + 3σ ). Thus P {|x − μ| < 3σ } ≈ 0.99. For
practical purposes, p(x) ≈ 0 if x < μ − 3σ or x > μ + 3σ . Z is unit normal,
namely, N (0, 1) (see ﬁgure A.1), and its density is written as
(A.43)

pZ (x) = √

x2
1
exp −
2
2π

526

A Probability

If X ∼ N (μ, σ 2 ) and Y = aX + b, then Y ∼ N (aμ + b, a2 σ 2 ). The
sum of independent normal variables is also normal with μ = i μi and
σ 2 = i σi2 . If X is N (μ, σ 2 ), then
(A.44)

central limit
theorem

(A.45)

X−μ
∼Z
σ
This is called z-normalization.
Let X1 , X2 , . . . , XN be a set of iid random variables all having mean μ
and variance σ 2 . Then the central limit theorem states that for large N,
the distribution of
X1 + X 2 + . . . + X N
is approximately N (Nμ, Nσ 2 ). For example, if X is binomial with parameters (N, p), X can be written as the sum of N Bernoulli trials and
(X − Np)/ Np(1 − p) is approximately unit normal.
Central limit theorem is also used to generate normally distributed random variables on computers. Programming languages have subroutines
that return uniformly distributed (pseudo-)random numbers in the range
12
[0, 1]. When Ui are such random variables, i=1 Ui − 6 is approximately
Z.
Let us say X t ∼ N (μ, σ 2 ). The estimated sample mean
N
t
t=1 X

(A.46)

m=

A.3.6

Chi-Square Distribution

N
is also normal with mean μ and variance σ 2 /N.

If Zi are independent unit normal random variables, then
(A.47)

2
2
2
X = Z1 + Z2 + . . . + Zn
2
is chi-square with n degrees of freedom, namely, X ∼ Xn , with

(A.48)

E[X] = n, Var(X) = 2n
When X t ∼ N (μ, σ 2 ), the estimated sample variance is

(A.49)

(A.50)

− m)2
N −1
and we have
S2
2
(N − 1) 2 ∼ XN−1
σ
It is also known that m and S 2 are independent.

S2 =

t (X

t

527

A.4 References

A.3.7

t Distribution
2
If Z ∼ Z and X ∼ Xn are independent, then

(A.51)

Tn =

Z
X/n

is t-distributed with n degrees of freedom with
(A.52)

E[Tn ] = 0, n > 1, Var(Tn ) =

n
,n > 2
n−2

Like the unit normal density, t is symmetric around 0. As n becomes
larger, t density becomes more and more like the unit normal, the diﬀerence being that t has thicker tails, indicating greater variability than does
normal.

A.3.8

F Distribution
2
2
If X1 ∼ Xn and X2 ∼ Xm are independent chi-square random variables
with n and m degrees of freedom, respectively,

(A.53)

Fn,m =

X1 /n
X2 /m

is F -distributed with n and m degrees of freedom with
(A.54)

A.4

E[Fn,m ] =

m
m2 (2m + 2n − 4)
, m > 2, Var(Fn,m ) =
,m > 4
m−2
n(m − 2)2 (m − 4)

References
Casella, G., and R. L. Berger. 1990. Statistical Inference. Belmont, CA: Duxburry.
Ross, S. M. 1987. Introduction to Probability and Statistics for Engineers and
Scientists. New York: Wiley.

Index

0/1 loss function, 51
5×2
cross-validation, 488
cv paired F test, 503
cv paired t test, 503
Active learning, 360
AdaBoost, 431
Adaptive resonance theory, 285
Additive models, 180
Agglomerative clustering, 157
AIC, see Akaike’s information
criterion
Akaike’s information criterion, 81
Alignment, 324
Analysis of variance, 504
Anchor, 291
ANOVA, see Analysis of variance
Approximate normal test, 500
Apriori algorithm, 56
Area under the curve, 491
ART, see Adaptive resonance theory
Artiﬁcial neural networks, 233
Association rule, 4, 55
Attribute, 87
AUC, see Area under the curve
Autoassociator, 268
Backpropagation, 250
through time, 272
Backup, 456

Backward selection, 111
Backward variable, 372
Bag of words, 102, 324
Bagging, 430
Base-learner, 419
Basis function, 211
cooperative vs. competitive, 297
for a kernel, 352
normalization, 295
Basket analysis, 55
Batch learning, 251
Baum-Welch algorithm, 376
Bayes’ ball, 402
Bayes’ classiﬁer, 51
Bayes’ estimator, 68
Bayes’ rule, 49, 521
Bayesian information criterion, 81
Bayesian model combination, 426
Bayesian model selection, 82
Bayesian networks, 387
Belief networks, 387
Belief state, 465
Bellman’s equation, 452
Beta distribution, 345
Between-class scatter matrix, 130
Bias, 65
Bias unit, 237
Bias/variance dilemma, 78
BIC, see Bayesian information
criterion
Binary split, 187

530

Index

Binding, 202
Binomial test, 499
Biometrics, 441
Blocking, 482
Bonferroni correction, 508
Boosting, 431
Bootstrap, 489
C4.5, 191
C4.5Rules, 197
CART, 191, 203
Cascade correlation, 264
Cascading, 438
Case-based reasoning, 180
Causality, 396
causal graph, 388
Central limit theorem, 526
Class
confusion matrix, 493
likelihood, 50
Classiﬁcation, 5
likelihood- vs.
discriminant-based, 209
Classiﬁcation tree, 188
Clique, 411
Cluster, 144
Clustering, 11
agglomerative, 157
divisive, 157
hierarchical, 157
online, 281
Code word, 146
Codebook vector, 146
Coeﬃcient of determination (of
regression), 76
Color quantization, 145
Common principal components,
119
Competitive basis functions, 297
Competitive learning, 280
Complete-link clustering, 158
Component density, 144

Compression, 8, 146
Condensed nearest neighbor, 173
Conditional independence, 389
Conﬁdence interval
one-sided, 495
two-sided, 494
Conﬁdence of an association rule,
55
Conjugate prior, 344
Connection weight, 237
Contingency table, 501
Correlation, 89
Cost-sensitive learning, 478
Coupled HMM, 400
Covariance function, 356
Covariance matrix, 88
Credit assignment, 448
Critic, 448
CRM, see Customer relationship
management
Cross-entropy, 221
Cross-validation, 40, 80, 486
5 × 2, 488
K-fold, 487
Curse of dimensionality, 170
Customer relationship
management, 155
Customer segmentation, 155
d-separation, 402
Decision node, 185
Decision region, 53
Decision tree, 185
multivariate, 202
omnivariate, 205
soft, 305
univariate, 187
Delve repository, 17
Dendrogram, 158
Density estimation, 11
Dichotomizer, 53
Diﬀusion kernel, 325

531

Index

Dimensionality reduction
nonlinear, 269
Directed acyclic graph, 387
Dirichlet distribution, 344
Discount rate, 451
Discriminant, 5
function, 53
linear, 97
quadratic, 95
Discriminant adaptive nearest
neighbor, 172
Discriminant-based classiﬁcation,
209
Distributed vs. local
representation, 156, 289
Diversity, 420
Divisive clustering, 157
Document categorization, 102
Doubt, 26
Dual representation, 337, 352
Dynamic classiﬁer selection, 435
Dynamic graphical models, 415
Dynamic node creation, 264
Dynamic programming, 453
Early stopping, 223, 258
ECOC, 327, see Error-correcting
output codes
Edit distance, 324
Eigendigits, 118
Eigenfaces, 118
Eligibility trace, 459
EM, see Expectation-Maximization
Emission probability, 367
Empirical error, 24
Empirical kernel map, 324
Ensemble, 424
Ensemble selection, 437
Entropy, 188
Episode, 451
Epoch, 251
Error

type I, 497
type II, 497
Error-correcting output codes, 427
Euclidean distance, 98
Evidence, 50
Example, 87
Expectation-Maximization, 150
supervised, 299
Expected error, 476
Expected utility, 54
Experiment
design, 478
factorial, 481
strategies, 480
Explaining away, 393
Extrapolation, 35
FA, see Factor analysis
Factor analysis, 120
Factor graph, 412
Factorial HMM, 400
Feature, 87
extraction, 110
selection, 110
Finite-horizon, 451
First-order rule, 201
Fisher kernel, 325
Fisher’s linear discriminant, 129
Flexible discriminant analysis, 120
Floating search, 112
Foil, 199
Forward selection, 110
Forward variable, 370
Forward-backward procedure, 370
Fuzzy k-means, 160
Fuzzy membership function, 295
Fuzzy rule, 295
Gamma distribution, 347
Gamma function, 344
Gaussian prior, 349
Generalization, 24, 39

532

Index

Generalized linear models, 230
Generative model, 342, 397
Generative topographic mapping,
306
Geodesic distance, 133
Gini index, 189
Gradient descent, 219
stochastic, 241
Gradient vector, 219
Gram matrix, 321
Graphical models, 387
Group, 144
GTM, see Generative topographic
mapping
Hamming distance, 171
Hebbian learning, 283
Hidden layer, 246
Hidden Markov model, 367, 398
coupled, 400
factorial, 400
input-output, 379, 400
left-to-right, 380
switching, 400
Hidden variables, 57, 396
Hierarchical clustering, 157
Hierarchical cone, 260
Hierarchical mixture of experts,
304
Higher-order term, 211
Hinge loss, 317
Hint, 261
Histogram, 165
HMM, see Hidden Markov model
Hybrid learning, 291
Hypothesis, 23
class, 23
most general, 24
most speciﬁc, 24
Hypothesis testing, 496
ID3, 191
IF-THEN rules, 197

Iid (independent and identically
distributed), 41
Ill-posed problem, 38
Impurity measure, 188
Imputation, 89
Independence, 388
Inductive bias, 38
Inductive logic programming, 202
Inﬁnite-horizon, 451
Inﬂuence diagrams, 414
Information retrieval, 491
Initial probability, 364
Input, 87
Input representation, 21
Input-output HMM, 379, 399
Instance, 87
Instance-based learning, 164
Interest of an association rule, 55
Interpolation, 35
Interpretability, 197
Interval estimation, 493
Irep, 199
Isometric feature mapping, 133
Job shop scheduling, 471
Junction tree, 410
K-armed bandit, 449
K-fold
cross-validation, 487
cv paired t test, 502
k-means clustering, 147
fuzzy, 160
online, 281
k-nearest neighbor
classiﬁer, 172
density estimate, 169
smoother, 177
k-nn, see k-nearest neighbor
Kalman ﬁlter, 400
Karhunen-Loève expansion, 119
Kernel estimator, 167

Index

Kernel function, 167, 320, 353
Kernel PCA, 336
Kernel smoother, 176
kernelization, 321
Knowledge extraction, 8, 198, 295
Kolmogorov complexity, 82
Kruskal-Wallis test, 511
Laplace approximation, 354
Laplacian prior, 350
lasso, 352
Latent factors, 120
Lateral inhibition, 282
LDA, see Linear discriminant
analysis
Leader cluster algorithm, 148
Leaf node, 186
Learning automata, 471
Learning vector quantization, 300
Least square diﬀerence test, 507
Least squares estimate, 74
Leave-one-out, 487
Left-to-right HMM, 380
Level of signiﬁcance, 497
Levels of analysis, 234
Lift of an association rule, 55
Likelihood, 62
Likelihood ratio, 58
Likelihood-based classiﬁcation, 209
Linear classiﬁer, 97, 216
Linear discriminant, 97, 210
Linear discriminant analysis, 128
Linear dynamical system, 400
Linear opinion pool, 424
Linear regression, 74
multivariate, 103
Linear separability, 215
Local representation, 288
Locally linear embedding, 135
Locally weighted running line
smoother, 177

533

Loess, see Locally weighted running
line smoother
Log likelihood, 62
Log odds, 58, 218
Logistic discrimination, 220
Logistic function, 218
Logit, 218
Loss function, 51
LSD, see Least square diﬀerence
test
LVQ, see Learning vector
quantization
Mahalanobis distance, 90
Margin, 25, 311, 433
Markov decision process, 451
Markov mixture of experts, 379
Markov model, 364
hidden, 367
learning, 366, 375
observable, 365
Markov random ﬁeld, 410
Max-product algorithm, 413
Maximum a posteriori (MAP)
estimate, 68, 343
Maximum likelihood estimation, 62
McNemar’s test, 501
MDP, see Markov decision process
MDS, see Multidimensional scaling
Mean square error, 65
Mean vector, 88
Memory-based learning, 164
Minimum description length, 82
Mixture components, 144
Mixture density, 144
Mixture of experts, 301, 434
competitive, 304
cooperative, 303
hierarchical, 305
Markov, 379, 400
Mixture of factor analyzers, 155
Mixture of mixtures, 156

534

Index

Mixture of probabilistic principal
component analyzers, 155
Mixture proportion, 144
MLE, see Maximum likelihood
estimation
Model combination
multiexpert, 423
multistage, 423
Model selection, 38
MoE, see Mixture of experts
Momentum, 257
Moralization, 411
Multidimensional scaling, 125
nonlinear, 287
using MLP, 269
Multilayer perceptrons, 246
Multiple comparisons, 507
Multiple kernel learning, 326, 442
Multivariate linear regression, 103
Multivariate polynomial regression,
104
Multivariate tree, 202
Naive Bayes’ classiﬁer, 397
discrete inputs, 102
numeric inputs, 97
Naive estimator, 166
Nearest mean classiﬁer, 98
Nearest neighbor classiﬁer, 172
condensed, 173
Negative examples, 21
Neuron, 233
No Free Lunch Theorem, 477
Noise, 30
Noisy OR, 409
Nonparametric estimation, 163
Nonparametric tests, 508
Null hypothesis, 497
Observable Markov model, 365
Observable variable, 48
Observation, 87
Observation probability, 367

OC1, 203
Occam’s razor, 32
Oﬀ-policy, 458
Omnivariate decision tree, 205
On-policy, 458
One-class classiﬁcation, 333
One-sided conﬁdence interval, 495
One-sided test, 498
Online k-means, 281
Online learning, 241
Optimal policy, 452
Optimal separating hyperplane, 311
Outlier detection, 9, 333
Overﬁtting, 39, 79
Overtraining, 258
PAC, see Probably approximately
correct
Paired test, 501
Pairing, 482
Pairwise separation, 216, 428
Parallel processing, 236
Partially observable Markov
decision process, 464
Parzen windows, 167
Pattern recognition, 6
PCA, see Principal components
analysis
Pedigree, 400
Perceptron, 237
Phone, 381
Phylogenetic tree, 398
Piecewise approximation
constant, 248, 300
linear, 301
Policy, 451
Polychotomizer, 53
Polynomial regression, 75
multivariate, 104
Polytree, 407
POMDP, see Partially observable
Markov decision process

Index

Positive examples, 21
Posterior probability distribution,
341
Posterior probability of a class, 50
Posterior probability of a
parameter, 67
Posthoc testing, 507
Postpruning, 194
Potential function, 212, 411
Power function, 498
Precision
in information retrieval, 492
reciprocal of variance, 347
Predicate, 201
Prediction, 5
Prepruning, 194
Principal components analysis, 113
Prior knowledge, 294
Prior probability distribution, 341
Prior probability of a class, 50
Prior probability of a parameter, 67
Probabilistic networks, 387
Probabilistic PCA, 123
Probably approximately correct
learning, 29
Probit function, 355
Product term, 211
Projection pursuit, 274
Proportion of variance, 116
Propositional rule, 201
Pruning
postpruning, 194
prepruning, 194
set, 194
Q learning, 458
Quadratic discriminant, 95, 211
Quantization, 146
Radial basis function, 290
Random Subspace, 421
Randomization, 482
RBF, see Radial basis function

535

Real time recurrent learning, 272
Recall, 492
Receiver operating characteristics,
490
Receptive ﬁeld, 288
Reconstruction error, 119, 146
Recurrent network, 271
Reference vector, 146
Regression, 9, 35
linear, 74
polynomial, 75
polynomial multivariate, 104
robust, 329
Regression tree, 192
Regressogram, 175
Regularization, 80, 266
Regularized discriminant analysis,
100
Reinforcement learning, 13
Reject, 34, 52
Relative square error, 76
Replication, 482
Representation, 21
distributed vs. local, 288
Response surface design, 481
Ridge regression, 266, 350
Ripper, 199
Risk function, 51
Robust regression, 329
ROC, see Receiver operating
characteristics
RSE, see Relative square error
Rule
extraction, 295
induction, 198
pruning, 198
Rule support, 198
Rule value metric, 199
Running smoother
line, 177
mean, 175

536

Index

Sammon mapping, 128
using MLP, 269
Sammon stress, 128
Sample, 48
correlation, 89
covariance, 89
mean, 89
Sarsa, 458
Sarsa(λ), 461
Scatter, 129
Scree graph, 116
Self-organizing map, 286
Semiparametric density estimation,
144
Sensitivity, 493
Sensor fusion, 421
Sequential covering, 199
Sigmoid, 218
Sign test, 509
Single-link clustering, 157
Slack variable, 315
Smoother, 174
Smoothing splines, 178
Soft count, 376
Soft error, 315
Soft weight sharing, 267
Softmax, 224
SOM, see Self-organizing map
Spam ﬁltering, 103
Speciﬁcity, 493
Spectral decomposition, 115
Speech recognition, 380
Sphere node, 203
Stability-plasticity dilemma, 281
Stacked generalization, 435
Statlib repository, 17
Stochastic automaton, 364
Stochastic gradient descent, 241
Stratiﬁcation, 487
Strong learner, 431
Structural adaptation, 263
Structural risk minimization, 82

Subset selection, 110
Sum-product algorithm, 412
Supervised learning, 9
Support of an association rule, 55
Support vector machine, 313
SVM, see Support vector machine
Switching HMM, 400
Synapse, 234
Synaptic weight, 237
t distribution, 495
t test, 498
Tangent prop, 263
TD, see Temporal diﬀerence
Template matching, 98
Temporal diﬀerence, 455
learning, 458
TD(0), 459
TD-Gammon, 471
Test set, 40
Threshold, 212
function, 238
Time delay neural network, 270
Topographical map, 287
Transition probability, 364
Traveling salesman problem, 306
Triple trade-oﬀ, 39
Tukey’s test, 512
Two-sided conﬁdence interval, 494
Two-sided test, 497
Type 2 maximum likelihood
procedure, 360
Type I error, 497
Type II error, 497
UCI repository, 17
Unbiased estimator, 65
Underﬁtting, 39, 79
Unfolding in time, 272
Unit normal distribution, 493
Univariate tree, 187
Universal approximation, 248

Index

Unobservable variable, 48
Unstable algorithm, 430
Utility function, 54
Utility theory, 54
Validation set, 40
Value iteration, 453
Value of information, 464, 469
Vapnik-Chervonenkis (VC)
dimension, 27
Variance, 66
Vector quantization, 146
supervised, 300
Version space, 24
Vigilance, 285
Virtual example, 262
Viterbi algorithm, 374
Voronoi tesselation, 172
Voting, 424
Weak learner, 431
Weight
decay, 263
sharing, 260
sharing soft, 267
vector, 212
Wilcoxon signed rank test, 511
Winner-take-all, 280
Within-class scatter matrix, 130
Wrappers, 138
z, see Unit normal distribution
z-normalization, 91, 526
Zero-one loss, 51

537

Adaptive Computation and Machine Learning

Thomas Dietterich, Editor
Christopher Bishop, David Heckerman, Michael Jordan, and Michael
Kearns, Associate Editors

Bioinformatics: The Machine Learning Approach, Pierre Baldi and Søren
Brunak
Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew
G. Barto
Graphical Models for Machine Learning and Digital Communication,
Brendan J. Frey
Learning in Graphical Models, Michael I. Jordan
Causation, Prediction, and Search, second edition, Peter Spirtes, Clark
Glymour, and Richard Scheines
Principles of Data Mining, David Hand, Heikki Mannila, and Padhraic
Smyth
Bioinformatics: The Machine Learning Approach, second edition, Pierre
Baldi and Søren Brunak
Learning Kernel Classiﬁers: Theory and Algorithms, Ralf Herbrich
Learning with Kernels: Support Vector Machines, Regularization,
Optimization, and Beyond, Bernhard Schölkopf and Alexander J. Smola
Introduction to Machine Learning, Ethem Alpaydın
Gaussian Processes for Machine Learning, Carl Edward Rasmussen and
Christopher K. I. Williams
Semi-Supervised Learning, Olivier Chapelle, Bernhard Schölkopf, and
Alexander Zien, Eds.

The Minimum Description Length Principle, Peter D. Grünwald
Introduction to Statistical Relational Learning, Lise Getoor and Ben
Taskar, Eds.
Probabilistic Graphical Models: Principles and Techniques, Daphne Koller
and Nir Friedman
Introduction to Machine Learning, second edition, Ethem Alpaydın

